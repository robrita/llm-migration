{
  "run_id": "run-380047d2",
  "dataset_id": "ds-0e3b6637",
  "model_names": [
    "gpt-5.2",
    "gpt-5.1",
    "gpt-4.1",
    "gpt-4o"
  ],
  "results": [
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-001",
      "turn_number": 1,
      "question": "What topics are covered under the Model AI Governance Framework for Agentic AI?",
      "page_reference": 1,
      "response": "Under **Section 2: Model AI Governance Framework for Agentic AI**, the document covers four main topic areas (with key subtopics):\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment  \n   - Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation  \n   - Design for meaningful human oversight  \n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls  \n   - Before deploying, test agents  \n   - When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility**\n   - Different users, different needs  \n   - Users who interact with agents  \n   - Users who integrate agents into their work processes",
      "latency_ms": 10770.57409286499,
      "api_latency_ms": 10770.57409286499,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 467,
      "completion_tokens": 167,
      "cached_tokens": 0,
      "total_tokens": 634,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "Under **Section 2: Model AI Governance Framework for Agentic AI**, the document covers four main topic areas (with key subtopics):\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment  \n   - Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation  \n   - Design for meaningful human oversight  \n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls  \n   - Before deploying, test agents  \n   - When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility**\n   - Different users, different needs  \n   - Users who interact with agents  \n   - Users who integrate agents into their work processes"
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-001",
      "turn_number": 2,
      "question": "Within that framework, what are the specific steps listed under implementing technical controls and processes?",
      "page_reference": 1,
      "response": "Within the **Model AI Governance Framework for Agentic AI**, the specific steps listed under **2.3 Implement technical controls and processes** are:\n\n- **2.3.1** During design and development, use technical controls  \n- **2.3.2** Before deploying, test agents  \n- **2.3.3** When deploying, continuously monitor and test",
      "latency_ms": 1620.1906204223633,
      "api_latency_ms": 1620.1906204223633,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "Under **Section 2: Model AI Governance Framework for Agentic AI**, the document covers four main topic areas (with key subtopics):\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment  \n   - Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation  \n   - Design for meaningful human oversight  \n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls  \n   - Before deploying, test agents  \n   - When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility**\n   - Different users, different needs  \n   - Users who interact with agents  \n   - Users who integrate agents into their work processes"
        }
      ],
      "prompt_tokens": 657,
      "completion_tokens": 79,
      "cached_tokens": 0,
      "total_tokens": 736,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "Under **Section 2: Model AI Governance Framework for Agentic AI**, the document covers four main topic areas (with key subtopics):\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment  \n   - Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation  \n   - Design for meaningful human oversight  \n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls  \n   - Before deploying, test agents  \n   - When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility**\n   - Different users, different needs  \n   - Users who interact with agents  \n   - Users who integrate agents into their work processes"
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Within the **Model AI Governance Framework for Agentic AI**, the specific steps listed under **2.3 Implement technical controls and processes** are:\n\n- **2.3.1** During design and development, use technical controls  \n- **2.3.2** Before deploying, test agents  \n- **2.3.3** When deploying, continuously monitor and test"
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-001",
      "turn_number": 3,
      "question": "How does the framework describe enabling end-user responsibility in terms of different user types?",
      "page_reference": 1,
      "response": "Under **2.4 Enable end-user responsibility**, the framework distinguishes end-user responsibility by recognizing that **different user types have different needs and responsibilities**:\n\n- **Different users, different needs (2.4.1):** End-user responsibility should be tailored depending on who the user is and how they engage with the agent.\n- **Users who interact with agents (2.4.2):** Users whose role is to directly use or communicate with the agent.\n- **Users who integrate agents into their work processes (2.4.3):** Users who embed the agent into workflows or operational processes, taking on responsibility for how it is used within those processes.",
      "latency_ms": 4649.766206741333,
      "api_latency_ms": 4649.766206741333,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "Under **Section 2: Model AI Governance Framework for Agentic AI**, the document covers four main topic areas (with key subtopics):\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment  \n   - Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation  \n   - Design for meaningful human oversight  \n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls  \n   - Before deploying, test agents  \n   - When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility**\n   - Different users, different needs  \n   - Users who interact with agents  \n   - Users who integrate agents into their work processes"
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Within the **Model AI Governance Framework for Agentic AI**, the specific steps listed under **2.3 Implement technical controls and processes** are:\n\n- **2.3.1** During design and development, use technical controls  \n- **2.3.2** Before deploying, test agents  \n- **2.3.3** When deploying, continuously monitor and test"
        }
      ],
      "prompt_tokens": 758,
      "completion_tokens": 138,
      "cached_tokens": 0,
      "total_tokens": 896,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "Under **Section 2: Model AI Governance Framework for Agentic AI**, the document covers four main topic areas (with key subtopics):\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment  \n   - Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation  \n   - Design for meaningful human oversight  \n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls  \n   - Before deploying, test agents  \n   - When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility**\n   - Different users, different needs  \n   - Users who interact with agents  \n   - Users who integrate agents into their work processes"
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Within the **Model AI Governance Framework for Agentic AI**, the specific steps listed under **2.3 Implement technical controls and processes** are:\n\n- **2.3.1** During design and development, use technical controls  \n- **2.3.2** Before deploying, test agents  \n- **2.3.3** When deploying, continuously monitor and test"
        },
        {
          "role": "user",
          "content": "How does the framework describe enabling end-user responsibility in terms of different user types?"
        },
        {
          "role": "assistant",
          "content": "Under **2.4 Enable end-user responsibility**, the framework distinguishes end-user responsibility by recognizing that **different user types have different needs and responsibilities**:\n\n- **Different users, different needs (2.4.1):** End-user responsibility should be tailored depending on who the user is and how they engage with the agent.\n- **Users who interact with agents (2.4.2):** Users whose role is to directly use or communicate with the agent.\n- **Users who integrate agents into their work processes (2.4.3):** Users who embed the agent into workflows or operational processes, taking on responsibility for how it is used within those processes."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-002",
      "turn_number": 1,
      "question": "What capabilities distinguish AI agents from generative AI?",
      "page_reference": 2,
      "response": "AI agents go beyond generative AI by being able to:\n\n- **Take actions** (not just generate content), e.g., update a database or make a payment.  \n- **Adapt to new information** as they operate, adjusting their plans based on what they observe.  \n- **Interact with other agents and external systems/tools** to complete end-to-end tasks on behalf of humans.",
      "latency_ms": 11164.191484451294,
      "api_latency_ms": 11164.191484451294,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 620,
      "completion_tokens": 82,
      "cached_tokens": 0,
      "total_tokens": 702,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents go beyond generative AI by being able to:\n\n- **Take actions** (not just generate content), e.g., update a database or make a payment.  \n- **Adapt to new information** as they operate, adjusting their plans based on what they observe.  \n- **Interact with other agents and external systems/tools** to complete end-to-end tasks on behalf of humans."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-002",
      "turn_number": 2,
      "question": "What workplace examples are given for how agents are already transforming work?",
      "page_reference": 2,
      "response": "The examples given are **coding assistants**, **customer service agents**, and **automating enterprise productivity workflows**.",
      "latency_ms": 1138.4687423706055,
      "api_latency_ms": 1138.4687423706055,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents go beyond generative AI by being able to:\n\n- **Take actions** (not just generate content), e.g., update a database or make a payment.  \n- **Adapt to new information** as they operate, adjusting their plans based on what they observe.  \n- **Interact with other agents and external systems/tools** to complete end-to-end tasks on behalf of humans."
        }
      ],
      "prompt_tokens": 721,
      "completion_tokens": 26,
      "cached_tokens": 0,
      "total_tokens": 747,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents go beyond generative AI by being able to:\n\n- **Take actions** (not just generate content), e.g., update a database or make a payment.  \n- **Adapt to new information** as they operate, adjusting their plans based on what they observe.  \n- **Interact with other agents and external systems/tools** to complete end-to-end tasks on behalf of humans."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The examples given are **coding assistants**, **customer service agents**, and **automating enterprise productivity workflows**."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-002",
      "turn_number": 3,
      "question": "What factors determine the risks posed by an agent’s actions, and what are two planning-stage steps organisations can take to manage those risks early?",
      "page_reference": 2,
      "response": "**Risk factors:** the agent’s **scope of actions**, the **reversibility** of those actions, and the agent’s **level of autonomy**.\n\n**Two planning-stage steps to manage risks early:**\n1. **Limit the agent’s scope of impact** by setting appropriate boundaries upfront (e.g., restricting access to tools and external systems).\n2. **Make actions traceable and controllable** by establishing robust **identity management and access controls** for agents.",
      "latency_ms": 4735.174894332886,
      "api_latency_ms": 4735.174894332886,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents go beyond generative AI by being able to:\n\n- **Take actions** (not just generate content), e.g., update a database or make a payment.  \n- **Adapt to new information** as they operate, adjusting their plans based on what they observe.  \n- **Interact with other agents and external systems/tools** to complete end-to-end tasks on behalf of humans."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The examples given are **coding assistants**, **customer service agents**, and **automating enterprise productivity workflows**."
        }
      ],
      "prompt_tokens": 781,
      "completion_tokens": 99,
      "cached_tokens": 0,
      "total_tokens": 880,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents go beyond generative AI by being able to:\n\n- **Take actions** (not just generate content), e.g., update a database or make a payment.  \n- **Adapt to new information** as they operate, adjusting their plans based on what they observe.  \n- **Interact with other agents and external systems/tools** to complete end-to-end tasks on behalf of humans."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The examples given are **coding assistants**, **customer service agents**, and **automating enterprise productivity workflows**."
        },
        {
          "role": "user",
          "content": "What factors determine the risks posed by an agent’s actions, and what are two planning-stage steps organisations can take to manage those risks early?"
        },
        {
          "role": "assistant",
          "content": "**Risk factors:** the agent’s **scope of actions**, the **reversibility** of those actions, and the agent’s **level of autonomy**.\n\n**Two planning-stage steps to manage risks early:**\n1. **Limit the agent’s scope of impact** by setting appropriate boundaries upfront (e.g., restricting access to tools and external systems).\n2. **Make actions traceable and controllable** by establishing robust **identity management and access controls** for agents."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-003",
      "turn_number": 1,
      "question": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?",
      "page_reference": 3,
      "response": "“Human-in-the-loop” should be adapted to explicitly counter automation bias by:\n\n- **Defining “significant checkpoints” in the agent workflow that require human approval**, especially for **high-stakes or irreversible actions**.\n- **Regularly auditing human oversight** to verify that the human review remains **meaningful and effective over time**, rather than becoming a rubber-stamp as agents appear more capable.",
      "latency_ms": 11494.032144546509,
      "api_latency_ms": 11494.032144546509,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 496,
      "completion_tokens": 85,
      "cached_tokens": 0,
      "total_tokens": 581,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to explicitly counter automation bias by:\n\n- **Defining “significant checkpoints” in the agent workflow that require human approval**, especially for **high-stakes or irreversible actions**.\n- **Regularly auditing human oversight** to verify that the human review remains **meaningful and effective over time**, rather than becoming a rubber-stamp as agents appear more capable."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-003",
      "turn_number": 2,
      "question": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?",
      "page_reference": 3,
      "response": "- **Checkpoints requiring human approval:** “Significant checkpoints” in the agentic workflow—especially **high-stakes** decisions or **irreversible actions** (e.g., committing funds, sending external communications, changing production systems/data, executing destructive operations, or taking actions with legal/safety impact).\n\n- **Ongoing recommended activity:** **Regularly audit human oversight** to confirm reviewers are actually catching issues and exercising independent judgment (i.e., that oversight hasn’t degraded into automation-biased rubber-stamping).",
      "latency_ms": 5546.816825866699,
      "api_latency_ms": 5546.816825866699,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to explicitly counter automation bias by:\n\n- **Defining “significant checkpoints” in the agent workflow that require human approval**, especially for **high-stakes or irreversible actions**.\n- **Regularly auditing human oversight** to verify that the human review remains **meaningful and effective over time**, rather than becoming a rubber-stamp as agents appear more capable."
        }
      ],
      "prompt_tokens": 616,
      "completion_tokens": 107,
      "cached_tokens": 0,
      "total_tokens": 723,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to explicitly counter automation bias by:\n\n- **Defining “significant checkpoints” in the agent workflow that require human approval**, especially for **high-stakes or irreversible actions**.\n- **Regularly auditing human oversight** to verify that the human review remains **meaningful and effective over time**, rather than becoming a rubber-stamp as agents appear more capable."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "- **Checkpoints requiring human approval:** “Significant checkpoints” in the agentic workflow—especially **high-stakes** decisions or **irreversible actions** (e.g., committing funds, sending external communications, changing production systems/data, executing destructive operations, or taking actions with legal/safety impact).\n\n- **Ongoing recommended activity:** **Regularly audit human oversight** to confirm reviewers are actually catching issues and exercising independent judgment (i.e., that oversight hasn’t degraded into automation-biased rubber-stamping)."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-003",
      "turn_number": 3,
      "question": "What is the recommended approach during and after deployment to handle the fact that agents interact dynamically with their environment and not all risks can be anticipated upfront?",
      "page_reference": 3,
      "response": "During and after deployment, the recommendation is to **gradually roll out agents** and pair that with **continuous post-deployment monitoring**, since agents’ real-world interactions can surface risks that weren’t foreseeable upfront.",
      "latency_ms": 1952.7413845062256,
      "api_latency_ms": 1952.7413845062256,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to explicitly counter automation bias by:\n\n- **Defining “significant checkpoints” in the agent workflow that require human approval**, especially for **high-stakes or irreversible actions**.\n- **Regularly auditing human oversight** to verify that the human review remains **meaningful and effective over time**, rather than becoming a rubber-stamp as agents appear more capable."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "- **Checkpoints requiring human approval:** “Significant checkpoints” in the agentic workflow—especially **high-stakes** decisions or **irreversible actions** (e.g., committing funds, sending external communications, changing production systems/data, executing destructive operations, or taking actions with legal/safety impact).\n\n- **Ongoing recommended activity:** **Regularly audit human oversight** to confirm reviewers are actually catching issues and exercising independent judgment (i.e., that oversight hasn’t degraded into automation-biased rubber-stamping)."
        }
      ],
      "prompt_tokens": 758,
      "completion_tokens": 46,
      "cached_tokens": 0,
      "total_tokens": 804,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to explicitly counter automation bias by:\n\n- **Defining “significant checkpoints” in the agent workflow that require human approval**, especially for **high-stakes or irreversible actions**.\n- **Regularly auditing human oversight** to verify that the human review remains **meaningful and effective over time**, rather than becoming a rubber-stamp as agents appear more capable."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "- **Checkpoints requiring human approval:** “Significant checkpoints” in the agentic workflow—especially **high-stakes** decisions or **irreversible actions** (e.g., committing funds, sending external communications, changing production systems/data, executing destructive operations, or taking actions with legal/safety impact).\n\n- **Ongoing recommended activity:** **Regularly audit human oversight** to confirm reviewers are actually catching issues and exercising independent judgment (i.e., that oversight hasn’t degraded into automation-biased rubber-stamping)."
        },
        {
          "role": "user",
          "content": "What is the recommended approach during and after deployment to handle the fact that agents interact dynamically with their environment and not all risks can be anticipated upfront?"
        },
        {
          "role": "assistant",
          "content": "During and after deployment, the recommendation is to **gradually roll out agents** and pair that with **continuous post-deployment monitoring**, since agents’ real-world interactions can surface risks that weren’t foreseeable upfront."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-004",
      "turn_number": 1,
      "question": "What are agentic AI systems, and what common features do agents usually have?",
      "page_reference": 4,
      "response": "Agentic AI systems are systems that use AI agents to plan across multiple steps to achieve specified objectives.\n\nThere’s no single agreed definition of an “agent,” but agents usually share these common features:\n- **Independent planning and decision-making** over multiple steps toward a user-defined goal.\n- **Action-taking capability**, often by interacting with external resources or systems (e.g., searching the web, creating files, using tools/APIs) rather than only producing a single response.",
      "latency_ms": 12885.723114013672,
      "api_latency_ms": 12885.723114013672,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 475,
      "completion_tokens": 98,
      "cached_tokens": 0,
      "total_tokens": 573,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that use AI agents to plan across multiple steps to achieve specified objectives.\n\nThere’s no single agreed definition of an “agent,” but agents usually share these common features:\n- **Independent planning and decision-making** over multiple steps toward a user-defined goal.\n- **Action-taking capability**, often by interacting with external resources or systems (e.g., searching the web, creating files, using tools/APIs) rather than only producing a single response."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-004",
      "turn_number": 2,
      "question": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?",
      "page_reference": 4,
      "response": "In this framework, the agent’s “brain” is the **model**—the central reasoning and planning engine that interprets user inputs, follows instructions, and generates appropriate outputs.\n\nThe model can be a:\n- **Small Language Model (SLM)**\n- **Large Language Model (LLM)**\n- **Multimodal Large Language Model (MLLM)**",
      "latency_ms": 4154.128551483154,
      "api_latency_ms": 4154.128551483154,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that use AI agents to plan across multiple steps to achieve specified objectives.\n\nThere’s no single agreed definition of an “agent,” but agents usually share these common features:\n- **Independent planning and decision-making** over multiple steps toward a user-defined goal.\n- **Action-taking capability**, often by interacting with external resources or systems (e.g., searching the web, creating files, using tools/APIs) rather than only producing a single response."
        }
      ],
      "prompt_tokens": 606,
      "completion_tokens": 78,
      "cached_tokens": 0,
      "total_tokens": 684,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that use AI agents to plan across multiple steps to achieve specified objectives.\n\nThere’s no single agreed definition of an “agent,” but agents usually share these common features:\n- **Independent planning and decision-making** over multiple steps toward a user-defined goal.\n- **Action-taking capability**, often by interacting with external resources or systems (e.g., searching the web, creating files, using tools/APIs) rather than only producing a single response."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In this framework, the agent’s “brain” is the **model**—the central reasoning and planning engine that interprets user inputs, follows instructions, and generates appropriate outputs.\n\nThe model can be a:\n- **Small Language Model (SLM)**\n- **Large Language Model (LLM)**\n- **Multimodal Large Language Model (MLLM)**"
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-004",
      "turn_number": 3,
      "question": "What is the role of the model component in a simple LLM-based app, and what does it do with user inputs and instructions?",
      "page_reference": 4,
      "response": "In a simple LLM-based app, the **model** is the central reasoning and planning engine (the “brain”). Its role is to **process instructions**, **interpret user inputs**, and **generate contextually appropriate responses** based on them.",
      "latency_ms": 1611.3150119781494,
      "api_latency_ms": 1611.3150119781494,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that use AI agents to plan across multiple steps to achieve specified objectives.\n\nThere’s no single agreed definition of an “agent,” but agents usually share these common features:\n- **Independent planning and decision-making** over multiple steps toward a user-defined goal.\n- **Action-taking capability**, often by interacting with external resources or systems (e.g., searching the web, creating files, using tools/APIs) rather than only producing a single response."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In this framework, the agent’s “brain” is the **model**—the central reasoning and planning engine that interprets user inputs, follows instructions, and generates appropriate outputs.\n\nThe model can be a:\n- **Small Language Model (SLM)**\n- **Large Language Model (LLM)**\n- **Multimodal Large Language Model (MLLM)**"
        }
      ],
      "prompt_tokens": 717,
      "completion_tokens": 53,
      "cached_tokens": 0,
      "total_tokens": 770,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that use AI agents to plan across multiple steps to achieve specified objectives.\n\nThere’s no single agreed definition of an “agent,” but agents usually share these common features:\n- **Independent planning and decision-making** over multiple steps toward a user-defined goal.\n- **Action-taking capability**, often by interacting with external resources or systems (e.g., searching the web, creating files, using tools/APIs) rather than only producing a single response."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In this framework, the agent’s “brain” is the **model**—the central reasoning and planning engine that interprets user inputs, follows instructions, and generates appropriate outputs.\n\nThe model can be a:\n- **Small Language Model (SLM)**\n- **Large Language Model (LLM)**\n- **Multimodal Large Language Model (MLLM)**"
        },
        {
          "role": "user",
          "content": "What is the role of the model component in a simple LLM-based app, and what does it do with user inputs and instructions?"
        },
        {
          "role": "assistant",
          "content": "In a simple LLM-based app, the **model** is the central reasoning and planning engine (the “brain”). Its role is to **process instructions**, **interpret user inputs**, and **generate contextually appropriate responses** based on them."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-005",
      "turn_number": 1,
      "question": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?",
      "page_reference": 5,
      "response": "In an agentic system, **tools** are the components that let an agent **take actions in the world** rather than only generate text. The model can *call* these tools to execute steps needed to complete a task.\n\nTools can enable actions such as:\n\n- **Writing to and reading from files** (create/edit documents, logs, configs)\n- **Interacting with databases** (querying, inserting, updating records)\n- **Calling external services/APIs** (fetching data, triggering workflows, integrating with other software)\n- **Controlling devices or software** (operating applications, IoT devices, system settings)\n- **Performing transactions** (e.g., submitting orders, initiating payments, executing approved business operations)\n\nIn short, tools expand an agent’s **action-space/authority**—what it is permitted and able to do—by giving it controlled interfaces to other systems.",
      "latency_ms": 17039.851665496826,
      "api_latency_ms": 17039.851665496826,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 620,
      "completion_tokens": 186,
      "cached_tokens": 0,
      "total_tokens": 806,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are the components that let an agent **take actions in the world** rather than only generate text. The model can *call* these tools to execute steps needed to complete a task.\n\nTools can enable actions such as:\n\n- **Writing to and reading from files** (create/edit documents, logs, configs)\n- **Interacting with databases** (querying, inserting, updating records)\n- **Calling external services/APIs** (fetching data, triggering workflows, integrating with other software)\n- **Controlling devices or software** (operating applications, IoT devices, system settings)\n- **Performing transactions** (e.g., submitting orders, initiating payments, executing approved business operations)\n\nIn short, tools expand an agent’s **action-space/authority**—what it is permitted and able to do—by giving it controlled interfaces to other systems."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-005",
      "turn_number": 2,
      "question": "How does the model use tools to complete a task?",
      "page_reference": 5,
      "response": "The model uses tools by **deciding when an external action is needed**, then **issuing a tool call** with the required inputs, and finally **using the tool’s output** to continue planning and produce the result.\n\nTypical flow:\n\n1. **Plan/decide**: The model reasons about the task and identifies a step it can’t reliably do with text alone (e.g., fetch data, write a file, run a query).\n2. **Select a tool**: It chooses an appropriate permitted tool (based on its available action-space).\n3. **Call the tool**: It outputs a structured request specifying the tool name and parameters (e.g., a database query, API endpoint + arguments).\n4. **Observe results**: The tool executes in the external system and returns results (data, confirmation, errors).\n5. **Integrate and continue**: The model incorporates the returned information, updates its plan if needed, and may call more tools until the task is complete.",
      "latency_ms": 5501.856327056885,
      "api_latency_ms": 5500.8649826049805,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are the components that let an agent **take actions in the world** rather than only generate text. The model can *call* these tools to execute steps needed to complete a task.\n\nTools can enable actions such as:\n\n- **Writing to and reading from files** (create/edit documents, logs, configs)\n- **Interacting with databases** (querying, inserting, updating records)\n- **Calling external services/APIs** (fetching data, triggering workflows, integrating with other software)\n- **Controlling devices or software** (operating applications, IoT devices, system settings)\n- **Performing transactions** (e.g., submitting orders, initiating payments, executing approved business operations)\n\nIn short, tools expand an agent’s **action-space/authority**—what it is permitted and able to do—by giving it controlled interfaces to other systems."
        }
      ],
      "prompt_tokens": 823,
      "completion_tokens": 205,
      "cached_tokens": 0,
      "total_tokens": 1028,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are the components that let an agent **take actions in the world** rather than only generate text. The model can *call* these tools to execute steps needed to complete a task.\n\nTools can enable actions such as:\n\n- **Writing to and reading from files** (create/edit documents, logs, configs)\n- **Interacting with databases** (querying, inserting, updating records)\n- **Calling external services/APIs** (fetching data, triggering workflows, integrating with other software)\n- **Controlling devices or software** (operating applications, IoT devices, system settings)\n- **Performing transactions** (e.g., submitting orders, initiating payments, executing approved business operations)\n\nIn short, tools expand an agent’s **action-space/authority**—what it is permitted and able to do—by giving it controlled interfaces to other systems."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools by **deciding when an external action is needed**, then **issuing a tool call** with the required inputs, and finally **using the tool’s output** to continue planning and produce the result.\n\nTypical flow:\n\n1. **Plan/decide**: The model reasons about the task and identifies a step it can’t reliably do with text alone (e.g., fetch data, write a file, run a query).\n2. **Select a tool**: It chooses an appropriate permitted tool (based on its available action-space).\n3. **Call the tool**: It outputs a structured request specifying the tool name and parameters (e.g., a database query, API endpoint + arguments).\n4. **Observe results**: The tool executes in the external system and returns results (data, confirmation, errors).\n5. **Integrate and continue**: The model incorporates the returned information, updates its plan if needed, and may call more tools until the task is complete."
        }
      ]
    },
    {
      "model_name": "gpt-5.2",
      "conversation_id": "conv-005",
      "turn_number": 3,
      "question": "What is action-space, and what determines the range of actions an agent is permitted to take?",
      "page_reference": 5,
      "response": "**Action-space** (also called an agent’s **authority** or **capabilities**) is the **set of actions the agent is able and allowed to take** in an agentic system—i.e., what it can actually do beyond generating text.\n\nThe range of permitted actions is mainly determined by:\n\n- **Which tools the agent is given access to** (e.g., file writing, database access, external APIs, device control tools).\n- **What those tools allow the agent to do** (their built-in functions and limits).\n- **Permissions/constraints on tool use**, such as:\n  - access controls and roles (read-only vs write/admin),\n  - transaction limits/approval requirements,\n  - sandboxing and environment restrictions,\n  - policy rules encoded in the agent’s instructions (what it is not allowed to do).\n\nSo, an agent’s action-space is defined by its **tooling plus the permissions and rules governing those tools and transactions**.",
      "latency_ms": 9768.194675445557,
      "api_latency_ms": 9768.194675445557,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are the components that let an agent **take actions in the world** rather than only generate text. The model can *call* these tools to execute steps needed to complete a task.\n\nTools can enable actions such as:\n\n- **Writing to and reading from files** (create/edit documents, logs, configs)\n- **Interacting with databases** (querying, inserting, updating records)\n- **Calling external services/APIs** (fetching data, triggering workflows, integrating with other software)\n- **Controlling devices or software** (operating applications, IoT devices, system settings)\n- **Performing transactions** (e.g., submitting orders, initiating payments, executing approved business operations)\n\nIn short, tools expand an agent’s **action-space/authority**—what it is permitted and able to do—by giving it controlled interfaces to other systems."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools by **deciding when an external action is needed**, then **issuing a tool call** with the required inputs, and finally **using the tool’s output** to continue planning and produce the result.\n\nTypical flow:\n\n1. **Plan/decide**: The model reasons about the task and identifies a step it can’t reliably do with text alone (e.g., fetch data, write a file, run a query).\n2. **Select a tool**: It chooses an appropriate permitted tool (based on its available action-space).\n3. **Call the tool**: It outputs a structured request specifying the tool name and parameters (e.g., a database query, API endpoint + arguments).\n4. **Observe results**: The tool executes in the external system and returns results (data, confirmation, errors).\n5. **Integrate and continue**: The model incorporates the returned information, updates its plan if needed, and may call more tools until the task is complete."
        }
      ],
      "prompt_tokens": 1053,
      "completion_tokens": 198,
      "cached_tokens": 0,
      "total_tokens": 1251,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are the components that let an agent **take actions in the world** rather than only generate text. The model can *call* these tools to execute steps needed to complete a task.\n\nTools can enable actions such as:\n\n- **Writing to and reading from files** (create/edit documents, logs, configs)\n- **Interacting with databases** (querying, inserting, updating records)\n- **Calling external services/APIs** (fetching data, triggering workflows, integrating with other software)\n- **Controlling devices or software** (operating applications, IoT devices, system settings)\n- **Performing transactions** (e.g., submitting orders, initiating payments, executing approved business operations)\n\nIn short, tools expand an agent’s **action-space/authority**—what it is permitted and able to do—by giving it controlled interfaces to other systems."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools by **deciding when an external action is needed**, then **issuing a tool call** with the required inputs, and finally **using the tool’s output** to continue planning and produce the result.\n\nTypical flow:\n\n1. **Plan/decide**: The model reasons about the task and identifies a step it can’t reliably do with text alone (e.g., fetch data, write a file, run a query).\n2. **Select a tool**: It chooses an appropriate permitted tool (based on its available action-space).\n3. **Call the tool**: It outputs a structured request specifying the tool name and parameters (e.g., a database query, API endpoint + arguments).\n4. **Observe results**: The tool executes in the external system and returns results (data, confirmation, errors).\n5. **Integrate and continue**: The model incorporates the returned information, updates its plan if needed, and may call more tools until the task is complete."
        },
        {
          "role": "user",
          "content": "What is action-space, and what determines the range of actions an agent is permitted to take?"
        },
        {
          "role": "assistant",
          "content": "**Action-space** (also called an agent’s **authority** or **capabilities**) is the **set of actions the agent is able and allowed to take** in an agentic system—i.e., what it can actually do beyond generating text.\n\nThe range of permitted actions is mainly determined by:\n\n- **Which tools the agent is given access to** (e.g., file writing, database access, external APIs, device control tools).\n- **What those tools allow the agent to do** (their built-in functions and limits).\n- **Permissions/constraints on tool use**, such as:\n  - access controls and roles (read-only vs write/admin),\n  - transaction limits/approval requirements,\n  - sandboxing and environment restrictions,\n  - policy rules encoded in the agent’s instructions (what it is not allowed to do).\n\nSo, an agent’s action-space is defined by its **tooling plus the permissions and rules governing those tools and transactions**."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-001",
      "turn_number": 1,
      "question": "What topics are covered under the Model AI Governance Framework for Agentic AI?",
      "page_reference": 1,
      "response": "The “Model AI Governance Framework for Agentic AI” in your document covers four main topic areas (Section 2):\n\n1. **Assess and bound the risks upfront (2.1)**  \n   - 2.1.1 Determine suitable use cases for agent deployment  \n   - 2.1.2 Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable (2.2)**  \n   - 2.2.1 Clear allocation of responsibilities within and outside the organisation  \n   - 2.2.2 Design for meaningful human oversight  \n\n3. **Implement technical controls and processes (2.3)**  \n   - 2.3.1 During design and development, use technical controls  \n   - 2.3.2 Before deploying, test agents  \n   - 2.3.3 When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility (2.4)**  \n   - 2.4.1 Different users, different needs  \n   - 2.4.2 Users who interact with agents  \n   - 2.4.3 Users who integrate agents into their work processes",
      "latency_ms": 10354.921102523804,
      "api_latency_ms": 10354.921102523804,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 467,
      "completion_tokens": 249,
      "cached_tokens": 0,
      "total_tokens": 716,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The “Model AI Governance Framework for Agentic AI” in your document covers four main topic areas (Section 2):\n\n1. **Assess and bound the risks upfront (2.1)**  \n   - 2.1.1 Determine suitable use cases for agent deployment  \n   - 2.1.2 Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable (2.2)**  \n   - 2.2.1 Clear allocation of responsibilities within and outside the organisation  \n   - 2.2.2 Design for meaningful human oversight  \n\n3. **Implement technical controls and processes (2.3)**  \n   - 2.3.1 During design and development, use technical controls  \n   - 2.3.2 Before deploying, test agents  \n   - 2.3.3 When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility (2.4)**  \n   - 2.4.1 Different users, different needs  \n   - 2.4.2 Users who interact with agents  \n   - 2.4.3 Users who integrate agents into their work processes"
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-001",
      "turn_number": 2,
      "question": "Within that framework, what are the specific steps listed under implementing technical controls and processes?",
      "page_reference": 1,
      "response": "Within the “Implement technical controls and processes” section (2.3), the framework lists three specific steps:\n\n1. **During design and development, use technical controls (2.3.1)**  \n2. **Before deploying, test agents (2.3.2)**  \n3. **When deploying, continuously monitor and test (2.3.3)**",
      "latency_ms": 1417.7272319793701,
      "api_latency_ms": 1417.7272319793701,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The “Model AI Governance Framework for Agentic AI” in your document covers four main topic areas (Section 2):\n\n1. **Assess and bound the risks upfront (2.1)**  \n   - 2.1.1 Determine suitable use cases for agent deployment  \n   - 2.1.2 Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable (2.2)**  \n   - 2.2.1 Clear allocation of responsibilities within and outside the organisation  \n   - 2.2.2 Design for meaningful human oversight  \n\n3. **Implement technical controls and processes (2.3)**  \n   - 2.3.1 During design and development, use technical controls  \n   - 2.3.2 Before deploying, test agents  \n   - 2.3.3 When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility (2.4)**  \n   - 2.4.1 Different users, different needs  \n   - 2.4.2 Users who interact with agents  \n   - 2.4.3 Users who integrate agents into their work processes"
        }
      ],
      "prompt_tokens": 733,
      "completion_tokens": 84,
      "cached_tokens": 0,
      "total_tokens": 817,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The “Model AI Governance Framework for Agentic AI” in your document covers four main topic areas (Section 2):\n\n1. **Assess and bound the risks upfront (2.1)**  \n   - 2.1.1 Determine suitable use cases for agent deployment  \n   - 2.1.2 Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable (2.2)**  \n   - 2.2.1 Clear allocation of responsibilities within and outside the organisation  \n   - 2.2.2 Design for meaningful human oversight  \n\n3. **Implement technical controls and processes (2.3)**  \n   - 2.3.1 During design and development, use technical controls  \n   - 2.3.2 Before deploying, test agents  \n   - 2.3.3 When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility (2.4)**  \n   - 2.4.1 Different users, different needs  \n   - 2.4.2 Users who interact with agents  \n   - 2.4.3 Users who integrate agents into their work processes"
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Within the “Implement technical controls and processes” section (2.3), the framework lists three specific steps:\n\n1. **During design and development, use technical controls (2.3.1)**  \n2. **Before deploying, test agents (2.3.2)**  \n3. **When deploying, continuously monitor and test (2.3.3)**"
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-001",
      "turn_number": 3,
      "question": "How does the framework describe enabling end-user responsibility in terms of different user types?",
      "page_reference": 1,
      "response": "The framework describes enabling end‑user responsibility by distinguishing between different types of users and tailoring guidance to each:\n\n1. **Different users, different needs (2.4.1)**  \n   - Recognises that “end users” are not a single group.  \n   - Emphasises that responsibilities, safeguards, and support must be calibrated to the user’s role, capabilities, and how they interact with the agent.\n\n2. **Users who interact with agents (2.4.2)**  \n   - These are users who *use* an agent’s interface or outputs (e.g., employees or customers using an AI assistant).  \n   - The framework focuses on informing them about:  \n     - What the agent can and cannot do  \n     - How to use it appropriately  \n     - Their responsibility to review and verify important outputs  \n     - How to report issues or unexpected behaviour\n\n3. **Users who integrate agents into their work processes (2.4.3)**  \n   - These are users who *embed* agents into workflows, tools, or services (e.g., product teams, process owners, developers).  \n   - The framework stresses their responsibility to:  \n     - Assess risks in their specific context  \n     - Configure limits, permissions, and safeguards appropriately  \n     - Ensure suitable human oversight and escalation paths  \n     - Provide guidance and training to downstream users who will interact with the integrated agent.",
      "latency_ms": 5268.200635910034,
      "api_latency_ms": 5268.200635910034,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 2.0,
        "relevance": 4.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The “Model AI Governance Framework for Agentic AI” in your document covers four main topic areas (Section 2):\n\n1. **Assess and bound the risks upfront (2.1)**  \n   - 2.1.1 Determine suitable use cases for agent deployment  \n   - 2.1.2 Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable (2.2)**  \n   - 2.2.1 Clear allocation of responsibilities within and outside the organisation  \n   - 2.2.2 Design for meaningful human oversight  \n\n3. **Implement technical controls and processes (2.3)**  \n   - 2.3.1 During design and development, use technical controls  \n   - 2.3.2 Before deploying, test agents  \n   - 2.3.3 When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility (2.4)**  \n   - 2.4.1 Different users, different needs  \n   - 2.4.2 Users who interact with agents  \n   - 2.4.3 Users who integrate agents into their work processes"
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Within the “Implement technical controls and processes” section (2.3), the framework lists three specific steps:\n\n1. **During design and development, use technical controls (2.3.1)**  \n2. **Before deploying, test agents (2.3.2)**  \n3. **When deploying, continuously monitor and test (2.3.3)**"
        }
      ],
      "prompt_tokens": 833,
      "completion_tokens": 294,
      "cached_tokens": 0,
      "total_tokens": 1127,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The “Model AI Governance Framework for Agentic AI” in your document covers four main topic areas (Section 2):\n\n1. **Assess and bound the risks upfront (2.1)**  \n   - 2.1.1 Determine suitable use cases for agent deployment  \n   - 2.1.2 Bound risks through design by defining agents’ limits and permissions  \n\n2. **Make humans meaningfully accountable (2.2)**  \n   - 2.2.1 Clear allocation of responsibilities within and outside the organisation  \n   - 2.2.2 Design for meaningful human oversight  \n\n3. **Implement technical controls and processes (2.3)**  \n   - 2.3.1 During design and development, use technical controls  \n   - 2.3.2 Before deploying, test agents  \n   - 2.3.3 When deploying, continuously monitor and test  \n\n4. **Enable end-user responsibility (2.4)**  \n   - 2.4.1 Different users, different needs  \n   - 2.4.2 Users who interact with agents  \n   - 2.4.3 Users who integrate agents into their work processes"
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Within the “Implement technical controls and processes” section (2.3), the framework lists three specific steps:\n\n1. **During design and development, use technical controls (2.3.1)**  \n2. **Before deploying, test agents (2.3.2)**  \n3. **When deploying, continuously monitor and test (2.3.3)**"
        },
        {
          "role": "user",
          "content": "How does the framework describe enabling end-user responsibility in terms of different user types?"
        },
        {
          "role": "assistant",
          "content": "The framework describes enabling end‑user responsibility by distinguishing between different types of users and tailoring guidance to each:\n\n1. **Different users, different needs (2.4.1)**  \n   - Recognises that “end users” are not a single group.  \n   - Emphasises that responsibilities, safeguards, and support must be calibrated to the user’s role, capabilities, and how they interact with the agent.\n\n2. **Users who interact with agents (2.4.2)**  \n   - These are users who *use* an agent’s interface or outputs (e.g., employees or customers using an AI assistant).  \n   - The framework focuses on informing them about:  \n     - What the agent can and cannot do  \n     - How to use it appropriately  \n     - Their responsibility to review and verify important outputs  \n     - How to report issues or unexpected behaviour\n\n3. **Users who integrate agents into their work processes (2.4.3)**  \n   - These are users who *embed* agents into workflows, tools, or services (e.g., product teams, process owners, developers).  \n   - The framework stresses their responsibility to:  \n     - Assess risks in their specific context  \n     - Configure limits, permissions, and safeguards appropriately  \n     - Ensure suitable human oversight and escalation paths  \n     - Provide guidance and training to downstream users who will interact with the integrated agent."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-002",
      "turn_number": 1,
      "question": "What capabilities distinguish AI agents from generative AI?",
      "page_reference": 2,
      "response": "AI agents are distinguished from “plain” generative AI by what they can *do* in the world, not just what they can *produce*. Key differences:\n\n1. **Action-taking vs. content generation**  \n   - **Generative AI**: Primarily produces outputs (text, images, code, etc.) in response to prompts. It stops there.  \n   - **AI agents**: Can *take actions* based on those outputs—e.g., send emails, update a database, file a ticket, make an API call, or trigger workflows in other systems.\n\n2. **Tool and system integration**  \n   - Generative models: Usually confined to a chat or generation interface unless a human manually copies outputs into other tools.  \n   - Agents: Are wired into tools and systems (APIs, enterprise apps, payment systems, CRMs) and can call them directly to complete tasks end to end.\n\n3. **Autonomy and multi-step workflows**  \n   - Generative AI: Responds one prompt at a time; each step is usually initiated by a person.  \n   - Agents: Can plan and execute multi-step workflows, decide intermediate steps, and iterate until a goal is reached (within boundaries you set).\n\n4. **Adaptation to new information**  \n   - Generative AI: Answers based on its training data and current prompt; it doesn’t maintain an ongoing “situation model” across tools and systems.  \n   - Agents: Can observe results of their actions (e.g., “API call failed”, “payment declined”), adapt their plan, and try alternative paths.\n\n5. **Interaction with other agents and services**  \n   - Generative AI: Typically interacts only with the user.  \n   - Agents: Can coordinate with other agents or services—e.g., a customer service agent handing off to a billing agent, or a coding agent working with a testing agent.\n\n6. **Persistent identity and access**  \n   - Generative AI: No real “account” or identity in your systems; it’s a model you query.  \n   - Agents: May have identities, roles, and permissions managed like user accounts (with access controls, audit logs, and traceability of actions).\n\nBecause of these capabilities, AI agents can transform workflows (coding assistants that modify codebases, customer service agents that actually resolve issues, productivity agents that update records and send communications), but they also introduce new risks around data access, unintended actions, and accountability—requiring stronger governance and oversight than standard generative AI.",
      "latency_ms": 17039.191007614136,
      "api_latency_ms": 17039.191007614136,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 620,
      "completion_tokens": 524,
      "cached_tokens": 0,
      "total_tokens": 1144,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from “plain” generative AI by what they can *do* in the world, not just what they can *produce*. Key differences:\n\n1. **Action-taking vs. content generation**  \n   - **Generative AI**: Primarily produces outputs (text, images, code, etc.) in response to prompts. It stops there.  \n   - **AI agents**: Can *take actions* based on those outputs—e.g., send emails, update a database, file a ticket, make an API call, or trigger workflows in other systems.\n\n2. **Tool and system integration**  \n   - Generative models: Usually confined to a chat or generation interface unless a human manually copies outputs into other tools.  \n   - Agents: Are wired into tools and systems (APIs, enterprise apps, payment systems, CRMs) and can call them directly to complete tasks end to end.\n\n3. **Autonomy and multi-step workflows**  \n   - Generative AI: Responds one prompt at a time; each step is usually initiated by a person.  \n   - Agents: Can plan and execute multi-step workflows, decide intermediate steps, and iterate until a goal is reached (within boundaries you set).\n\n4. **Adaptation to new information**  \n   - Generative AI: Answers based on its training data and current prompt; it doesn’t maintain an ongoing “situation model” across tools and systems.  \n   - Agents: Can observe results of their actions (e.g., “API call failed”, “payment declined”), adapt their plan, and try alternative paths.\n\n5. **Interaction with other agents and services**  \n   - Generative AI: Typically interacts only with the user.  \n   - Agents: Can coordinate with other agents or services—e.g., a customer service agent handing off to a billing agent, or a coding agent working with a testing agent.\n\n6. **Persistent identity and access**  \n   - Generative AI: No real “account” or identity in your systems; it’s a model you query.  \n   - Agents: May have identities, roles, and permissions managed like user accounts (with access controls, audit logs, and traceability of actions).\n\nBecause of these capabilities, AI agents can transform workflows (coding assistants that modify codebases, customer service agents that actually resolve issues, productivity agents that update records and send communications), but they also introduce new risks around data access, unintended actions, and accountability—requiring stronger governance and oversight than standard generative AI."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-002",
      "turn_number": 2,
      "question": "What workplace examples are given for how agents are already transforming work?",
      "page_reference": 2,
      "response": "The examples given are:\n\n- **Coding assistants**  \n- **Customer service agents**  \n- **Automating enterprise productivity workflows** (e.g., automating internal business processes and tasks)",
      "latency_ms": 1614.3205165863037,
      "api_latency_ms": 1614.3205165863037,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from “plain” generative AI by what they can *do* in the world, not just what they can *produce*. Key differences:\n\n1. **Action-taking vs. content generation**  \n   - **Generative AI**: Primarily produces outputs (text, images, code, etc.) in response to prompts. It stops there.  \n   - **AI agents**: Can *take actions* based on those outputs—e.g., send emails, update a database, file a ticket, make an API call, or trigger workflows in other systems.\n\n2. **Tool and system integration**  \n   - Generative models: Usually confined to a chat or generation interface unless a human manually copies outputs into other tools.  \n   - Agents: Are wired into tools and systems (APIs, enterprise apps, payment systems, CRMs) and can call them directly to complete tasks end to end.\n\n3. **Autonomy and multi-step workflows**  \n   - Generative AI: Responds one prompt at a time; each step is usually initiated by a person.  \n   - Agents: Can plan and execute multi-step workflows, decide intermediate steps, and iterate until a goal is reached (within boundaries you set).\n\n4. **Adaptation to new information**  \n   - Generative AI: Answers based on its training data and current prompt; it doesn’t maintain an ongoing “situation model” across tools and systems.  \n   - Agents: Can observe results of their actions (e.g., “API call failed”, “payment declined”), adapt their plan, and try alternative paths.\n\n5. **Interaction with other agents and services**  \n   - Generative AI: Typically interacts only with the user.  \n   - Agents: Can coordinate with other agents or services—e.g., a customer service agent handing off to a billing agent, or a coding agent working with a testing agent.\n\n6. **Persistent identity and access**  \n   - Generative AI: No real “account” or identity in your systems; it’s a model you query.  \n   - Agents: May have identities, roles, and permissions managed like user accounts (with access controls, audit logs, and traceability of actions).\n\nBecause of these capabilities, AI agents can transform workflows (coding assistants that modify codebases, customer service agents that actually resolve issues, productivity agents that update records and send communications), but they also introduce new risks around data access, unintended actions, and accountability—requiring stronger governance and oversight than standard generative AI."
        }
      ],
      "prompt_tokens": 1157,
      "completion_tokens": 48,
      "cached_tokens": 0,
      "total_tokens": 1205,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from “plain” generative AI by what they can *do* in the world, not just what they can *produce*. Key differences:\n\n1. **Action-taking vs. content generation**  \n   - **Generative AI**: Primarily produces outputs (text, images, code, etc.) in response to prompts. It stops there.  \n   - **AI agents**: Can *take actions* based on those outputs—e.g., send emails, update a database, file a ticket, make an API call, or trigger workflows in other systems.\n\n2. **Tool and system integration**  \n   - Generative models: Usually confined to a chat or generation interface unless a human manually copies outputs into other tools.  \n   - Agents: Are wired into tools and systems (APIs, enterprise apps, payment systems, CRMs) and can call them directly to complete tasks end to end.\n\n3. **Autonomy and multi-step workflows**  \n   - Generative AI: Responds one prompt at a time; each step is usually initiated by a person.  \n   - Agents: Can plan and execute multi-step workflows, decide intermediate steps, and iterate until a goal is reached (within boundaries you set).\n\n4. **Adaptation to new information**  \n   - Generative AI: Answers based on its training data and current prompt; it doesn’t maintain an ongoing “situation model” across tools and systems.  \n   - Agents: Can observe results of their actions (e.g., “API call failed”, “payment declined”), adapt their plan, and try alternative paths.\n\n5. **Interaction with other agents and services**  \n   - Generative AI: Typically interacts only with the user.  \n   - Agents: Can coordinate with other agents or services—e.g., a customer service agent handing off to a billing agent, or a coding agent working with a testing agent.\n\n6. **Persistent identity and access**  \n   - Generative AI: No real “account” or identity in your systems; it’s a model you query.  \n   - Agents: May have identities, roles, and permissions managed like user accounts (with access controls, audit logs, and traceability of actions).\n\nBecause of these capabilities, AI agents can transform workflows (coding assistants that modify codebases, customer service agents that actually resolve issues, productivity agents that update records and send communications), but they also introduce new risks around data access, unintended actions, and accountability—requiring stronger governance and oversight than standard generative AI."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The examples given are:\n\n- **Coding assistants**  \n- **Customer service agents**  \n- **Automating enterprise productivity workflows** (e.g., automating internal business processes and tasks)"
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-002",
      "turn_number": 3,
      "question": "What factors determine the risks posed by an agent’s actions, and what are two planning-stage steps organisations can take to manage those risks early?",
      "page_reference": 2,
      "response": "The risks posed by an agent’s actions depend on factors such as:\n\n- **The scope of actions** the agent can take (what systems it can touch, what it’s allowed to do).  \n- **The reversibility of those actions** (how easily mistakes can be undone).  \n- **The agent’s level of autonomy** (how much it can act without human intervention).\n\nTwo planning-stage steps organisations can take to manage these risks early are:\n\n1. **Limit the scope of impact** by designing appropriate boundaries, such as restricting the agent’s access to tools and external systems.  \n2. **Ensure actions are traceable and controllable** by establishing robust identity management and access controls for agents.",
      "latency_ms": 3430.018663406372,
      "api_latency_ms": 3430.018663406372,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from “plain” generative AI by what they can *do* in the world, not just what they can *produce*. Key differences:\n\n1. **Action-taking vs. content generation**  \n   - **Generative AI**: Primarily produces outputs (text, images, code, etc.) in response to prompts. It stops there.  \n   - **AI agents**: Can *take actions* based on those outputs—e.g., send emails, update a database, file a ticket, make an API call, or trigger workflows in other systems.\n\n2. **Tool and system integration**  \n   - Generative models: Usually confined to a chat or generation interface unless a human manually copies outputs into other tools.  \n   - Agents: Are wired into tools and systems (APIs, enterprise apps, payment systems, CRMs) and can call them directly to complete tasks end to end.\n\n3. **Autonomy and multi-step workflows**  \n   - Generative AI: Responds one prompt at a time; each step is usually initiated by a person.  \n   - Agents: Can plan and execute multi-step workflows, decide intermediate steps, and iterate until a goal is reached (within boundaries you set).\n\n4. **Adaptation to new information**  \n   - Generative AI: Answers based on its training data and current prompt; it doesn’t maintain an ongoing “situation model” across tools and systems.  \n   - Agents: Can observe results of their actions (e.g., “API call failed”, “payment declined”), adapt their plan, and try alternative paths.\n\n5. **Interaction with other agents and services**  \n   - Generative AI: Typically interacts only with the user.  \n   - Agents: Can coordinate with other agents or services—e.g., a customer service agent handing off to a billing agent, or a coding agent working with a testing agent.\n\n6. **Persistent identity and access**  \n   - Generative AI: No real “account” or identity in your systems; it’s a model you query.  \n   - Agents: May have identities, roles, and permissions managed like user accounts (with access controls, audit logs, and traceability of actions).\n\nBecause of these capabilities, AI agents can transform workflows (coding assistants that modify codebases, customer service agents that actually resolve issues, productivity agents that update records and send communications), but they also introduce new risks around data access, unintended actions, and accountability—requiring stronger governance and oversight than standard generative AI."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The examples given are:\n\n- **Coding assistants**  \n- **Customer service agents**  \n- **Automating enterprise productivity workflows** (e.g., automating internal business processes and tasks)"
        }
      ],
      "prompt_tokens": 1233,
      "completion_tokens": 152,
      "cached_tokens": 1024,
      "total_tokens": 1385,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from “plain” generative AI by what they can *do* in the world, not just what they can *produce*. Key differences:\n\n1. **Action-taking vs. content generation**  \n   - **Generative AI**: Primarily produces outputs (text, images, code, etc.) in response to prompts. It stops there.  \n   - **AI agents**: Can *take actions* based on those outputs—e.g., send emails, update a database, file a ticket, make an API call, or trigger workflows in other systems.\n\n2. **Tool and system integration**  \n   - Generative models: Usually confined to a chat or generation interface unless a human manually copies outputs into other tools.  \n   - Agents: Are wired into tools and systems (APIs, enterprise apps, payment systems, CRMs) and can call them directly to complete tasks end to end.\n\n3. **Autonomy and multi-step workflows**  \n   - Generative AI: Responds one prompt at a time; each step is usually initiated by a person.  \n   - Agents: Can plan and execute multi-step workflows, decide intermediate steps, and iterate until a goal is reached (within boundaries you set).\n\n4. **Adaptation to new information**  \n   - Generative AI: Answers based on its training data and current prompt; it doesn’t maintain an ongoing “situation model” across tools and systems.  \n   - Agents: Can observe results of their actions (e.g., “API call failed”, “payment declined”), adapt their plan, and try alternative paths.\n\n5. **Interaction with other agents and services**  \n   - Generative AI: Typically interacts only with the user.  \n   - Agents: Can coordinate with other agents or services—e.g., a customer service agent handing off to a billing agent, or a coding agent working with a testing agent.\n\n6. **Persistent identity and access**  \n   - Generative AI: No real “account” or identity in your systems; it’s a model you query.  \n   - Agents: May have identities, roles, and permissions managed like user accounts (with access controls, audit logs, and traceability of actions).\n\nBecause of these capabilities, AI agents can transform workflows (coding assistants that modify codebases, customer service agents that actually resolve issues, productivity agents that update records and send communications), but they also introduce new risks around data access, unintended actions, and accountability—requiring stronger governance and oversight than standard generative AI."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The examples given are:\n\n- **Coding assistants**  \n- **Customer service agents**  \n- **Automating enterprise productivity workflows** (e.g., automating internal business processes and tasks)"
        },
        {
          "role": "user",
          "content": "What factors determine the risks posed by an agent’s actions, and what are two planning-stage steps organisations can take to manage those risks early?"
        },
        {
          "role": "assistant",
          "content": "The risks posed by an agent’s actions depend on factors such as:\n\n- **The scope of actions** the agent can take (what systems it can touch, what it’s allowed to do).  \n- **The reversibility of those actions** (how easily mistakes can be undone).  \n- **The agent’s level of autonomy** (how much it can act without human intervention).\n\nTwo planning-stage steps organisations can take to manage these risks early are:\n\n1. **Limit the scope of impact** by designing appropriate boundaries, such as restricting the agent’s access to tools and external systems.  \n2. **Ensure actions are traceable and controllable** by establishing robust identity management and access controls for agents."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-003",
      "turn_number": 1,
      "question": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?",
      "page_reference": 3,
      "response": "“Human-in-the-loop” needs to be redesigned so that humans don’t just rubber‑stamp agent decisions. In the context you provided, that means:\n\n1. **Define explicit checkpoints for human approval**  \n   - Identify points in the agentic workflow where the stakes are high:  \n     - High‑impact financial decisions  \n     - Safety‑critical actions  \n     - Irreversible operations (e.g., deleting data, sending external communications, changing configurations)  \n   - Require human review and explicit approval at these checkpoints before the agent can proceed.\n\n2. **Design oversight to counter automation bias, not amplify it**  \n   - Present alternative options or rationales, not just the agent’s “best” recommendation, so humans are less likely to accept it blindly.  \n   - Provide uncertainty estimates, limitations, and relevant context so humans can critically assess the output.  \n   - Avoid UI patterns that over‑signal confidence (e.g., “recommended” badges without explanation).\n\n3. **Regularly audit human oversight itself**  \n   - Monitor how often humans override, modify, or question agent actions. A very low override rate can be a red flag for automation bias.  \n   - Periodically review samples of human‑approved actions to check whether oversight is substantive and effective.  \n   - Update training, workflows, and interfaces if audits show that humans are just clicking “approve.”\n\n4. **Train and support human reviewers**  \n   - Make clear that their role is to *critically evaluate* the agent, not to speed it through.  \n   - Provide guidance, checklists, or decision aids to help them spot errors, edge cases, and policy violations.  \n\nIn short, “human-in-the-loop” must shift from nominal sign‑off to structured, auditable, and well‑supported critical oversight, anchored at clearly defined high‑risk checkpoints and continually evaluated for effectiveness.",
      "latency_ms": 12450.852632522583,
      "api_latency_ms": 12450.852632522583,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 496,
      "completion_tokens": 391,
      "cached_tokens": 0,
      "total_tokens": 887,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” needs to be redesigned so that humans don’t just rubber‑stamp agent decisions. In the context you provided, that means:\n\n1. **Define explicit checkpoints for human approval**  \n   - Identify points in the agentic workflow where the stakes are high:  \n     - High‑impact financial decisions  \n     - Safety‑critical actions  \n     - Irreversible operations (e.g., deleting data, sending external communications, changing configurations)  \n   - Require human review and explicit approval at these checkpoints before the agent can proceed.\n\n2. **Design oversight to counter automation bias, not amplify it**  \n   - Present alternative options or rationales, not just the agent’s “best” recommendation, so humans are less likely to accept it blindly.  \n   - Provide uncertainty estimates, limitations, and relevant context so humans can critically assess the output.  \n   - Avoid UI patterns that over‑signal confidence (e.g., “recommended” badges without explanation).\n\n3. **Regularly audit human oversight itself**  \n   - Monitor how often humans override, modify, or question agent actions. A very low override rate can be a red flag for automation bias.  \n   - Periodically review samples of human‑approved actions to check whether oversight is substantive and effective.  \n   - Update training, workflows, and interfaces if audits show that humans are just clicking “approve.”\n\n4. **Train and support human reviewers**  \n   - Make clear that their role is to *critically evaluate* the agent, not to speed it through.  \n   - Provide guidance, checklists, or decision aids to help them spot errors, edge cases, and policy violations.  \n\nIn short, “human-in-the-loop” must shift from nominal sign‑off to structured, auditable, and well‑supported critical oversight, anchored at clearly defined high‑risk checkpoints and continually evaluated for effectiveness."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-003",
      "turn_number": 2,
      "question": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?",
      "page_reference": 3,
      "response": "Checkpoints and ongoing activity should be adapted in two ways:\n\n**1. Checkpoints that should require human approval**\n\nHuman sign‑off should be built into points in the agentic workflow where actions are:\n\n- **High‑stakes**  \n  - Large financial transactions or commitments  \n  - Safety‑critical decisions (health, security, legal exposure)  \n  - Actions affecting many users, customers, or employees at once  \n\n- **Irreversible or hard to undo**  \n  - Deleting or overwriting important data  \n  - Publishing or sending external communications (e.g., emails to clients, public posts, regulatory filings)  \n  - Changing production system configurations, access controls, or infrastructure  \n\n- **Policy- or ethics‑sensitive**  \n  - Anything touching on compliance (legal, regulatory, privacy, IP)  \n  - Use of sensitive or personal data  \n  - Decisions that could introduce bias, discrimination, or reputational risk  \n\nAt these checkpoints, the agent should pause and require explicit human review and approval before proceeding.\n\n**2. Ongoing activity to keep oversight effective**\n\nTo ensure humans don’t drift into rubber‑stamping and that automation bias is managed, organisations should:\n\n- **Regularly audit human oversight**  \n  - Sample and review human‑approved actions to see whether reviewers are catching issues.  \n  - Track override/modify rates; a consistently near‑zero override rate can signal blind trust.  \n  - Check that humans are actually examining rationales, evidence, and alternatives, not just clicking “approve.”\n\n- **Continuously improve oversight processes**  \n  - Update guidance, workflows, and training based on audit findings.  \n  - Adjust which checkpoints require approval as the agent’s capabilities, risks, and operating environment change.  \n\nThis combination—clear high‑risk checkpoints plus regular auditing and adjustment—helps “human‑in‑the‑loop” remain meaningful and effective as agents become more capable.",
      "latency_ms": 5165.985822677612,
      "api_latency_ms": 5165.985822677612,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” needs to be redesigned so that humans don’t just rubber‑stamp agent decisions. In the context you provided, that means:\n\n1. **Define explicit checkpoints for human approval**  \n   - Identify points in the agentic workflow where the stakes are high:  \n     - High‑impact financial decisions  \n     - Safety‑critical actions  \n     - Irreversible operations (e.g., deleting data, sending external communications, changing configurations)  \n   - Require human review and explicit approval at these checkpoints before the agent can proceed.\n\n2. **Design oversight to counter automation bias, not amplify it**  \n   - Present alternative options or rationales, not just the agent’s “best” recommendation, so humans are less likely to accept it blindly.  \n   - Provide uncertainty estimates, limitations, and relevant context so humans can critically assess the output.  \n   - Avoid UI patterns that over‑signal confidence (e.g., “recommended” badges without explanation).\n\n3. **Regularly audit human oversight itself**  \n   - Monitor how often humans override, modify, or question agent actions. A very low override rate can be a red flag for automation bias.  \n   - Periodically review samples of human‑approved actions to check whether oversight is substantive and effective.  \n   - Update training, workflows, and interfaces if audits show that humans are just clicking “approve.”\n\n4. **Train and support human reviewers**  \n   - Make clear that their role is to *critically evaluate* the agent, not to speed it through.  \n   - Provide guidance, checklists, or decision aids to help them spot errors, edge cases, and policy violations.  \n\nIn short, “human-in-the-loop” must shift from nominal sign‑off to structured, auditable, and well‑supported critical oversight, anchored at clearly defined high‑risk checkpoints and continually evaluated for effectiveness."
        }
      ],
      "prompt_tokens": 916,
      "completion_tokens": 401,
      "cached_tokens": 0,
      "total_tokens": 1317,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” needs to be redesigned so that humans don’t just rubber‑stamp agent decisions. In the context you provided, that means:\n\n1. **Define explicit checkpoints for human approval**  \n   - Identify points in the agentic workflow where the stakes are high:  \n     - High‑impact financial decisions  \n     - Safety‑critical actions  \n     - Irreversible operations (e.g., deleting data, sending external communications, changing configurations)  \n   - Require human review and explicit approval at these checkpoints before the agent can proceed.\n\n2. **Design oversight to counter automation bias, not amplify it**  \n   - Present alternative options or rationales, not just the agent’s “best” recommendation, so humans are less likely to accept it blindly.  \n   - Provide uncertainty estimates, limitations, and relevant context so humans can critically assess the output.  \n   - Avoid UI patterns that over‑signal confidence (e.g., “recommended” badges without explanation).\n\n3. **Regularly audit human oversight itself**  \n   - Monitor how often humans override, modify, or question agent actions. A very low override rate can be a red flag for automation bias.  \n   - Periodically review samples of human‑approved actions to check whether oversight is substantive and effective.  \n   - Update training, workflows, and interfaces if audits show that humans are just clicking “approve.”\n\n4. **Train and support human reviewers**  \n   - Make clear that their role is to *critically evaluate* the agent, not to speed it through.  \n   - Provide guidance, checklists, or decision aids to help them spot errors, edge cases, and policy violations.  \n\nIn short, “human-in-the-loop” must shift from nominal sign‑off to structured, auditable, and well‑supported critical oversight, anchored at clearly defined high‑risk checkpoints and continually evaluated for effectiveness."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "Checkpoints and ongoing activity should be adapted in two ways:\n\n**1. Checkpoints that should require human approval**\n\nHuman sign‑off should be built into points in the agentic workflow where actions are:\n\n- **High‑stakes**  \n  - Large financial transactions or commitments  \n  - Safety‑critical decisions (health, security, legal exposure)  \n  - Actions affecting many users, customers, or employees at once  \n\n- **Irreversible or hard to undo**  \n  - Deleting or overwriting important data  \n  - Publishing or sending external communications (e.g., emails to clients, public posts, regulatory filings)  \n  - Changing production system configurations, access controls, or infrastructure  \n\n- **Policy- or ethics‑sensitive**  \n  - Anything touching on compliance (legal, regulatory, privacy, IP)  \n  - Use of sensitive or personal data  \n  - Decisions that could introduce bias, discrimination, or reputational risk  \n\nAt these checkpoints, the agent should pause and require explicit human review and approval before proceeding.\n\n**2. Ongoing activity to keep oversight effective**\n\nTo ensure humans don’t drift into rubber‑stamping and that automation bias is managed, organisations should:\n\n- **Regularly audit human oversight**  \n  - Sample and review human‑approved actions to see whether reviewers are catching issues.  \n  - Track override/modify rates; a consistently near‑zero override rate can signal blind trust.  \n  - Check that humans are actually examining rationales, evidence, and alternatives, not just clicking “approve.”\n\n- **Continuously improve oversight processes**  \n  - Update guidance, workflows, and training based on audit findings.  \n  - Adjust which checkpoints require approval as the agent’s capabilities, risks, and operating environment change.  \n\nThis combination—clear high‑risk checkpoints plus regular auditing and adjustment—helps “human‑in‑the‑loop” remain meaningful and effective as agents become more capable."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-003",
      "turn_number": 3,
      "question": "What is the recommended approach during and after deployment to handle the fact that agents interact dynamically with their environment and not all risks can be anticipated upfront?",
      "page_reference": 3,
      "response": "During and after deployment, the recommended approach is to:\n\n1. **Roll out agents gradually**  \n   - Start with limited scope, access, and user groups.  \n   - Incrementally expand capabilities, permissions, and scale as the agent demonstrates safe and reliable behavior.  \n\n2. **Continuously monitor after deployment**  \n   - Track the agent’s actions, tool use, and outcomes in real time or near real time.  \n   - Watch for policy violations, unexpected behaviors, failures, or new attack patterns.  \n   - Use these observations to update safeguards, policies, and technical controls as new risks emerge.\n\nThis staged rollout plus ongoing monitoring is meant to manage the fact that agents operate in dynamic environments where not all risks are knowable in advance.",
      "latency_ms": 4474.432945251465,
      "api_latency_ms": 4474.432945251465,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” needs to be redesigned so that humans don’t just rubber‑stamp agent decisions. In the context you provided, that means:\n\n1. **Define explicit checkpoints for human approval**  \n   - Identify points in the agentic workflow where the stakes are high:  \n     - High‑impact financial decisions  \n     - Safety‑critical actions  \n     - Irreversible operations (e.g., deleting data, sending external communications, changing configurations)  \n   - Require human review and explicit approval at these checkpoints before the agent can proceed.\n\n2. **Design oversight to counter automation bias, not amplify it**  \n   - Present alternative options or rationales, not just the agent’s “best” recommendation, so humans are less likely to accept it blindly.  \n   - Provide uncertainty estimates, limitations, and relevant context so humans can critically assess the output.  \n   - Avoid UI patterns that over‑signal confidence (e.g., “recommended” badges without explanation).\n\n3. **Regularly audit human oversight itself**  \n   - Monitor how often humans override, modify, or question agent actions. A very low override rate can be a red flag for automation bias.  \n   - Periodically review samples of human‑approved actions to check whether oversight is substantive and effective.  \n   - Update training, workflows, and interfaces if audits show that humans are just clicking “approve.”\n\n4. **Train and support human reviewers**  \n   - Make clear that their role is to *critically evaluate* the agent, not to speed it through.  \n   - Provide guidance, checklists, or decision aids to help them spot errors, edge cases, and policy violations.  \n\nIn short, “human-in-the-loop” must shift from nominal sign‑off to structured, auditable, and well‑supported critical oversight, anchored at clearly defined high‑risk checkpoints and continually evaluated for effectiveness."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "Checkpoints and ongoing activity should be adapted in two ways:\n\n**1. Checkpoints that should require human approval**\n\nHuman sign‑off should be built into points in the agentic workflow where actions are:\n\n- **High‑stakes**  \n  - Large financial transactions or commitments  \n  - Safety‑critical decisions (health, security, legal exposure)  \n  - Actions affecting many users, customers, or employees at once  \n\n- **Irreversible or hard to undo**  \n  - Deleting or overwriting important data  \n  - Publishing or sending external communications (e.g., emails to clients, public posts, regulatory filings)  \n  - Changing production system configurations, access controls, or infrastructure  \n\n- **Policy- or ethics‑sensitive**  \n  - Anything touching on compliance (legal, regulatory, privacy, IP)  \n  - Use of sensitive or personal data  \n  - Decisions that could introduce bias, discrimination, or reputational risk  \n\nAt these checkpoints, the agent should pause and require explicit human review and approval before proceeding.\n\n**2. Ongoing activity to keep oversight effective**\n\nTo ensure humans don’t drift into rubber‑stamping and that automation bias is managed, organisations should:\n\n- **Regularly audit human oversight**  \n  - Sample and review human‑approved actions to see whether reviewers are catching issues.  \n  - Track override/modify rates; a consistently near‑zero override rate can signal blind trust.  \n  - Check that humans are actually examining rationales, evidence, and alternatives, not just clicking “approve.”\n\n- **Continuously improve oversight processes**  \n  - Update guidance, workflows, and training based on audit findings.  \n  - Adjust which checkpoints require approval as the agent’s capabilities, risks, and operating environment change.  \n\nThis combination—clear high‑risk checkpoints plus regular auditing and adjustment—helps “human‑in‑the‑loop” remain meaningful and effective as agents become more capable."
        }
      ],
      "prompt_tokens": 1346,
      "completion_tokens": 162,
      "cached_tokens": 0,
      "total_tokens": 1508,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” needs to be redesigned so that humans don’t just rubber‑stamp agent decisions. In the context you provided, that means:\n\n1. **Define explicit checkpoints for human approval**  \n   - Identify points in the agentic workflow where the stakes are high:  \n     - High‑impact financial decisions  \n     - Safety‑critical actions  \n     - Irreversible operations (e.g., deleting data, sending external communications, changing configurations)  \n   - Require human review and explicit approval at these checkpoints before the agent can proceed.\n\n2. **Design oversight to counter automation bias, not amplify it**  \n   - Present alternative options or rationales, not just the agent’s “best” recommendation, so humans are less likely to accept it blindly.  \n   - Provide uncertainty estimates, limitations, and relevant context so humans can critically assess the output.  \n   - Avoid UI patterns that over‑signal confidence (e.g., “recommended” badges without explanation).\n\n3. **Regularly audit human oversight itself**  \n   - Monitor how often humans override, modify, or question agent actions. A very low override rate can be a red flag for automation bias.  \n   - Periodically review samples of human‑approved actions to check whether oversight is substantive and effective.  \n   - Update training, workflows, and interfaces if audits show that humans are just clicking “approve.”\n\n4. **Train and support human reviewers**  \n   - Make clear that their role is to *critically evaluate* the agent, not to speed it through.  \n   - Provide guidance, checklists, or decision aids to help them spot errors, edge cases, and policy violations.  \n\nIn short, “human-in-the-loop” must shift from nominal sign‑off to structured, auditable, and well‑supported critical oversight, anchored at clearly defined high‑risk checkpoints and continually evaluated for effectiveness."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "Checkpoints and ongoing activity should be adapted in two ways:\n\n**1. Checkpoints that should require human approval**\n\nHuman sign‑off should be built into points in the agentic workflow where actions are:\n\n- **High‑stakes**  \n  - Large financial transactions or commitments  \n  - Safety‑critical decisions (health, security, legal exposure)  \n  - Actions affecting many users, customers, or employees at once  \n\n- **Irreversible or hard to undo**  \n  - Deleting or overwriting important data  \n  - Publishing or sending external communications (e.g., emails to clients, public posts, regulatory filings)  \n  - Changing production system configurations, access controls, or infrastructure  \n\n- **Policy- or ethics‑sensitive**  \n  - Anything touching on compliance (legal, regulatory, privacy, IP)  \n  - Use of sensitive or personal data  \n  - Decisions that could introduce bias, discrimination, or reputational risk  \n\nAt these checkpoints, the agent should pause and require explicit human review and approval before proceeding.\n\n**2. Ongoing activity to keep oversight effective**\n\nTo ensure humans don’t drift into rubber‑stamping and that automation bias is managed, organisations should:\n\n- **Regularly audit human oversight**  \n  - Sample and review human‑approved actions to see whether reviewers are catching issues.  \n  - Track override/modify rates; a consistently near‑zero override rate can signal blind trust.  \n  - Check that humans are actually examining rationales, evidence, and alternatives, not just clicking “approve.”\n\n- **Continuously improve oversight processes**  \n  - Update guidance, workflows, and training based on audit findings.  \n  - Adjust which checkpoints require approval as the agent’s capabilities, risks, and operating environment change.  \n\nThis combination—clear high‑risk checkpoints plus regular auditing and adjustment—helps “human‑in‑the‑loop” remain meaningful and effective as agents become more capable."
        },
        {
          "role": "user",
          "content": "What is the recommended approach during and after deployment to handle the fact that agents interact dynamically with their environment and not all risks can be anticipated upfront?"
        },
        {
          "role": "assistant",
          "content": "During and after deployment, the recommended approach is to:\n\n1. **Roll out agents gradually**  \n   - Start with limited scope, access, and user groups.  \n   - Incrementally expand capabilities, permissions, and scale as the agent demonstrates safe and reliable behavior.  \n\n2. **Continuously monitor after deployment**  \n   - Track the agent’s actions, tool use, and outcomes in real time or near real time.  \n   - Watch for policy violations, unexpected behaviors, failures, or new attack patterns.  \n   - Use these observations to update safeguards, policies, and technical controls as new risks emerge.\n\nThis staged rollout plus ongoing monitoring is meant to manage the fact that agents operate in dynamic environments where not all risks are knowable in advance."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-004",
      "turn_number": 1,
      "question": "What are agentic AI systems, and what common features do agents usually have?",
      "page_reference": 4,
      "response": "Agentic AI systems are AI systems that can plan across multiple steps to achieve specified objectives, using AI agents built on top of language models (small, large, or multimodal). The language model serves as the “brain,” making decisions and carrying out tasks to reach a user-defined goal.\n\nCommon features agents usually have include:\n- Some degree of independent planning: they can break a goal into steps and decide what to do next without the user specifying every action.\n- Some degree of independent action-taking: they can perform actions such as searching the web or creating files.\n- Multi-step operation: they work over multiple steps or iterations to move toward the user-defined objective.",
      "latency_ms": 10615.081310272217,
      "api_latency_ms": 10615.081310272217,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 475,
      "completion_tokens": 143,
      "cached_tokens": 0,
      "total_tokens": 618,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are AI systems that can plan across multiple steps to achieve specified objectives, using AI agents built on top of language models (small, large, or multimodal). The language model serves as the “brain,” making decisions and carrying out tasks to reach a user-defined goal.\n\nCommon features agents usually have include:\n- Some degree of independent planning: they can break a goal into steps and decide what to do next without the user specifying every action.\n- Some degree of independent action-taking: they can perform actions such as searching the web or creating files.\n- Multi-step operation: they work over multiple steps or iterations to move toward the user-defined objective."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-004",
      "turn_number": 2,
      "question": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?",
      "page_reference": 4,
      "response": "In this framework, the “brain” of an agent is the **model** that acts as the central reasoning and planning engine.\n\nThe kinds of language models that can fill this role are:\n- **SLM** – Small Language Model  \n- **LLM** – Large Language Model  \n- **MLLM** – Multimodal Large Language Model",
      "latency_ms": 1403.1431674957275,
      "api_latency_ms": 1403.1431674957275,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are AI systems that can plan across multiple steps to achieve specified objectives, using AI agents built on top of language models (small, large, or multimodal). The language model serves as the “brain,” making decisions and carrying out tasks to reach a user-defined goal.\n\nCommon features agents usually have include:\n- Some degree of independent planning: they can break a goal into steps and decide what to do next without the user specifying every action.\n- Some degree of independent action-taking: they can perform actions such as searching the web or creating files.\n- Multi-step operation: they work over multiple steps or iterations to move toward the user-defined objective."
        }
      ],
      "prompt_tokens": 645,
      "completion_tokens": 80,
      "cached_tokens": 0,
      "total_tokens": 725,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are AI systems that can plan across multiple steps to achieve specified objectives, using AI agents built on top of language models (small, large, or multimodal). The language model serves as the “brain,” making decisions and carrying out tasks to reach a user-defined goal.\n\nCommon features agents usually have include:\n- Some degree of independent planning: they can break a goal into steps and decide what to do next without the user specifying every action.\n- Some degree of independent action-taking: they can perform actions such as searching the web or creating files.\n- Multi-step operation: they work over multiple steps or iterations to move toward the user-defined objective."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In this framework, the “brain” of an agent is the **model** that acts as the central reasoning and planning engine.\n\nThe kinds of language models that can fill this role are:\n- **SLM** – Small Language Model  \n- **LLM** – Large Language Model  \n- **MLLM** – Multimodal Large Language Model"
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-004",
      "turn_number": 3,
      "question": "What is the role of the model component in a simple LLM-based app, and what does it do with user inputs and instructions?",
      "page_reference": 4,
      "response": "The **model** component is the central reasoning and planning engine—the “brain” of a simple LLM‑based app.\n\nIts role is to:\n- **Process instructions and user inputs**  \n- **Interpret** what the user wants  \n- **Generate contextually appropriate responses** to move toward the specified goal.",
      "latency_ms": 1715.7466411590576,
      "api_latency_ms": 1715.7466411590576,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are AI systems that can plan across multiple steps to achieve specified objectives, using AI agents built on top of language models (small, large, or multimodal). The language model serves as the “brain,” making decisions and carrying out tasks to reach a user-defined goal.\n\nCommon features agents usually have include:\n- Some degree of independent planning: they can break a goal into steps and decide what to do next without the user specifying every action.\n- Some degree of independent action-taking: they can perform actions such as searching the web or creating files.\n- Multi-step operation: they work over multiple steps or iterations to move toward the user-defined objective."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In this framework, the “brain” of an agent is the **model** that acts as the central reasoning and planning engine.\n\nThe kinds of language models that can fill this role are:\n- **SLM** – Small Language Model  \n- **LLM** – Large Language Model  \n- **MLLM** – Multimodal Large Language Model"
        }
      ],
      "prompt_tokens": 752,
      "completion_tokens": 73,
      "cached_tokens": 0,
      "total_tokens": 825,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are AI systems that can plan across multiple steps to achieve specified objectives, using AI agents built on top of language models (small, large, or multimodal). The language model serves as the “brain,” making decisions and carrying out tasks to reach a user-defined goal.\n\nCommon features agents usually have include:\n- Some degree of independent planning: they can break a goal into steps and decide what to do next without the user specifying every action.\n- Some degree of independent action-taking: they can perform actions such as searching the web or creating files.\n- Multi-step operation: they work over multiple steps or iterations to move toward the user-defined objective."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In this framework, the “brain” of an agent is the **model** that acts as the central reasoning and planning engine.\n\nThe kinds of language models that can fill this role are:\n- **SLM** – Small Language Model  \n- **LLM** – Large Language Model  \n- **MLLM** – Multimodal Large Language Model"
        },
        {
          "role": "user",
          "content": "What is the role of the model component in a simple LLM-based app, and what does it do with user inputs and instructions?"
        },
        {
          "role": "assistant",
          "content": "The **model** component is the central reasoning and planning engine—the “brain” of a simple LLM‑based app.\n\nIts role is to:\n- **Process instructions and user inputs**  \n- **Interpret** what the user wants  \n- **Generate contextually appropriate responses** to move toward the specified goal."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-005",
      "turn_number": 1,
      "question": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?",
      "page_reference": 5,
      "response": "In an agentic system, **tools** are external functions or services that the agent can call to take real actions in the world, beyond just generating text. The model decides *when* and *how* to use these tools, but the tools themselves do the acting.\n\nYou can think of tools as the main way an agent’s **action-space** (its practical capabilities and authority) is defined.\n\n### What are tools?\n\n- They are **interfaces** to other systems: APIs, databases, devices, apps, scripts, etc.\n- The agent calls them via a **protocol** (such as the Model Context Protocol, MCP), sending structured inputs and receiving structured outputs.\n- Tools let an agent:\n  - Read from the world (e.g., fetch data)\n  - Write to the world (e.g., update a database, send an email)\n  - Control other systems (e.g., trigger workflows, control devices)\n\nWithout tools, an agent is mostly limited to reasoning and generating text; with tools, it can actually *do* things.\n\n---\n\n### Types of actions tools can enable\n\nBelow are common categories of actions that tools provide, with examples.\n\n#### 1. Data access and retrieval\n\nTools that let an agent **read information** from external sources:\n\n- Query databases (SQL/NoSQL)\n- Call web APIs (weather, finance, maps, news, etc.)\n- Search the web or internal knowledge bases\n- Read documents or files from storage\n\nExample:  \n“Find all customers in Europe who haven’t logged in for 90 days” → tool queries a customer database and returns a list.\n\n---\n\n#### 2. Data creation, update, and deletion\n\nTools that let an agent **modify stored information**:\n\n- Insert/update/delete database records\n- Write to files, spreadsheets, or document stores\n- Update CRM entries, tickets, or user profiles\n\nExample:  \n“Create a new support ticket for this user” → tool writes a new record into the ticketing system.\n\n---\n\n#### 3. Communication and messaging\n\nTools that let an agent **communicate with people or systems**:\n\n- Send emails, chat messages, or SMS\n- Post to Slack/Teams channels\n- Create calendar events and invitations\n- Send notifications or alerts\n\nExample:  \n“Email the customer with a summary of our call and a follow-up date” → tool sends an email using a template the agent fills in.\n\n---\n\n#### 4. Workflow and process control\n\nTools that let an agent **trigger or orchestrate workflows**:\n\n- Start or stop automated workflows (e.g., CI/CD pipelines, ETL jobs)\n- Trigger business processes (order fulfillment, onboarding, approvals)\n- Interact with ticketing or issue tracking systems (Jira, ServiceNow)\n\nExample:  \n“Deploy the latest version of the app to staging if tests pass” → tools run tests and call a deployment pipeline.\n\n---\n\n#### 5. Transactions and financial operations\n\nTools that let an agent **perform transactions**:\n\n- Initiate payments or refunds\n- Move funds between accounts (within configured limits)\n- Create invoices or purchase orders\n- Check balances or transaction history\n\nExample:  \n“Issue a refund up to $50 if the product is defective” → tool processes a refund transaction.\n\n---\n\n#### 6. Device and environment control\n\nTools that let an agent **control physical or virtual devices**:\n\n- Smart home devices (lights, thermostats, locks)\n- Robots, drones, or manufacturing equipment\n- Cloud infrastructure (start/stop servers, scale resources)\n- Virtual desktops or browsers (click, type, navigate)\n\nExample:  \n“Turn off office lights after 8 p.m. if no motion is detected” → tool controls building systems.\n\n---\n\n#### 7. Content generation and transformation\n\nTools that let an agent **process or transform content** beyond what’s easy to do in-text:\n\n- Image, audio, or video generation and editing\n- OCR (extract text from images/PDFs)\n- Code execution and evaluation (e.g., Python sandboxes)\n- Format conversion (PDF ↔ Word, CSV ↔ JSON)\n\nExample:  \n“Extract all tables from this scanned PDF and put them into a spreadsheet” → tools run OCR and write to a sheet.\n\n---\n\n#### 8. Security, identity, and access control\n\nTools that let an agent **respect and enforce permissions**:\n\n- Authenticate users (login, tokens)\n- Check roles and permissions\n- Log actions for auditing\n- Request approvals for sensitive actions\n\nExample:  \n“Reset the user’s password only after verifying their identity” → tools perform identity checks and then reset credentials.\n\n---\n\n### How tools relate to an agent’s capabilities\n\n- The **set of tools** an agent is allowed to use defines its **action-space**:\n  - A “read-only analyst” agent might only have data query tools.\n  - A “support agent” might also have tools to update tickets and send emails.\n  - A “finance agent” might have payment tools but with strict limits and approval flows.\n- Governance and safety are implemented by:\n  - Choosing which tools the agent can access\n  - Imposing limits (e.g., transaction caps, read-only vs write)\n  - Logging and reviewing tool calls\n\n---\n\nIn summary:  \n**Tools are the bridges between an agent’s reasoning and real-world impact.** They let the agent not just think and talk, but also fetch data, update systems, communicate, control devices, and perform transactions—within whatever boundaries designers set.",
      "latency_ms": 18324.089527130127,
      "api_latency_ms": 18324.089527130127,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 620,
      "completion_tokens": 1133,
      "cached_tokens": 0,
      "total_tokens": 1753,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external functions or services that the agent can call to take real actions in the world, beyond just generating text. The model decides *when* and *how* to use these tools, but the tools themselves do the acting.\n\nYou can think of tools as the main way an agent’s **action-space** (its practical capabilities and authority) is defined.\n\n### What are tools?\n\n- They are **interfaces** to other systems: APIs, databases, devices, apps, scripts, etc.\n- The agent calls them via a **protocol** (such as the Model Context Protocol, MCP), sending structured inputs and receiving structured outputs.\n- Tools let an agent:\n  - Read from the world (e.g., fetch data)\n  - Write to the world (e.g., update a database, send an email)\n  - Control other systems (e.g., trigger workflows, control devices)\n\nWithout tools, an agent is mostly limited to reasoning and generating text; with tools, it can actually *do* things.\n\n---\n\n### Types of actions tools can enable\n\nBelow are common categories of actions that tools provide, with examples.\n\n#### 1. Data access and retrieval\n\nTools that let an agent **read information** from external sources:\n\n- Query databases (SQL/NoSQL)\n- Call web APIs (weather, finance, maps, news, etc.)\n- Search the web or internal knowledge bases\n- Read documents or files from storage\n\nExample:  \n“Find all customers in Europe who haven’t logged in for 90 days” → tool queries a customer database and returns a list.\n\n---\n\n#### 2. Data creation, update, and deletion\n\nTools that let an agent **modify stored information**:\n\n- Insert/update/delete database records\n- Write to files, spreadsheets, or document stores\n- Update CRM entries, tickets, or user profiles\n\nExample:  \n“Create a new support ticket for this user” → tool writes a new record into the ticketing system.\n\n---\n\n#### 3. Communication and messaging\n\nTools that let an agent **communicate with people or systems**:\n\n- Send emails, chat messages, or SMS\n- Post to Slack/Teams channels\n- Create calendar events and invitations\n- Send notifications or alerts\n\nExample:  \n“Email the customer with a summary of our call and a follow-up date” → tool sends an email using a template the agent fills in.\n\n---\n\n#### 4. Workflow and process control\n\nTools that let an agent **trigger or orchestrate workflows**:\n\n- Start or stop automated workflows (e.g., CI/CD pipelines, ETL jobs)\n- Trigger business processes (order fulfillment, onboarding, approvals)\n- Interact with ticketing or issue tracking systems (Jira, ServiceNow)\n\nExample:  \n“Deploy the latest version of the app to staging if tests pass” → tools run tests and call a deployment pipeline.\n\n---\n\n#### 5. Transactions and financial operations\n\nTools that let an agent **perform transactions**:\n\n- Initiate payments or refunds\n- Move funds between accounts (within configured limits)\n- Create invoices or purchase orders\n- Check balances or transaction history\n\nExample:  \n“Issue a refund up to $50 if the product is defective” → tool processes a refund transaction.\n\n---\n\n#### 6. Device and environment control\n\nTools that let an agent **control physical or virtual devices**:\n\n- Smart home devices (lights, thermostats, locks)\n- Robots, drones, or manufacturing equipment\n- Cloud infrastructure (start/stop servers, scale resources)\n- Virtual desktops or browsers (click, type, navigate)\n\nExample:  \n“Turn off office lights after 8 p.m. if no motion is detected” → tool controls building systems.\n\n---\n\n#### 7. Content generation and transformation\n\nTools that let an agent **process or transform content** beyond what’s easy to do in-text:\n\n- Image, audio, or video generation and editing\n- OCR (extract text from images/PDFs)\n- Code execution and evaluation (e.g., Python sandboxes)\n- Format conversion (PDF ↔ Word, CSV ↔ JSON)\n\nExample:  \n“Extract all tables from this scanned PDF and put them into a spreadsheet” → tools run OCR and write to a sheet.\n\n---\n\n#### 8. Security, identity, and access control\n\nTools that let an agent **respect and enforce permissions**:\n\n- Authenticate users (login, tokens)\n- Check roles and permissions\n- Log actions for auditing\n- Request approvals for sensitive actions\n\nExample:  \n“Reset the user’s password only after verifying their identity” → tools perform identity checks and then reset credentials.\n\n---\n\n### How tools relate to an agent’s capabilities\n\n- The **set of tools** an agent is allowed to use defines its **action-space**:\n  - A “read-only analyst” agent might only have data query tools.\n  - A “support agent” might also have tools to update tickets and send emails.\n  - A “finance agent” might have payment tools but with strict limits and approval flows.\n- Governance and safety are implemented by:\n  - Choosing which tools the agent can access\n  - Imposing limits (e.g., transaction caps, read-only vs write)\n  - Logging and reviewing tool calls\n\n---\n\nIn summary:  \n**Tools are the bridges between an agent’s reasoning and real-world impact.** They let the agent not just think and talk, but also fetch data, update systems, communicate, control devices, and perform transactions—within whatever boundaries designers set."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-005",
      "turn_number": 2,
      "question": "How does the model use tools to complete a task?",
      "page_reference": 5,
      "response": "The model uses tools as part of a loop of **reason → act (via tools) → observe → adjust → continue** until the task is done. At a high level:\n\n1. **Interpret the user’s request**  \n   - The model reads the instructions, current context, and any memory.  \n   - It decides what subtasks are needed (plan), e.g.:\n     - “Understand the problem”\n     - “Look up data”\n     - “Take an action (e.g., send email, update record)”\n     - “Summarise the result for the user”\n\n2. **Decide whether a tool is needed**  \n   For each step, the model asks (implicitly):  \n   - Can I answer with my internal knowledge alone?  \n   - Do I need fresh or external data?  \n   - Do I need to change something in the world (write to DB, send message, trigger workflow)?  \n   If the answer is yes, it chooses a tool from its allowed **action-space**.\n\n3. **Formulate a tool call**  \n   - The model outputs a **structured request** to a specific tool (e.g., via MCP or another protocol).  \n   - This includes:\n     - Tool name (e.g. `query_database`, `send_email`)\n     - Arguments (e.g. SQL query, email address, message body)\n   - The agent framework or runtime executes the tool call; the model itself does not run the code or access the API directly.\n\n4. **Receive and interpret the tool’s output**  \n   - The tool returns structured data (e.g. JSON, text, status codes).  \n   - This output is fed back to the model as new context (an “observation”).  \n   - The model:\n     - Checks if the result makes sense\n     - Handles errors (e.g., “user not found”, “API timeout”)\n     - Decides whether to call another tool, adjust the plan, or finish.\n\n5. **Iterate with planning and reasoning**  \n   The model may repeat the cycle:\n   - Call more tools (e.g., first search a database, then send an email to the result list).\n   - Refine the plan based on new information.\n   - In a multi-agent system, hand off to another agent, or respond to a supervisor agent’s instructions.\n\n6. **Produce a final answer or action summary**  \n   Once the task is complete (or as complete as possible), the model:\n   - Synthesises the tool results into a user-facing response: explanation, summary, or confirmation.\n   - Optionally logs what tools were used and why (for auditing and safety).\n\n---\n\n### Concrete example\n\nUser: “Find all customers in Europe who haven’t logged in for 90 days and email them a reactivation offer.”\n\nThe model might:\n\n1. Plan:  \n   - Step 1: Query customer database  \n   - Step 2: Filter by region and last login  \n   - Step 3: Send emails  \n   - Step 4: Report back\n\n2. Call `query_customers` tool with appropriate filters.  \n3. Receive list of customers; check if it’s empty or too large.  \n4. Call `send_email` tool in batches with a prepared template.  \n5. After success/fail responses, reply to the user with a summary:  \n   - “I emailed 1,245 eligible customers; 3 addresses bounced.”\n\nThroughout, the model’s **reasoning** decides *which* tools to call and *in what order*, while the tools actually perform the operations.",
      "latency_ms": 8684.843063354492,
      "api_latency_ms": 8684.843063354492,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 3.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external functions or services that the agent can call to take real actions in the world, beyond just generating text. The model decides *when* and *how* to use these tools, but the tools themselves do the acting.\n\nYou can think of tools as the main way an agent’s **action-space** (its practical capabilities and authority) is defined.\n\n### What are tools?\n\n- They are **interfaces** to other systems: APIs, databases, devices, apps, scripts, etc.\n- The agent calls them via a **protocol** (such as the Model Context Protocol, MCP), sending structured inputs and receiving structured outputs.\n- Tools let an agent:\n  - Read from the world (e.g., fetch data)\n  - Write to the world (e.g., update a database, send an email)\n  - Control other systems (e.g., trigger workflows, control devices)\n\nWithout tools, an agent is mostly limited to reasoning and generating text; with tools, it can actually *do* things.\n\n---\n\n### Types of actions tools can enable\n\nBelow are common categories of actions that tools provide, with examples.\n\n#### 1. Data access and retrieval\n\nTools that let an agent **read information** from external sources:\n\n- Query databases (SQL/NoSQL)\n- Call web APIs (weather, finance, maps, news, etc.)\n- Search the web or internal knowledge bases\n- Read documents or files from storage\n\nExample:  \n“Find all customers in Europe who haven’t logged in for 90 days” → tool queries a customer database and returns a list.\n\n---\n\n#### 2. Data creation, update, and deletion\n\nTools that let an agent **modify stored information**:\n\n- Insert/update/delete database records\n- Write to files, spreadsheets, or document stores\n- Update CRM entries, tickets, or user profiles\n\nExample:  \n“Create a new support ticket for this user” → tool writes a new record into the ticketing system.\n\n---\n\n#### 3. Communication and messaging\n\nTools that let an agent **communicate with people or systems**:\n\n- Send emails, chat messages, or SMS\n- Post to Slack/Teams channels\n- Create calendar events and invitations\n- Send notifications or alerts\n\nExample:  \n“Email the customer with a summary of our call and a follow-up date” → tool sends an email using a template the agent fills in.\n\n---\n\n#### 4. Workflow and process control\n\nTools that let an agent **trigger or orchestrate workflows**:\n\n- Start or stop automated workflows (e.g., CI/CD pipelines, ETL jobs)\n- Trigger business processes (order fulfillment, onboarding, approvals)\n- Interact with ticketing or issue tracking systems (Jira, ServiceNow)\n\nExample:  \n“Deploy the latest version of the app to staging if tests pass” → tools run tests and call a deployment pipeline.\n\n---\n\n#### 5. Transactions and financial operations\n\nTools that let an agent **perform transactions**:\n\n- Initiate payments or refunds\n- Move funds between accounts (within configured limits)\n- Create invoices or purchase orders\n- Check balances or transaction history\n\nExample:  \n“Issue a refund up to $50 if the product is defective” → tool processes a refund transaction.\n\n---\n\n#### 6. Device and environment control\n\nTools that let an agent **control physical or virtual devices**:\n\n- Smart home devices (lights, thermostats, locks)\n- Robots, drones, or manufacturing equipment\n- Cloud infrastructure (start/stop servers, scale resources)\n- Virtual desktops or browsers (click, type, navigate)\n\nExample:  \n“Turn off office lights after 8 p.m. if no motion is detected” → tool controls building systems.\n\n---\n\n#### 7. Content generation and transformation\n\nTools that let an agent **process or transform content** beyond what’s easy to do in-text:\n\n- Image, audio, or video generation and editing\n- OCR (extract text from images/PDFs)\n- Code execution and evaluation (e.g., Python sandboxes)\n- Format conversion (PDF ↔ Word, CSV ↔ JSON)\n\nExample:  \n“Extract all tables from this scanned PDF and put them into a spreadsheet” → tools run OCR and write to a sheet.\n\n---\n\n#### 8. Security, identity, and access control\n\nTools that let an agent **respect and enforce permissions**:\n\n- Authenticate users (login, tokens)\n- Check roles and permissions\n- Log actions for auditing\n- Request approvals for sensitive actions\n\nExample:  \n“Reset the user’s password only after verifying their identity” → tools perform identity checks and then reset credentials.\n\n---\n\n### How tools relate to an agent’s capabilities\n\n- The **set of tools** an agent is allowed to use defines its **action-space**:\n  - A “read-only analyst” agent might only have data query tools.\n  - A “support agent” might also have tools to update tickets and send emails.\n  - A “finance agent” might have payment tools but with strict limits and approval flows.\n- Governance and safety are implemented by:\n  - Choosing which tools the agent can access\n  - Imposing limits (e.g., transaction caps, read-only vs write)\n  - Logging and reviewing tool calls\n\n---\n\nIn summary:  \n**Tools are the bridges between an agent’s reasoning and real-world impact.** They let the agent not just think and talk, but also fetch data, update systems, communicate, control devices, and perform transactions—within whatever boundaries designers set."
        }
      ],
      "prompt_tokens": 1764,
      "completion_tokens": 751,
      "cached_tokens": 0,
      "total_tokens": 2515,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external functions or services that the agent can call to take real actions in the world, beyond just generating text. The model decides *when* and *how* to use these tools, but the tools themselves do the acting.\n\nYou can think of tools as the main way an agent’s **action-space** (its practical capabilities and authority) is defined.\n\n### What are tools?\n\n- They are **interfaces** to other systems: APIs, databases, devices, apps, scripts, etc.\n- The agent calls them via a **protocol** (such as the Model Context Protocol, MCP), sending structured inputs and receiving structured outputs.\n- Tools let an agent:\n  - Read from the world (e.g., fetch data)\n  - Write to the world (e.g., update a database, send an email)\n  - Control other systems (e.g., trigger workflows, control devices)\n\nWithout tools, an agent is mostly limited to reasoning and generating text; with tools, it can actually *do* things.\n\n---\n\n### Types of actions tools can enable\n\nBelow are common categories of actions that tools provide, with examples.\n\n#### 1. Data access and retrieval\n\nTools that let an agent **read information** from external sources:\n\n- Query databases (SQL/NoSQL)\n- Call web APIs (weather, finance, maps, news, etc.)\n- Search the web or internal knowledge bases\n- Read documents or files from storage\n\nExample:  \n“Find all customers in Europe who haven’t logged in for 90 days” → tool queries a customer database and returns a list.\n\n---\n\n#### 2. Data creation, update, and deletion\n\nTools that let an agent **modify stored information**:\n\n- Insert/update/delete database records\n- Write to files, spreadsheets, or document stores\n- Update CRM entries, tickets, or user profiles\n\nExample:  \n“Create a new support ticket for this user” → tool writes a new record into the ticketing system.\n\n---\n\n#### 3. Communication and messaging\n\nTools that let an agent **communicate with people or systems**:\n\n- Send emails, chat messages, or SMS\n- Post to Slack/Teams channels\n- Create calendar events and invitations\n- Send notifications or alerts\n\nExample:  \n“Email the customer with a summary of our call and a follow-up date” → tool sends an email using a template the agent fills in.\n\n---\n\n#### 4. Workflow and process control\n\nTools that let an agent **trigger or orchestrate workflows**:\n\n- Start or stop automated workflows (e.g., CI/CD pipelines, ETL jobs)\n- Trigger business processes (order fulfillment, onboarding, approvals)\n- Interact with ticketing or issue tracking systems (Jira, ServiceNow)\n\nExample:  \n“Deploy the latest version of the app to staging if tests pass” → tools run tests and call a deployment pipeline.\n\n---\n\n#### 5. Transactions and financial operations\n\nTools that let an agent **perform transactions**:\n\n- Initiate payments or refunds\n- Move funds between accounts (within configured limits)\n- Create invoices or purchase orders\n- Check balances or transaction history\n\nExample:  \n“Issue a refund up to $50 if the product is defective” → tool processes a refund transaction.\n\n---\n\n#### 6. Device and environment control\n\nTools that let an agent **control physical or virtual devices**:\n\n- Smart home devices (lights, thermostats, locks)\n- Robots, drones, or manufacturing equipment\n- Cloud infrastructure (start/stop servers, scale resources)\n- Virtual desktops or browsers (click, type, navigate)\n\nExample:  \n“Turn off office lights after 8 p.m. if no motion is detected” → tool controls building systems.\n\n---\n\n#### 7. Content generation and transformation\n\nTools that let an agent **process or transform content** beyond what’s easy to do in-text:\n\n- Image, audio, or video generation and editing\n- OCR (extract text from images/PDFs)\n- Code execution and evaluation (e.g., Python sandboxes)\n- Format conversion (PDF ↔ Word, CSV ↔ JSON)\n\nExample:  \n“Extract all tables from this scanned PDF and put them into a spreadsheet” → tools run OCR and write to a sheet.\n\n---\n\n#### 8. Security, identity, and access control\n\nTools that let an agent **respect and enforce permissions**:\n\n- Authenticate users (login, tokens)\n- Check roles and permissions\n- Log actions for auditing\n- Request approvals for sensitive actions\n\nExample:  \n“Reset the user’s password only after verifying their identity” → tools perform identity checks and then reset credentials.\n\n---\n\n### How tools relate to an agent’s capabilities\n\n- The **set of tools** an agent is allowed to use defines its **action-space**:\n  - A “read-only analyst” agent might only have data query tools.\n  - A “support agent” might also have tools to update tickets and send emails.\n  - A “finance agent” might have payment tools but with strict limits and approval flows.\n- Governance and safety are implemented by:\n  - Choosing which tools the agent can access\n  - Imposing limits (e.g., transaction caps, read-only vs write)\n  - Logging and reviewing tool calls\n\n---\n\nIn summary:  \n**Tools are the bridges between an agent’s reasoning and real-world impact.** They let the agent not just think and talk, but also fetch data, update systems, communicate, control devices, and perform transactions—within whatever boundaries designers set."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools as part of a loop of **reason → act (via tools) → observe → adjust → continue** until the task is done. At a high level:\n\n1. **Interpret the user’s request**  \n   - The model reads the instructions, current context, and any memory.  \n   - It decides what subtasks are needed (plan), e.g.:\n     - “Understand the problem”\n     - “Look up data”\n     - “Take an action (e.g., send email, update record)”\n     - “Summarise the result for the user”\n\n2. **Decide whether a tool is needed**  \n   For each step, the model asks (implicitly):  \n   - Can I answer with my internal knowledge alone?  \n   - Do I need fresh or external data?  \n   - Do I need to change something in the world (write to DB, send message, trigger workflow)?  \n   If the answer is yes, it chooses a tool from its allowed **action-space**.\n\n3. **Formulate a tool call**  \n   - The model outputs a **structured request** to a specific tool (e.g., via MCP or another protocol).  \n   - This includes:\n     - Tool name (e.g. `query_database`, `send_email`)\n     - Arguments (e.g. SQL query, email address, message body)\n   - The agent framework or runtime executes the tool call; the model itself does not run the code or access the API directly.\n\n4. **Receive and interpret the tool’s output**  \n   - The tool returns structured data (e.g. JSON, text, status codes).  \n   - This output is fed back to the model as new context (an “observation”).  \n   - The model:\n     - Checks if the result makes sense\n     - Handles errors (e.g., “user not found”, “API timeout”)\n     - Decides whether to call another tool, adjust the plan, or finish.\n\n5. **Iterate with planning and reasoning**  \n   The model may repeat the cycle:\n   - Call more tools (e.g., first search a database, then send an email to the result list).\n   - Refine the plan based on new information.\n   - In a multi-agent system, hand off to another agent, or respond to a supervisor agent’s instructions.\n\n6. **Produce a final answer or action summary**  \n   Once the task is complete (or as complete as possible), the model:\n   - Synthesises the tool results into a user-facing response: explanation, summary, or confirmation.\n   - Optionally logs what tools were used and why (for auditing and safety).\n\n---\n\n### Concrete example\n\nUser: “Find all customers in Europe who haven’t logged in for 90 days and email them a reactivation offer.”\n\nThe model might:\n\n1. Plan:  \n   - Step 1: Query customer database  \n   - Step 2: Filter by region and last login  \n   - Step 3: Send emails  \n   - Step 4: Report back\n\n2. Call `query_customers` tool with appropriate filters.  \n3. Receive list of customers; check if it’s empty or too large.  \n4. Call `send_email` tool in batches with a prepared template.  \n5. After success/fail responses, reply to the user with a summary:  \n   - “I emailed 1,245 eligible customers; 3 addresses bounced.”\n\nThroughout, the model’s **reasoning** decides *which* tools to call and *in what order*, while the tools actually perform the operations."
        }
      ]
    },
    {
      "model_name": "gpt-5.1",
      "conversation_id": "conv-005",
      "turn_number": 3,
      "question": "What is action-space, and what determines the range of actions an agent is permitted to take?",
      "page_reference": 5,
      "response": "Action-space is the set of **all actions an agent is allowed and able to take** in the world. It defines the agent’s **practical authority and capabilities**—what it can *do*, not just what it can *say*.\n\nIn an agentic system, the action-space is mainly determined by:\n\n1. **Tools the agent can access**  \n   - Each tool (e.g., `query_database`, `send_payment`, `send_email`, `control_device`) adds possible actions.  \n   - If an agent has only read-only tools, it can observe but not change systems.  \n   - If it has write/transaction tools, it can modify data, send messages, trigger workflows, etc.\n\n2. **Permissions and constraints on those tools**  \n   Even if a tool exists, the agent’s use of it can be restricted by:\n   - **Scope**: which databases, APIs, or devices it can touch.  \n   - **Mode**: read-only vs read–write vs admin.  \n   - **Limits**: transaction caps, rate limits, geographic restrictions, etc.  \n   - **Guardrails**: “can refund up to $50 without approval”, “cannot delete records”, “can only email internal domains”.\n\n3. **System and role instructions**  \n   The instructions (system prompt, role definition, policies) tell the agent:\n   - What it is **for** (e.g., analyst vs operator vs finance assistant).  \n   - What it **should** and **should not** do, even if tools technically allow it.  \n   - When to ask for human approval or escalation.\n\n4. **Protocols and environment**  \n   - Protocols like **MCP** define *how* the agent can talk to tools.  \n   - Integration choices (which tools are wired up, how they’re exposed) shape what’s practically possible.  \n   - In multi-agent setups, some agents may only talk to other agents, not directly to external tools.\n\n5. **Governance, policy, and safety rules**  \n   - Organisational policies (compliance, privacy, security) can narrow the action-space.  \n   - Example: An HR agent may be blocked from accessing medical records; a support agent may be blocked from viewing full payment card data.  \n   - Logging, approvals, and audits can be required for high-risk actions, effectively constraining how freely the agent can act.\n\nPutting it together:\n\n- **Action-space = Tools × Permissions × Instructions × Policies.**  \n- By designing and limiting these, you control what the agent can actually do—its **authority** in the system—regardless of how capable its underlying model is at reasoning or generating text.",
      "latency_ms": 6109.513759613037,
      "api_latency_ms": 6109.513759613037,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 3.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external functions or services that the agent can call to take real actions in the world, beyond just generating text. The model decides *when* and *how* to use these tools, but the tools themselves do the acting.\n\nYou can think of tools as the main way an agent’s **action-space** (its practical capabilities and authority) is defined.\n\n### What are tools?\n\n- They are **interfaces** to other systems: APIs, databases, devices, apps, scripts, etc.\n- The agent calls them via a **protocol** (such as the Model Context Protocol, MCP), sending structured inputs and receiving structured outputs.\n- Tools let an agent:\n  - Read from the world (e.g., fetch data)\n  - Write to the world (e.g., update a database, send an email)\n  - Control other systems (e.g., trigger workflows, control devices)\n\nWithout tools, an agent is mostly limited to reasoning and generating text; with tools, it can actually *do* things.\n\n---\n\n### Types of actions tools can enable\n\nBelow are common categories of actions that tools provide, with examples.\n\n#### 1. Data access and retrieval\n\nTools that let an agent **read information** from external sources:\n\n- Query databases (SQL/NoSQL)\n- Call web APIs (weather, finance, maps, news, etc.)\n- Search the web or internal knowledge bases\n- Read documents or files from storage\n\nExample:  \n“Find all customers in Europe who haven’t logged in for 90 days” → tool queries a customer database and returns a list.\n\n---\n\n#### 2. Data creation, update, and deletion\n\nTools that let an agent **modify stored information**:\n\n- Insert/update/delete database records\n- Write to files, spreadsheets, or document stores\n- Update CRM entries, tickets, or user profiles\n\nExample:  \n“Create a new support ticket for this user” → tool writes a new record into the ticketing system.\n\n---\n\n#### 3. Communication and messaging\n\nTools that let an agent **communicate with people or systems**:\n\n- Send emails, chat messages, or SMS\n- Post to Slack/Teams channels\n- Create calendar events and invitations\n- Send notifications or alerts\n\nExample:  \n“Email the customer with a summary of our call and a follow-up date” → tool sends an email using a template the agent fills in.\n\n---\n\n#### 4. Workflow and process control\n\nTools that let an agent **trigger or orchestrate workflows**:\n\n- Start or stop automated workflows (e.g., CI/CD pipelines, ETL jobs)\n- Trigger business processes (order fulfillment, onboarding, approvals)\n- Interact with ticketing or issue tracking systems (Jira, ServiceNow)\n\nExample:  \n“Deploy the latest version of the app to staging if tests pass” → tools run tests and call a deployment pipeline.\n\n---\n\n#### 5. Transactions and financial operations\n\nTools that let an agent **perform transactions**:\n\n- Initiate payments or refunds\n- Move funds between accounts (within configured limits)\n- Create invoices or purchase orders\n- Check balances or transaction history\n\nExample:  \n“Issue a refund up to $50 if the product is defective” → tool processes a refund transaction.\n\n---\n\n#### 6. Device and environment control\n\nTools that let an agent **control physical or virtual devices**:\n\n- Smart home devices (lights, thermostats, locks)\n- Robots, drones, or manufacturing equipment\n- Cloud infrastructure (start/stop servers, scale resources)\n- Virtual desktops or browsers (click, type, navigate)\n\nExample:  \n“Turn off office lights after 8 p.m. if no motion is detected” → tool controls building systems.\n\n---\n\n#### 7. Content generation and transformation\n\nTools that let an agent **process or transform content** beyond what’s easy to do in-text:\n\n- Image, audio, or video generation and editing\n- OCR (extract text from images/PDFs)\n- Code execution and evaluation (e.g., Python sandboxes)\n- Format conversion (PDF ↔ Word, CSV ↔ JSON)\n\nExample:  \n“Extract all tables from this scanned PDF and put them into a spreadsheet” → tools run OCR and write to a sheet.\n\n---\n\n#### 8. Security, identity, and access control\n\nTools that let an agent **respect and enforce permissions**:\n\n- Authenticate users (login, tokens)\n- Check roles and permissions\n- Log actions for auditing\n- Request approvals for sensitive actions\n\nExample:  \n“Reset the user’s password only after verifying their identity” → tools perform identity checks and then reset credentials.\n\n---\n\n### How tools relate to an agent’s capabilities\n\n- The **set of tools** an agent is allowed to use defines its **action-space**:\n  - A “read-only analyst” agent might only have data query tools.\n  - A “support agent” might also have tools to update tickets and send emails.\n  - A “finance agent” might have payment tools but with strict limits and approval flows.\n- Governance and safety are implemented by:\n  - Choosing which tools the agent can access\n  - Imposing limits (e.g., transaction caps, read-only vs write)\n  - Logging and reviewing tool calls\n\n---\n\nIn summary:  \n**Tools are the bridges between an agent’s reasoning and real-world impact.** They let the agent not just think and talk, but also fetch data, update systems, communicate, control devices, and perform transactions—within whatever boundaries designers set."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools as part of a loop of **reason → act (via tools) → observe → adjust → continue** until the task is done. At a high level:\n\n1. **Interpret the user’s request**  \n   - The model reads the instructions, current context, and any memory.  \n   - It decides what subtasks are needed (plan), e.g.:\n     - “Understand the problem”\n     - “Look up data”\n     - “Take an action (e.g., send email, update record)”\n     - “Summarise the result for the user”\n\n2. **Decide whether a tool is needed**  \n   For each step, the model asks (implicitly):  \n   - Can I answer with my internal knowledge alone?  \n   - Do I need fresh or external data?  \n   - Do I need to change something in the world (write to DB, send message, trigger workflow)?  \n   If the answer is yes, it chooses a tool from its allowed **action-space**.\n\n3. **Formulate a tool call**  \n   - The model outputs a **structured request** to a specific tool (e.g., via MCP or another protocol).  \n   - This includes:\n     - Tool name (e.g. `query_database`, `send_email`)\n     - Arguments (e.g. SQL query, email address, message body)\n   - The agent framework or runtime executes the tool call; the model itself does not run the code or access the API directly.\n\n4. **Receive and interpret the tool’s output**  \n   - The tool returns structured data (e.g. JSON, text, status codes).  \n   - This output is fed back to the model as new context (an “observation”).  \n   - The model:\n     - Checks if the result makes sense\n     - Handles errors (e.g., “user not found”, “API timeout”)\n     - Decides whether to call another tool, adjust the plan, or finish.\n\n5. **Iterate with planning and reasoning**  \n   The model may repeat the cycle:\n   - Call more tools (e.g., first search a database, then send an email to the result list).\n   - Refine the plan based on new information.\n   - In a multi-agent system, hand off to another agent, or respond to a supervisor agent’s instructions.\n\n6. **Produce a final answer or action summary**  \n   Once the task is complete (or as complete as possible), the model:\n   - Synthesises the tool results into a user-facing response: explanation, summary, or confirmation.\n   - Optionally logs what tools were used and why (for auditing and safety).\n\n---\n\n### Concrete example\n\nUser: “Find all customers in Europe who haven’t logged in for 90 days and email them a reactivation offer.”\n\nThe model might:\n\n1. Plan:  \n   - Step 1: Query customer database  \n   - Step 2: Filter by region and last login  \n   - Step 3: Send emails  \n   - Step 4: Report back\n\n2. Call `query_customers` tool with appropriate filters.  \n3. Receive list of customers; check if it’s empty or too large.  \n4. Call `send_email` tool in batches with a prepared template.  \n5. After success/fail responses, reply to the user with a summary:  \n   - “I emailed 1,245 eligible customers; 3 addresses bounced.”\n\nThroughout, the model’s **reasoning** decides *which* tools to call and *in what order*, while the tools actually perform the operations."
        }
      ],
      "prompt_tokens": 2534,
      "completion_tokens": 565,
      "cached_tokens": 0,
      "total_tokens": 3099,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external functions or services that the agent can call to take real actions in the world, beyond just generating text. The model decides *when* and *how* to use these tools, but the tools themselves do the acting.\n\nYou can think of tools as the main way an agent’s **action-space** (its practical capabilities and authority) is defined.\n\n### What are tools?\n\n- They are **interfaces** to other systems: APIs, databases, devices, apps, scripts, etc.\n- The agent calls them via a **protocol** (such as the Model Context Protocol, MCP), sending structured inputs and receiving structured outputs.\n- Tools let an agent:\n  - Read from the world (e.g., fetch data)\n  - Write to the world (e.g., update a database, send an email)\n  - Control other systems (e.g., trigger workflows, control devices)\n\nWithout tools, an agent is mostly limited to reasoning and generating text; with tools, it can actually *do* things.\n\n---\n\n### Types of actions tools can enable\n\nBelow are common categories of actions that tools provide, with examples.\n\n#### 1. Data access and retrieval\n\nTools that let an agent **read information** from external sources:\n\n- Query databases (SQL/NoSQL)\n- Call web APIs (weather, finance, maps, news, etc.)\n- Search the web or internal knowledge bases\n- Read documents or files from storage\n\nExample:  \n“Find all customers in Europe who haven’t logged in for 90 days” → tool queries a customer database and returns a list.\n\n---\n\n#### 2. Data creation, update, and deletion\n\nTools that let an agent **modify stored information**:\n\n- Insert/update/delete database records\n- Write to files, spreadsheets, or document stores\n- Update CRM entries, tickets, or user profiles\n\nExample:  \n“Create a new support ticket for this user” → tool writes a new record into the ticketing system.\n\n---\n\n#### 3. Communication and messaging\n\nTools that let an agent **communicate with people or systems**:\n\n- Send emails, chat messages, or SMS\n- Post to Slack/Teams channels\n- Create calendar events and invitations\n- Send notifications or alerts\n\nExample:  \n“Email the customer with a summary of our call and a follow-up date” → tool sends an email using a template the agent fills in.\n\n---\n\n#### 4. Workflow and process control\n\nTools that let an agent **trigger or orchestrate workflows**:\n\n- Start or stop automated workflows (e.g., CI/CD pipelines, ETL jobs)\n- Trigger business processes (order fulfillment, onboarding, approvals)\n- Interact with ticketing or issue tracking systems (Jira, ServiceNow)\n\nExample:  \n“Deploy the latest version of the app to staging if tests pass” → tools run tests and call a deployment pipeline.\n\n---\n\n#### 5. Transactions and financial operations\n\nTools that let an agent **perform transactions**:\n\n- Initiate payments or refunds\n- Move funds between accounts (within configured limits)\n- Create invoices or purchase orders\n- Check balances or transaction history\n\nExample:  \n“Issue a refund up to $50 if the product is defective” → tool processes a refund transaction.\n\n---\n\n#### 6. Device and environment control\n\nTools that let an agent **control physical or virtual devices**:\n\n- Smart home devices (lights, thermostats, locks)\n- Robots, drones, or manufacturing equipment\n- Cloud infrastructure (start/stop servers, scale resources)\n- Virtual desktops or browsers (click, type, navigate)\n\nExample:  \n“Turn off office lights after 8 p.m. if no motion is detected” → tool controls building systems.\n\n---\n\n#### 7. Content generation and transformation\n\nTools that let an agent **process or transform content** beyond what’s easy to do in-text:\n\n- Image, audio, or video generation and editing\n- OCR (extract text from images/PDFs)\n- Code execution and evaluation (e.g., Python sandboxes)\n- Format conversion (PDF ↔ Word, CSV ↔ JSON)\n\nExample:  \n“Extract all tables from this scanned PDF and put them into a spreadsheet” → tools run OCR and write to a sheet.\n\n---\n\n#### 8. Security, identity, and access control\n\nTools that let an agent **respect and enforce permissions**:\n\n- Authenticate users (login, tokens)\n- Check roles and permissions\n- Log actions for auditing\n- Request approvals for sensitive actions\n\nExample:  \n“Reset the user’s password only after verifying their identity” → tools perform identity checks and then reset credentials.\n\n---\n\n### How tools relate to an agent’s capabilities\n\n- The **set of tools** an agent is allowed to use defines its **action-space**:\n  - A “read-only analyst” agent might only have data query tools.\n  - A “support agent” might also have tools to update tickets and send emails.\n  - A “finance agent” might have payment tools but with strict limits and approval flows.\n- Governance and safety are implemented by:\n  - Choosing which tools the agent can access\n  - Imposing limits (e.g., transaction caps, read-only vs write)\n  - Logging and reviewing tool calls\n\n---\n\nIn summary:  \n**Tools are the bridges between an agent’s reasoning and real-world impact.** They let the agent not just think and talk, but also fetch data, update systems, communicate, control devices, and perform transactions—within whatever boundaries designers set."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools as part of a loop of **reason → act (via tools) → observe → adjust → continue** until the task is done. At a high level:\n\n1. **Interpret the user’s request**  \n   - The model reads the instructions, current context, and any memory.  \n   - It decides what subtasks are needed (plan), e.g.:\n     - “Understand the problem”\n     - “Look up data”\n     - “Take an action (e.g., send email, update record)”\n     - “Summarise the result for the user”\n\n2. **Decide whether a tool is needed**  \n   For each step, the model asks (implicitly):  \n   - Can I answer with my internal knowledge alone?  \n   - Do I need fresh or external data?  \n   - Do I need to change something in the world (write to DB, send message, trigger workflow)?  \n   If the answer is yes, it chooses a tool from its allowed **action-space**.\n\n3. **Formulate a tool call**  \n   - The model outputs a **structured request** to a specific tool (e.g., via MCP or another protocol).  \n   - This includes:\n     - Tool name (e.g. `query_database`, `send_email`)\n     - Arguments (e.g. SQL query, email address, message body)\n   - The agent framework or runtime executes the tool call; the model itself does not run the code or access the API directly.\n\n4. **Receive and interpret the tool’s output**  \n   - The tool returns structured data (e.g. JSON, text, status codes).  \n   - This output is fed back to the model as new context (an “observation”).  \n   - The model:\n     - Checks if the result makes sense\n     - Handles errors (e.g., “user not found”, “API timeout”)\n     - Decides whether to call another tool, adjust the plan, or finish.\n\n5. **Iterate with planning and reasoning**  \n   The model may repeat the cycle:\n   - Call more tools (e.g., first search a database, then send an email to the result list).\n   - Refine the plan based on new information.\n   - In a multi-agent system, hand off to another agent, or respond to a supervisor agent’s instructions.\n\n6. **Produce a final answer or action summary**  \n   Once the task is complete (or as complete as possible), the model:\n   - Synthesises the tool results into a user-facing response: explanation, summary, or confirmation.\n   - Optionally logs what tools were used and why (for auditing and safety).\n\n---\n\n### Concrete example\n\nUser: “Find all customers in Europe who haven’t logged in for 90 days and email them a reactivation offer.”\n\nThe model might:\n\n1. Plan:  \n   - Step 1: Query customer database  \n   - Step 2: Filter by region and last login  \n   - Step 3: Send emails  \n   - Step 4: Report back\n\n2. Call `query_customers` tool with appropriate filters.  \n3. Receive list of customers; check if it’s empty or too large.  \n4. Call `send_email` tool in batches with a prepared template.  \n5. After success/fail responses, reply to the user with a summary:  \n   - “I emailed 1,245 eligible customers; 3 addresses bounced.”\n\nThroughout, the model’s **reasoning** decides *which* tools to call and *in what order*, while the tools actually perform the operations."
        },
        {
          "role": "user",
          "content": "What is action-space, and what determines the range of actions an agent is permitted to take?"
        },
        {
          "role": "assistant",
          "content": "Action-space is the set of **all actions an agent is allowed and able to take** in the world. It defines the agent’s **practical authority and capabilities**—what it can *do*, not just what it can *say*.\n\nIn an agentic system, the action-space is mainly determined by:\n\n1. **Tools the agent can access**  \n   - Each tool (e.g., `query_database`, `send_payment`, `send_email`, `control_device`) adds possible actions.  \n   - If an agent has only read-only tools, it can observe but not change systems.  \n   - If it has write/transaction tools, it can modify data, send messages, trigger workflows, etc.\n\n2. **Permissions and constraints on those tools**  \n   Even if a tool exists, the agent’s use of it can be restricted by:\n   - **Scope**: which databases, APIs, or devices it can touch.  \n   - **Mode**: read-only vs read–write vs admin.  \n   - **Limits**: transaction caps, rate limits, geographic restrictions, etc.  \n   - **Guardrails**: “can refund up to $50 without approval”, “cannot delete records”, “can only email internal domains”.\n\n3. **System and role instructions**  \n   The instructions (system prompt, role definition, policies) tell the agent:\n   - What it is **for** (e.g., analyst vs operator vs finance assistant).  \n   - What it **should** and **should not** do, even if tools technically allow it.  \n   - When to ask for human approval or escalation.\n\n4. **Protocols and environment**  \n   - Protocols like **MCP** define *how* the agent can talk to tools.  \n   - Integration choices (which tools are wired up, how they’re exposed) shape what’s practically possible.  \n   - In multi-agent setups, some agents may only talk to other agents, not directly to external tools.\n\n5. **Governance, policy, and safety rules**  \n   - Organisational policies (compliance, privacy, security) can narrow the action-space.  \n   - Example: An HR agent may be blocked from accessing medical records; a support agent may be blocked from viewing full payment card data.  \n   - Logging, approvals, and audits can be required for high-risk actions, effectively constraining how freely the agent can act.\n\nPutting it together:\n\n- **Action-space = Tools × Permissions × Instructions × Policies.**  \n- By designing and limiting these, you control what the agent can actually do—its **authority** in the system—regardless of how capable its underlying model is at reasoning or generating text."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-001",
      "turn_number": 1,
      "question": "What topics are covered under the Model AI Governance Framework for Agentic AI?",
      "page_reference": 1,
      "response": "The **Model AI Governance Framework for Agentic AI** covers the following main topics:\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment\n   - Bound risks through design by defining agents’ limits and permissions\n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation\n   - Design for meaningful human oversight\n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls\n   - Before deploying, test agents\n   - When deploying, continuously monitor and test\n\n4. **Enable end-user responsibility**\n   - Different users, different needs\n   - Users who interact with agents\n   - Users who integrate agents into their work processes\n\nThese topics collectively address risk assessment, accountability, technical safeguards, and user responsibilities in the governance of Agentic AI systems.",
      "latency_ms": 8347.432374954224,
      "api_latency_ms": 8347.432374954224,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 468,
      "completion_tokens": 178,
      "cached_tokens": 0,
      "total_tokens": 646,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following main topics:\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment\n   - Bound risks through design by defining agents’ limits and permissions\n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation\n   - Design for meaningful human oversight\n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls\n   - Before deploying, test agents\n   - When deploying, continuously monitor and test\n\n4. **Enable end-user responsibility**\n   - Different users, different needs\n   - Users who interact with agents\n   - Users who integrate agents into their work processes\n\nThese topics collectively address risk assessment, accountability, technical safeguards, and user responsibilities in the governance of Agentic AI systems."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-001",
      "turn_number": 2,
      "question": "Within that framework, what are the specific steps listed under implementing technical controls and processes?",
      "page_reference": 1,
      "response": "Within the **Model AI Governance Framework for Agentic AI**, the specific steps listed under **implementing technical controls and processes** are:\n\n1. **During design and development, use technical controls**\n2. **Before deploying, test agents**\n3. **When deploying, continuously monitor and test**\n\nThese steps emphasize incorporating technical safeguards throughout the lifecycle of agentic AI—from initial design and development, to pre-deployment testing, and ongoing monitoring and testing during deployment.",
      "latency_ms": 1482.6366901397705,
      "api_latency_ms": 1482.6366901397705,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following main topics:\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment\n   - Bound risks through design by defining agents’ limits and permissions\n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation\n   - Design for meaningful human oversight\n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls\n   - Before deploying, test agents\n   - When deploying, continuously monitor and test\n\n4. **Enable end-user responsibility**\n   - Different users, different needs\n   - Users who interact with agents\n   - Users who integrate agents into their work processes\n\nThese topics collectively address risk assessment, accountability, technical safeguards, and user responsibilities in the governance of Agentic AI systems."
        }
      ],
      "prompt_tokens": 670,
      "completion_tokens": 93,
      "cached_tokens": 0,
      "total_tokens": 763,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following main topics:\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment\n   - Bound risks through design by defining agents’ limits and permissions\n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation\n   - Design for meaningful human oversight\n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls\n   - Before deploying, test agents\n   - When deploying, continuously monitor and test\n\n4. **Enable end-user responsibility**\n   - Different users, different needs\n   - Users who interact with agents\n   - Users who integrate agents into their work processes\n\nThese topics collectively address risk assessment, accountability, technical safeguards, and user responsibilities in the governance of Agentic AI systems."
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Within the **Model AI Governance Framework for Agentic AI**, the specific steps listed under **implementing technical controls and processes** are:\n\n1. **During design and development, use technical controls**\n2. **Before deploying, test agents**\n3. **When deploying, continuously monitor and test**\n\nThese steps emphasize incorporating technical safeguards throughout the lifecycle of agentic AI—from initial design and development, to pre-deployment testing, and ongoing monitoring and testing during deployment."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-001",
      "turn_number": 3,
      "question": "How does the framework describe enabling end-user responsibility in terms of different user types?",
      "page_reference": 1,
      "response": "The framework describes enabling end-user responsibility by acknowledging that **different user types have different needs and responsibilities** when interacting with agentic AI systems. It distinguishes between:\n\n1. **Users who interact with agents**  \n   These users engage directly with agentic AI through interfaces or outputs. The framework suggests that responsibilities for these users include understanding how to use agents safely, being aware of agent limitations, and being able to report issues or unexpected behaviors.\n\n2. **Users who integrate agents into their work processes**  \n   These users go further by embedding agentic AI into broader workflows, products, or services. Their responsibilities may include ensuring that the integration aligns with safety and governance requirements, monitoring agent performance within their specific context, and establishing processes for oversight and intervention if needed.\n\nBy differentiating between these user types, the framework recommends tailoring support, guidance, and controls to match the specific contexts and responsibilities of each group—thus enabling meaningful end-user responsibility.",
      "latency_ms": 3028.982400894165,
      "api_latency_ms": 3028.982400894165,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 3.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following main topics:\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment\n   - Bound risks through design by defining agents’ limits and permissions\n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation\n   - Design for meaningful human oversight\n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls\n   - Before deploying, test agents\n   - When deploying, continuously monitor and test\n\n4. **Enable end-user responsibility**\n   - Different users, different needs\n   - Users who interact with agents\n   - Users who integrate agents into their work processes\n\nThese topics collectively address risk assessment, accountability, technical safeguards, and user responsibilities in the governance of Agentic AI systems."
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Within the **Model AI Governance Framework for Agentic AI**, the specific steps listed under **implementing technical controls and processes** are:\n\n1. **During design and development, use technical controls**\n2. **Before deploying, test agents**\n3. **When deploying, continuously monitor and test**\n\nThese steps emphasize incorporating technical safeguards throughout the lifecycle of agentic AI—from initial design and development, to pre-deployment testing, and ongoing monitoring and testing during deployment."
        }
      ],
      "prompt_tokens": 786,
      "completion_tokens": 190,
      "cached_tokens": 0,
      "total_tokens": 976,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following main topics:\n\n1. **Assess and bound the risks upfront**\n   - Determine suitable use cases for agent deployment\n   - Bound risks through design by defining agents’ limits and permissions\n\n2. **Make humans meaningfully accountable**\n   - Clear allocation of responsibilities within and outside the organisation\n   - Design for meaningful human oversight\n\n3. **Implement technical controls and processes**\n   - During design and development, use technical controls\n   - Before deploying, test agents\n   - When deploying, continuously monitor and test\n\n4. **Enable end-user responsibility**\n   - Different users, different needs\n   - Users who interact with agents\n   - Users who integrate agents into their work processes\n\nThese topics collectively address risk assessment, accountability, technical safeguards, and user responsibilities in the governance of Agentic AI systems."
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Within the **Model AI Governance Framework for Agentic AI**, the specific steps listed under **implementing technical controls and processes** are:\n\n1. **During design and development, use technical controls**\n2. **Before deploying, test agents**\n3. **When deploying, continuously monitor and test**\n\nThese steps emphasize incorporating technical safeguards throughout the lifecycle of agentic AI—from initial design and development, to pre-deployment testing, and ongoing monitoring and testing during deployment."
        },
        {
          "role": "user",
          "content": "How does the framework describe enabling end-user responsibility in terms of different user types?"
        },
        {
          "role": "assistant",
          "content": "The framework describes enabling end-user responsibility by acknowledging that **different user types have different needs and responsibilities** when interacting with agentic AI systems. It distinguishes between:\n\n1. **Users who interact with agents**  \n   These users engage directly with agentic AI through interfaces or outputs. The framework suggests that responsibilities for these users include understanding how to use agents safely, being aware of agent limitations, and being able to report issues or unexpected behaviors.\n\n2. **Users who integrate agents into their work processes**  \n   These users go further by embedding agentic AI into broader workflows, products, or services. Their responsibilities may include ensuring that the integration aligns with safety and governance requirements, monitoring agent performance within their specific context, and establishing processes for oversight and intervention if needed.\n\nBy differentiating between these user types, the framework recommends tailoring support, guidance, and controls to match the specific contexts and responsibilities of each group—thus enabling meaningful end-user responsibility."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-002",
      "turn_number": 1,
      "question": "What capabilities distinguish AI agents from generative AI?",
      "page_reference": 2,
      "response": "AI agents are distinguished from generative AI by several enhanced capabilities:\n\n1. **Ability to Take Actions:**  \n   - Unlike generative AI, which mainly produces content (text, images, code, etc.), AI agents can **take actions** in digital environments. For example, they can update databases, make payments, or interact with other software systems.\n\n2. **Adaptation to New Information:**  \n   - AI agents can **adapt** their behavior in response to new or changing information, allowing them to adjust their actions dynamically as tasks progress.\n\n3. **Interaction with Other Agents and Systems:**  \n   - AI agents can **interact with other agents and external systems** to accomplish complex tasks, whereas generative AI typically operates in isolation, producing outputs without direct environmental interaction.\n\n4. **Task Completion on Behalf of Humans:**  \n   - AI agents are designed to **autonomously complete multi-step tasks** on behalf of users, rather than just generating responses or outputs for immediate human use.\n\nIn summary, while generative AI focuses on content creation, AI agents are capable of autonomous action, environment interaction, and adaptive task execution.",
      "latency_ms": 5040.279388427734,
      "api_latency_ms": 5040.279388427734,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 621,
      "completion_tokens": 231,
      "cached_tokens": 0,
      "total_tokens": 852,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from generative AI by several enhanced capabilities:\n\n1. **Ability to Take Actions:**  \n   - Unlike generative AI, which mainly produces content (text, images, code, etc.), AI agents can **take actions** in digital environments. For example, they can update databases, make payments, or interact with other software systems.\n\n2. **Adaptation to New Information:**  \n   - AI agents can **adapt** their behavior in response to new or changing information, allowing them to adjust their actions dynamically as tasks progress.\n\n3. **Interaction with Other Agents and Systems:**  \n   - AI agents can **interact with other agents and external systems** to accomplish complex tasks, whereas generative AI typically operates in isolation, producing outputs without direct environmental interaction.\n\n4. **Task Completion on Behalf of Humans:**  \n   - AI agents are designed to **autonomously complete multi-step tasks** on behalf of users, rather than just generating responses or outputs for immediate human use.\n\nIn summary, while generative AI focuses on content creation, AI agents are capable of autonomous action, environment interaction, and adaptive task execution."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-002",
      "turn_number": 2,
      "question": "What workplace examples are given for how agents are already transforming work?",
      "page_reference": 2,
      "response": "The workplace examples given for how agents are already transforming work include:\n\n- **Coding assistants**\n- **Customer service agents**\n- **Automating enterprise productivity workflows**\n\nThese examples illustrate how AI agents are being used to assist with coding tasks, handle customer inquiries, and automate various business processes to increase efficiency.",
      "latency_ms": 1486.6812229156494,
      "api_latency_ms": 1486.6812229156494,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from generative AI by several enhanced capabilities:\n\n1. **Ability to Take Actions:**  \n   - Unlike generative AI, which mainly produces content (text, images, code, etc.), AI agents can **take actions** in digital environments. For example, they can update databases, make payments, or interact with other software systems.\n\n2. **Adaptation to New Information:**  \n   - AI agents can **adapt** their behavior in response to new or changing information, allowing them to adjust their actions dynamically as tasks progress.\n\n3. **Interaction with Other Agents and Systems:**  \n   - AI agents can **interact with other agents and external systems** to accomplish complex tasks, whereas generative AI typically operates in isolation, producing outputs without direct environmental interaction.\n\n4. **Task Completion on Behalf of Humans:**  \n   - AI agents are designed to **autonomously complete multi-step tasks** on behalf of users, rather than just generating responses or outputs for immediate human use.\n\nIn summary, while generative AI focuses on content creation, AI agents are capable of autonomous action, environment interaction, and adaptive task execution."
        }
      ],
      "prompt_tokens": 872,
      "completion_tokens": 61,
      "cached_tokens": 0,
      "total_tokens": 933,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from generative AI by several enhanced capabilities:\n\n1. **Ability to Take Actions:**  \n   - Unlike generative AI, which mainly produces content (text, images, code, etc.), AI agents can **take actions** in digital environments. For example, they can update databases, make payments, or interact with other software systems.\n\n2. **Adaptation to New Information:**  \n   - AI agents can **adapt** their behavior in response to new or changing information, allowing them to adjust their actions dynamically as tasks progress.\n\n3. **Interaction with Other Agents and Systems:**  \n   - AI agents can **interact with other agents and external systems** to accomplish complex tasks, whereas generative AI typically operates in isolation, producing outputs without direct environmental interaction.\n\n4. **Task Completion on Behalf of Humans:**  \n   - AI agents are designed to **autonomously complete multi-step tasks** on behalf of users, rather than just generating responses or outputs for immediate human use.\n\nIn summary, while generative AI focuses on content creation, AI agents are capable of autonomous action, environment interaction, and adaptive task execution."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The workplace examples given for how agents are already transforming work include:\n\n- **Coding assistants**\n- **Customer service agents**\n- **Automating enterprise productivity workflows**\n\nThese examples illustrate how AI agents are being used to assist with coding tasks, handle customer inquiries, and automate various business processes to increase efficiency."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-002",
      "turn_number": 3,
      "question": "What factors determine the risks posed by an agent’s actions, and what are two planning-stage steps organisations can take to manage those risks early?",
      "page_reference": 2,
      "response": "**Factors that determine the risks posed by an agent’s actions:**\n1. **Scope of Actions the Agent Can Take:** What the agent is allowed to do (e.g., access to databases, ability to make payments).\n2. **Reversibility of Those Actions:** Whether the actions the agent takes can be undone or corrected if something goes wrong.\n3. **Agent’s Level of Autonomy:** How independently the agent operates without human intervention.\n\n**Two planning-stage steps organisations can take to manage those risks early:**\n1. **Limit the Scope of Impact:** Design appropriate boundaries for the agent at the planning stage, such as restricting the agent’s access to tools and external systems.\n2. **Establish Traceability and Control:** Ensure the agent’s actions are traceable and controllable by setting up robust identity management and access controls for agents.",
      "latency_ms": 3637.159824371338,
      "api_latency_ms": 3637.159824371338,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from generative AI by several enhanced capabilities:\n\n1. **Ability to Take Actions:**  \n   - Unlike generative AI, which mainly produces content (text, images, code, etc.), AI agents can **take actions** in digital environments. For example, they can update databases, make payments, or interact with other software systems.\n\n2. **Adaptation to New Information:**  \n   - AI agents can **adapt** their behavior in response to new or changing information, allowing them to adjust their actions dynamically as tasks progress.\n\n3. **Interaction with Other Agents and Systems:**  \n   - AI agents can **interact with other agents and external systems** to accomplish complex tasks, whereas generative AI typically operates in isolation, producing outputs without direct environmental interaction.\n\n4. **Task Completion on Behalf of Humans:**  \n   - AI agents are designed to **autonomously complete multi-step tasks** on behalf of users, rather than just generating responses or outputs for immediate human use.\n\nIn summary, while generative AI focuses on content creation, AI agents are capable of autonomous action, environment interaction, and adaptive task execution."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The workplace examples given for how agents are already transforming work include:\n\n- **Coding assistants**\n- **Customer service agents**\n- **Automating enterprise productivity workflows**\n\nThese examples illustrate how AI agents are being used to assist with coding tasks, handle customer inquiries, and automate various business processes to increase efficiency."
        }
      ],
      "prompt_tokens": 968,
      "completion_tokens": 172,
      "cached_tokens": 0,
      "total_tokens": 1140,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents are distinguished from generative AI by several enhanced capabilities:\n\n1. **Ability to Take Actions:**  \n   - Unlike generative AI, which mainly produces content (text, images, code, etc.), AI agents can **take actions** in digital environments. For example, they can update databases, make payments, or interact with other software systems.\n\n2. **Adaptation to New Information:**  \n   - AI agents can **adapt** their behavior in response to new or changing information, allowing them to adjust their actions dynamically as tasks progress.\n\n3. **Interaction with Other Agents and Systems:**  \n   - AI agents can **interact with other agents and external systems** to accomplish complex tasks, whereas generative AI typically operates in isolation, producing outputs without direct environmental interaction.\n\n4. **Task Completion on Behalf of Humans:**  \n   - AI agents are designed to **autonomously complete multi-step tasks** on behalf of users, rather than just generating responses or outputs for immediate human use.\n\nIn summary, while generative AI focuses on content creation, AI agents are capable of autonomous action, environment interaction, and adaptive task execution."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The workplace examples given for how agents are already transforming work include:\n\n- **Coding assistants**\n- **Customer service agents**\n- **Automating enterprise productivity workflows**\n\nThese examples illustrate how AI agents are being used to assist with coding tasks, handle customer inquiries, and automate various business processes to increase efficiency."
        },
        {
          "role": "user",
          "content": "What factors determine the risks posed by an agent’s actions, and what are two planning-stage steps organisations can take to manage those risks early?"
        },
        {
          "role": "assistant",
          "content": "**Factors that determine the risks posed by an agent’s actions:**\n1. **Scope of Actions the Agent Can Take:** What the agent is allowed to do (e.g., access to databases, ability to make payments).\n2. **Reversibility of Those Actions:** Whether the actions the agent takes can be undone or corrected if something goes wrong.\n3. **Agent’s Level of Autonomy:** How independently the agent operates without human intervention.\n\n**Two planning-stage steps organisations can take to manage those risks early:**\n1. **Limit the Scope of Impact:** Design appropriate boundaries for the agent at the planning stage, such as restricting the agent’s access to tools and external systems.\n2. **Establish Traceability and Control:** Ensure the agent’s actions are traceable and controllable by setting up robust identity management and access controls for agents."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-003",
      "turn_number": 1,
      "question": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?",
      "page_reference": 3,
      "response": "“Human-in-the-loop” should be adapted to address automation bias in increasingly capable agents by:\n\n1. **Defining Significant Checkpoints:** Establish specific points in the agentic workflow where human approval is mandatory, especially for high-stakes or irreversible actions. This ensures humans remain actively involved in critical decisions and do not simply defer to the agent’s recommendations.\n\n2. **Regularly Auditing Human Oversight:** Continually assess and audit human involvement to ensure human oversight remains effective over time and does not diminish due to over-reliance on the agent.\n\nBy implementing these measures, organisations can reduce the risk of automation bias, where humans may place too much trust in the agent’s decisions, and ensure responsible and safe operation of increasingly capable AI agents.",
      "latency_ms": 5020.19739151001,
      "api_latency_ms": 5020.19739151001,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 497,
      "completion_tokens": 151,
      "cached_tokens": 0,
      "total_tokens": 648,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to address automation bias in increasingly capable agents by:\n\n1. **Defining Significant Checkpoints:** Establish specific points in the agentic workflow where human approval is mandatory, especially for high-stakes or irreversible actions. This ensures humans remain actively involved in critical decisions and do not simply defer to the agent’s recommendations.\n\n2. **Regularly Auditing Human Oversight:** Continually assess and audit human involvement to ensure human oversight remains effective over time and does not diminish due to over-reliance on the agent.\n\nBy implementing these measures, organisations can reduce the risk of automation bias, where humans may place too much trust in the agent’s decisions, and ensure responsible and safe operation of increasingly capable AI agents."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-003",
      "turn_number": 2,
      "question": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?",
      "page_reference": 3,
      "response": "**Checkpoints in an agentic workflow that should require human approval include:**\n\n- **High-stakes actions:** Any decisions or operations that could have significant legal, financial, safety, or reputational consequences for the organisation.\n- **Irreversible actions:** Steps that cannot be undone or easily reversed, such as deleting critical data, executing large financial transactions, or making permanent changes to systems or records.\n- **Boundary or exception cases:** Situations where the agent encounters scenarios outside its normal operating parameters, novel problems, or ambiguous contexts that require nuanced judgment.\n- **Policy deviations:** When an agent’s proposed action deviates from established organisational policies or standard operating procedures.\n\n**Ongoing activity recommended to ensure human oversight remains effective:**\n\n- **Regular auditing of human oversight:** Continuously review and evaluate how humans interact with agentic workflows to ensure they are not becoming overly reliant on the agent (i.e., to avoid automation bias), and that humans are consistently exercising informed and active oversight. This may include periodic audits, user feedback collection, and scenario-based drills to test human intervention in critical moments.",
      "latency_ms": 3017.680883407593,
      "api_latency_ms": 3017.680883407593,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 3.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to address automation bias in increasingly capable agents by:\n\n1. **Defining Significant Checkpoints:** Establish specific points in the agentic workflow where human approval is mandatory, especially for high-stakes or irreversible actions. This ensures humans remain actively involved in critical decisions and do not simply defer to the agent’s recommendations.\n\n2. **Regularly Auditing Human Oversight:** Continually assess and audit human involvement to ensure human oversight remains effective over time and does not diminish due to over-reliance on the agent.\n\nBy implementing these measures, organisations can reduce the risk of automation bias, where humans may place too much trust in the agent’s decisions, and ensure responsible and safe operation of increasingly capable AI agents."
        }
      ],
      "prompt_tokens": 684,
      "completion_tokens": 221,
      "cached_tokens": 0,
      "total_tokens": 905,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to address automation bias in increasingly capable agents by:\n\n1. **Defining Significant Checkpoints:** Establish specific points in the agentic workflow where human approval is mandatory, especially for high-stakes or irreversible actions. This ensures humans remain actively involved in critical decisions and do not simply defer to the agent’s recommendations.\n\n2. **Regularly Auditing Human Oversight:** Continually assess and audit human involvement to ensure human oversight remains effective over time and does not diminish due to over-reliance on the agent.\n\nBy implementing these measures, organisations can reduce the risk of automation bias, where humans may place too much trust in the agent’s decisions, and ensure responsible and safe operation of increasingly capable AI agents."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "**Checkpoints in an agentic workflow that should require human approval include:**\n\n- **High-stakes actions:** Any decisions or operations that could have significant legal, financial, safety, or reputational consequences for the organisation.\n- **Irreversible actions:** Steps that cannot be undone or easily reversed, such as deleting critical data, executing large financial transactions, or making permanent changes to systems or records.\n- **Boundary or exception cases:** Situations where the agent encounters scenarios outside its normal operating parameters, novel problems, or ambiguous contexts that require nuanced judgment.\n- **Policy deviations:** When an agent’s proposed action deviates from established organisational policies or standard operating procedures.\n\n**Ongoing activity recommended to ensure human oversight remains effective:**\n\n- **Regular auditing of human oversight:** Continuously review and evaluate how humans interact with agentic workflows to ensure they are not becoming overly reliant on the agent (i.e., to avoid automation bias), and that humans are consistently exercising informed and active oversight. This may include periodic audits, user feedback collection, and scenario-based drills to test human intervention in critical moments."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-003",
      "turn_number": 3,
      "question": "What is the recommended approach during and after deployment to handle the fact that agents interact dynamically with their environment and not all risks can be anticipated upfront?",
      "page_reference": 3,
      "response": "The recommended approach during and after deployment is to **gradually roll out agents alongside continuous monitoring after deployment**. This staged deployment allows organisations to observe how agents behave in real-world, dynamic environments, identifying and addressing risks that could not be fully anticipated upfront. Continuous monitoring ensures that new or evolving risks are detected promptly, allowing for timely intervention and adaptation of controls as needed.",
      "latency_ms": 1573.4190940856934,
      "api_latency_ms": 1573.4190940856934,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to address automation bias in increasingly capable agents by:\n\n1. **Defining Significant Checkpoints:** Establish specific points in the agentic workflow where human approval is mandatory, especially for high-stakes or irreversible actions. This ensures humans remain actively involved in critical decisions and do not simply defer to the agent’s recommendations.\n\n2. **Regularly Auditing Human Oversight:** Continually assess and audit human involvement to ensure human oversight remains effective over time and does not diminish due to over-reliance on the agent.\n\nBy implementing these measures, organisations can reduce the risk of automation bias, where humans may place too much trust in the agent’s decisions, and ensure responsible and safe operation of increasingly capable AI agents."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "**Checkpoints in an agentic workflow that should require human approval include:**\n\n- **High-stakes actions:** Any decisions or operations that could have significant legal, financial, safety, or reputational consequences for the organisation.\n- **Irreversible actions:** Steps that cannot be undone or easily reversed, such as deleting critical data, executing large financial transactions, or making permanent changes to systems or records.\n- **Boundary or exception cases:** Situations where the agent encounters scenarios outside its normal operating parameters, novel problems, or ambiguous contexts that require nuanced judgment.\n- **Policy deviations:** When an agent’s proposed action deviates from established organisational policies or standard operating procedures.\n\n**Ongoing activity recommended to ensure human oversight remains effective:**\n\n- **Regular auditing of human oversight:** Continuously review and evaluate how humans interact with agentic workflows to ensure they are not becoming overly reliant on the agent (i.e., to avoid automation bias), and that humans are consistently exercising informed and active oversight. This may include periodic audits, user feedback collection, and scenario-based drills to test human intervention in critical moments."
        }
      ],
      "prompt_tokens": 941,
      "completion_tokens": 75,
      "cached_tokens": 0,
      "total_tokens": 1016,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "“Human-in-the-loop” should be adapted to address automation bias in increasingly capable agents by:\n\n1. **Defining Significant Checkpoints:** Establish specific points in the agentic workflow where human approval is mandatory, especially for high-stakes or irreversible actions. This ensures humans remain actively involved in critical decisions and do not simply defer to the agent’s recommendations.\n\n2. **Regularly Auditing Human Oversight:** Continually assess and audit human involvement to ensure human oversight remains effective over time and does not diminish due to over-reliance on the agent.\n\nBy implementing these measures, organisations can reduce the risk of automation bias, where humans may place too much trust in the agent’s decisions, and ensure responsible and safe operation of increasingly capable AI agents."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "**Checkpoints in an agentic workflow that should require human approval include:**\n\n- **High-stakes actions:** Any decisions or operations that could have significant legal, financial, safety, or reputational consequences for the organisation.\n- **Irreversible actions:** Steps that cannot be undone or easily reversed, such as deleting critical data, executing large financial transactions, or making permanent changes to systems or records.\n- **Boundary or exception cases:** Situations where the agent encounters scenarios outside its normal operating parameters, novel problems, or ambiguous contexts that require nuanced judgment.\n- **Policy deviations:** When an agent’s proposed action deviates from established organisational policies or standard operating procedures.\n\n**Ongoing activity recommended to ensure human oversight remains effective:**\n\n- **Regular auditing of human oversight:** Continuously review and evaluate how humans interact with agentic workflows to ensure they are not becoming overly reliant on the agent (i.e., to avoid automation bias), and that humans are consistently exercising informed and active oversight. This may include periodic audits, user feedback collection, and scenario-based drills to test human intervention in critical moments."
        },
        {
          "role": "user",
          "content": "What is the recommended approach during and after deployment to handle the fact that agents interact dynamically with their environment and not all risks can be anticipated upfront?"
        },
        {
          "role": "assistant",
          "content": "The recommended approach during and after deployment is to **gradually roll out agents alongside continuous monitoring after deployment**. This staged deployment allows organisations to observe how agents behave in real-world, dynamic environments, identifying and addressing risks that could not be fully anticipated upfront. Continuous monitoring ensures that new or evolving risks are detected promptly, allowing for timely intervention and adaptation of controls as needed."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-004",
      "turn_number": 1,
      "question": "What are agentic AI systems, and what common features do agents usually have?",
      "page_reference": 4,
      "response": "**Agentic AI systems** are systems that can plan across multiple steps to achieve specified objectives, using AI agents. These systems are capable of independent planning and taking actions—such as searching the web or creating files—over multiple steps to accomplish a user-defined goal.\n\n**Common features** that agents usually have include:\n- Some degree of independent planning\n- The ability to take actions autonomously\n- Multi-step reasoning and execution to achieve objectives\n- The use of a central model (like a language model) to interpret instructions and make decisions\n\nOverall, agentic AI systems are designed to act with a certain level of autonomy to fulfill user-set goals, often by breaking down tasks and executing them step-by-step.",
      "latency_ms": 5012.685060501099,
      "api_latency_ms": 5012.685060501099,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 476,
      "completion_tokens": 144,
      "cached_tokens": 0,
      "total_tokens": 620,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "**Agentic AI systems** are systems that can plan across multiple steps to achieve specified objectives, using AI agents. These systems are capable of independent planning and taking actions—such as searching the web or creating files—over multiple steps to accomplish a user-defined goal.\n\n**Common features** that agents usually have include:\n- Some degree of independent planning\n- The ability to take actions autonomously\n- Multi-step reasoning and execution to achieve objectives\n- The use of a central model (like a language model) to interpret instructions and make decisions\n\nOverall, agentic AI systems are designed to act with a certain level of autonomy to fulfill user-set goals, often by breaking down tasks and executing them step-by-step."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-004",
      "turn_number": 2,
      "question": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?",
      "page_reference": 4,
      "response": "In the framework described, the **“brain” of an agent is the model** that serves as the central reasoning and planning engine. This model processes instructions, interprets user inputs, and generates contextually appropriate responses.\n\nThe kinds of language models that can fill this role are:\n- **Small Language Models (SLM)**\n- **Large Language Models (LLM)**\n- **Multimodal Large Language Models (MLLM)**\n\nAny of these types of language models can serve as the core decision-making and reasoning component of an agent.",
      "latency_ms": 3251.990795135498,
      "api_latency_ms": 3251.990795135498,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "**Agentic AI systems** are systems that can plan across multiple steps to achieve specified objectives, using AI agents. These systems are capable of independent planning and taking actions—such as searching the web or creating files—over multiple steps to accomplish a user-defined goal.\n\n**Common features** that agents usually have include:\n- Some degree of independent planning\n- The ability to take actions autonomously\n- Multi-step reasoning and execution to achieve objectives\n- The use of a central model (like a language model) to interpret instructions and make decisions\n\nOverall, agentic AI systems are designed to act with a certain level of autonomy to fulfill user-set goals, often by breaking down tasks and executing them step-by-step."
        }
      ],
      "prompt_tokens": 654,
      "completion_tokens": 111,
      "cached_tokens": 0,
      "total_tokens": 765,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "**Agentic AI systems** are systems that can plan across multiple steps to achieve specified objectives, using AI agents. These systems are capable of independent planning and taking actions—such as searching the web or creating files—over multiple steps to accomplish a user-defined goal.\n\n**Common features** that agents usually have include:\n- Some degree of independent planning\n- The ability to take actions autonomously\n- Multi-step reasoning and execution to achieve objectives\n- The use of a central model (like a language model) to interpret instructions and make decisions\n\nOverall, agentic AI systems are designed to act with a certain level of autonomy to fulfill user-set goals, often by breaking down tasks and executing them step-by-step."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In the framework described, the **“brain” of an agent is the model** that serves as the central reasoning and planning engine. This model processes instructions, interprets user inputs, and generates contextually appropriate responses.\n\nThe kinds of language models that can fill this role are:\n- **Small Language Models (SLM)**\n- **Large Language Models (LLM)**\n- **Multimodal Large Language Models (MLLM)**\n\nAny of these types of language models can serve as the core decision-making and reasoning component of an agent."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-004",
      "turn_number": 3,
      "question": "What is the role of the model component in a simple LLM-based app, and what does it do with user inputs and instructions?",
      "page_reference": 4,
      "response": "The **model component** in a simple LLM-based app serves as the **central reasoning and planning engine**—essentially, the “brain” of the agent. Its role includes:\n\n- **Processing instructions:** The model interprets and understands the tasks or objectives provided by the user.\n- **Interpreting user inputs:** It analyzes what the user says or requests, extracting meaning and intent.\n- **Generating contextually appropriate responses:** Based on the inputs and instructions, the model formulates relevant and accurate responses or actions to move toward achieving the specified goal.\n\nIn summary, the model takes user inputs and instructions, reasons about them, and then decides what actions or responses are needed to accomplish the user’s objectives.",
      "latency_ms": 2213.634729385376,
      "api_latency_ms": 2213.634729385376,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "**Agentic AI systems** are systems that can plan across multiple steps to achieve specified objectives, using AI agents. These systems are capable of independent planning and taking actions—such as searching the web or creating files—over multiple steps to accomplish a user-defined goal.\n\n**Common features** that agents usually have include:\n- Some degree of independent planning\n- The ability to take actions autonomously\n- Multi-step reasoning and execution to achieve objectives\n- The use of a central model (like a language model) to interpret instructions and make decisions\n\nOverall, agentic AI systems are designed to act with a certain level of autonomy to fulfill user-set goals, often by breaking down tasks and executing them step-by-step."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In the framework described, the **“brain” of an agent is the model** that serves as the central reasoning and planning engine. This model processes instructions, interprets user inputs, and generates contextually appropriate responses.\n\nThe kinds of language models that can fill this role are:\n- **Small Language Models (SLM)**\n- **Large Language Models (LLM)**\n- **Multimodal Large Language Models (MLLM)**\n\nAny of these types of language models can serve as the core decision-making and reasoning component of an agent."
        }
      ],
      "prompt_tokens": 799,
      "completion_tokens": 145,
      "cached_tokens": 0,
      "total_tokens": 944,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "**Agentic AI systems** are systems that can plan across multiple steps to achieve specified objectives, using AI agents. These systems are capable of independent planning and taking actions—such as searching the web or creating files—over multiple steps to accomplish a user-defined goal.\n\n**Common features** that agents usually have include:\n- Some degree of independent planning\n- The ability to take actions autonomously\n- Multi-step reasoning and execution to achieve objectives\n- The use of a central model (like a language model) to interpret instructions and make decisions\n\nOverall, agentic AI systems are designed to act with a certain level of autonomy to fulfill user-set goals, often by breaking down tasks and executing them step-by-step."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In the framework described, the **“brain” of an agent is the model** that serves as the central reasoning and planning engine. This model processes instructions, interprets user inputs, and generates contextually appropriate responses.\n\nThe kinds of language models that can fill this role are:\n- **Small Language Models (SLM)**\n- **Large Language Models (LLM)**\n- **Multimodal Large Language Models (MLLM)**\n\nAny of these types of language models can serve as the core decision-making and reasoning component of an agent."
        },
        {
          "role": "user",
          "content": "What is the role of the model component in a simple LLM-based app, and what does it do with user inputs and instructions?"
        },
        {
          "role": "assistant",
          "content": "The **model component** in a simple LLM-based app serves as the **central reasoning and planning engine**—essentially, the “brain” of the agent. Its role includes:\n\n- **Processing instructions:** The model interprets and understands the tasks or objectives provided by the user.\n- **Interpreting user inputs:** It analyzes what the user says or requests, extracting meaning and intent.\n- **Generating contextually appropriate responses:** Based on the inputs and instructions, the model formulates relevant and accurate responses or actions to move toward achieving the specified goal.\n\nIn summary, the model takes user inputs and instructions, reasons about them, and then decides what actions or responses are needed to accomplish the user’s objectives."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-005",
      "turn_number": 1,
      "question": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?",
      "page_reference": 5,
      "response": "In an agentic system, **tools** are external systems, APIs, or functionalities that the agent can use to interact with the outside world or perform specific tasks beyond its native capabilities. Tools act as extensions to the agent, allowing it to take concrete actions, access resources, or manipulate data.\n\n**Examples of actions that tools can enable an agent to take include:**\n- **Writing to files or databases:** Storing or updating information in external storage systems.\n- **Controlling devices:** Sending instructions to hardware (like IoT devices or robots).\n- **Performing transactions:** Initiating actions such as making purchases, transferring funds, or booking appointments.\n- **Calling external APIs:** Retrieving up-to-date information from the web, sending emails, or integrating with other software services.\n- **Executing code:** Running scripts or computational tasks.\n- **Communicating with other agents:** Using protocols to coordinate or delegate tasks to other agents.\n\nIn summary, tools expand the agent's **action-space**, enabling it to take meaningful actions in the real or digital world, such as modifying data, controlling systems, or interacting with external services.",
      "latency_ms": 7241.682767868042,
      "api_latency_ms": 7241.682767868042,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 621,
      "completion_tokens": 227,
      "cached_tokens": 0,
      "total_tokens": 848,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external systems, APIs, or functionalities that the agent can use to interact with the outside world or perform specific tasks beyond its native capabilities. Tools act as extensions to the agent, allowing it to take concrete actions, access resources, or manipulate data.\n\n**Examples of actions that tools can enable an agent to take include:**\n- **Writing to files or databases:** Storing or updating information in external storage systems.\n- **Controlling devices:** Sending instructions to hardware (like IoT devices or robots).\n- **Performing transactions:** Initiating actions such as making purchases, transferring funds, or booking appointments.\n- **Calling external APIs:** Retrieving up-to-date information from the web, sending emails, or integrating with other software services.\n- **Executing code:** Running scripts or computational tasks.\n- **Communicating with other agents:** Using protocols to coordinate or delegate tasks to other agents.\n\nIn summary, tools expand the agent's **action-space**, enabling it to take meaningful actions in the real or digital world, such as modifying data, controlling systems, or interacting with external services."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-005",
      "turn_number": 2,
      "question": "How does the model use tools to complete a task?",
      "page_reference": 5,
      "response": "The model uses tools to complete a task by following these general steps:\n\n1. **Task Understanding and Planning:**  \n   The model receives a user command or task description. It uses its reasoning and planning capabilities to break down the task into a series of actionable steps.\n\n2. **Identifying Tool Usage:**  \n   When a step requires an action the model cannot perform natively (such as accessing the internet, writing to a file, or controlling a device), it identifies which available tool is suitable for that action.\n\n3. **Tool Invocation:**  \n   The model generates a command or request to invoke the appropriate tool, following a standard protocol (like Model Context Protocol, or MCP) to communicate with the tool. For example, if the model needs to send an email, it would call an email-sending tool with the required details.\n\n4. **Receiving Results:**  \n   The tool executes the action and returns the results or outcome to the model. For example, a database tool might return queried data, or a web search tool might return search results.\n\n5. **Incorporating Output:**  \n   The model incorporates the tool’s output into its ongoing reasoning or workflow. It may use this information to inform the next steps, answer the user, or perform further actions.\n\n6. **Completing the Task:**  \n   The model repeats this process as needed, calling different tools for different steps, until the task is complete.\n\n**In summary:**  \nThe model acts as a coordinator, planning the steps needed to complete a task and calling on tools to perform actions it cannot do on its own. Tools enable the model to interact with external systems, and the results are fed back into the model’s workflow to achieve the final goal.",
      "latency_ms": 4098.294019699097,
      "api_latency_ms": 4098.294019699097,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external systems, APIs, or functionalities that the agent can use to interact with the outside world or perform specific tasks beyond its native capabilities. Tools act as extensions to the agent, allowing it to take concrete actions, access resources, or manipulate data.\n\n**Examples of actions that tools can enable an agent to take include:**\n- **Writing to files or databases:** Storing or updating information in external storage systems.\n- **Controlling devices:** Sending instructions to hardware (like IoT devices or robots).\n- **Performing transactions:** Initiating actions such as making purchases, transferring funds, or booking appointments.\n- **Calling external APIs:** Retrieving up-to-date information from the web, sending emails, or integrating with other software services.\n- **Executing code:** Running scripts or computational tasks.\n- **Communicating with other agents:** Using protocols to coordinate or delegate tasks to other agents.\n\nIn summary, tools expand the agent's **action-space**, enabling it to take meaningful actions in the real or digital world, such as modifying data, controlling systems, or interacting with external services."
        }
      ],
      "prompt_tokens": 866,
      "completion_tokens": 349,
      "cached_tokens": 0,
      "total_tokens": 1215,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external systems, APIs, or functionalities that the agent can use to interact with the outside world or perform specific tasks beyond its native capabilities. Tools act as extensions to the agent, allowing it to take concrete actions, access resources, or manipulate data.\n\n**Examples of actions that tools can enable an agent to take include:**\n- **Writing to files or databases:** Storing or updating information in external storage systems.\n- **Controlling devices:** Sending instructions to hardware (like IoT devices or robots).\n- **Performing transactions:** Initiating actions such as making purchases, transferring funds, or booking appointments.\n- **Calling external APIs:** Retrieving up-to-date information from the web, sending emails, or integrating with other software services.\n- **Executing code:** Running scripts or computational tasks.\n- **Communicating with other agents:** Using protocols to coordinate or delegate tasks to other agents.\n\nIn summary, tools expand the agent's **action-space**, enabling it to take meaningful actions in the real or digital world, such as modifying data, controlling systems, or interacting with external services."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools to complete a task by following these general steps:\n\n1. **Task Understanding and Planning:**  \n   The model receives a user command or task description. It uses its reasoning and planning capabilities to break down the task into a series of actionable steps.\n\n2. **Identifying Tool Usage:**  \n   When a step requires an action the model cannot perform natively (such as accessing the internet, writing to a file, or controlling a device), it identifies which available tool is suitable for that action.\n\n3. **Tool Invocation:**  \n   The model generates a command or request to invoke the appropriate tool, following a standard protocol (like Model Context Protocol, or MCP) to communicate with the tool. For example, if the model needs to send an email, it would call an email-sending tool with the required details.\n\n4. **Receiving Results:**  \n   The tool executes the action and returns the results or outcome to the model. For example, a database tool might return queried data, or a web search tool might return search results.\n\n5. **Incorporating Output:**  \n   The model incorporates the tool’s output into its ongoing reasoning or workflow. It may use this information to inform the next steps, answer the user, or perform further actions.\n\n6. **Completing the Task:**  \n   The model repeats this process as needed, calling different tools for different steps, until the task is complete.\n\n**In summary:**  \nThe model acts as a coordinator, planning the steps needed to complete a task and calling on tools to perform actions it cannot do on its own. Tools enable the model to interact with external systems, and the results are fed back into the model’s workflow to achieve the final goal."
        }
      ]
    },
    {
      "model_name": "gpt-4.1",
      "conversation_id": "conv-005",
      "turn_number": 3,
      "question": "What is action-space, and what determines the range of actions an agent is permitted to take?",
      "page_reference": 5,
      "response": "**Action-space** (sometimes called authority or capabilities) refers to the full range of actions that an agent is permitted to take within an agentic system. It defines what the agent can and cannot do when completing tasks.\n\n### What determines an agent’s action-space?\n\nThe action-space is determined by:\n- **The tools the agent is allowed to use:** If an agent has access to tools for writing files, sending emails, or controlling devices, its action-space includes those actions. If certain tools are restricted, those actions are not available.\n- **Permitted transactions or operations:** Specific permissions—like whether the agent can make purchases, update databases, or access sensitive information—also define the boundaries of its action-space.\n- **Instructions and behavioural constraints:** The rules set in the agent’s instructions (system prompts or policies) may explicitly allow or forbid certain actions, further shaping its action-space.\n- **Protocols and environment:** The protocols the agent follows and the environment it operates in (e.g., enterprise vs. public system) can add technical or policy constraints on what actions are available.\n\n**In summary:**  \nThe action-space is the set of all actions an agent is authorized and technically able to take, determined by its tool access, permissions, instructions, and system protocols.",
      "latency_ms": 3990.442991256714,
      "api_latency_ms": 3990.442991256714,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 3.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external systems, APIs, or functionalities that the agent can use to interact with the outside world or perform specific tasks beyond its native capabilities. Tools act as extensions to the agent, allowing it to take concrete actions, access resources, or manipulate data.\n\n**Examples of actions that tools can enable an agent to take include:**\n- **Writing to files or databases:** Storing or updating information in external storage systems.\n- **Controlling devices:** Sending instructions to hardware (like IoT devices or robots).\n- **Performing transactions:** Initiating actions such as making purchases, transferring funds, or booking appointments.\n- **Calling external APIs:** Retrieving up-to-date information from the web, sending emails, or integrating with other software services.\n- **Executing code:** Running scripts or computational tasks.\n- **Communicating with other agents:** Using protocols to coordinate or delegate tasks to other agents.\n\nIn summary, tools expand the agent's **action-space**, enabling it to take meaningful actions in the real or digital world, such as modifying data, controlling systems, or interacting with external services."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools to complete a task by following these general steps:\n\n1. **Task Understanding and Planning:**  \n   The model receives a user command or task description. It uses its reasoning and planning capabilities to break down the task into a series of actionable steps.\n\n2. **Identifying Tool Usage:**  \n   When a step requires an action the model cannot perform natively (such as accessing the internet, writing to a file, or controlling a device), it identifies which available tool is suitable for that action.\n\n3. **Tool Invocation:**  \n   The model generates a command or request to invoke the appropriate tool, following a standard protocol (like Model Context Protocol, or MCP) to communicate with the tool. For example, if the model needs to send an email, it would call an email-sending tool with the required details.\n\n4. **Receiving Results:**  \n   The tool executes the action and returns the results or outcome to the model. For example, a database tool might return queried data, or a web search tool might return search results.\n\n5. **Incorporating Output:**  \n   The model incorporates the tool’s output into its ongoing reasoning or workflow. It may use this information to inform the next steps, answer the user, or perform further actions.\n\n6. **Completing the Task:**  \n   The model repeats this process as needed, calling different tools for different steps, until the task is complete.\n\n**In summary:**  \nThe model acts as a coordinator, planning the steps needed to complete a task and calling on tools to perform actions it cannot do on its own. Tools enable the model to interact with external systems, and the results are fed back into the model’s workflow to achieve the final goal."
        }
      ],
      "prompt_tokens": 1241,
      "completion_tokens": 257,
      "cached_tokens": 1152,
      "total_tokens": 1498,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, **tools** are external systems, APIs, or functionalities that the agent can use to interact with the outside world or perform specific tasks beyond its native capabilities. Tools act as extensions to the agent, allowing it to take concrete actions, access resources, or manipulate data.\n\n**Examples of actions that tools can enable an agent to take include:**\n- **Writing to files or databases:** Storing or updating information in external storage systems.\n- **Controlling devices:** Sending instructions to hardware (like IoT devices or robots).\n- **Performing transactions:** Initiating actions such as making purchases, transferring funds, or booking appointments.\n- **Calling external APIs:** Retrieving up-to-date information from the web, sending emails, or integrating with other software services.\n- **Executing code:** Running scripts or computational tasks.\n- **Communicating with other agents:** Using protocols to coordinate or delegate tasks to other agents.\n\nIn summary, tools expand the agent's **action-space**, enabling it to take meaningful actions in the real or digital world, such as modifying data, controlling systems, or interacting with external services."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools to complete a task by following these general steps:\n\n1. **Task Understanding and Planning:**  \n   The model receives a user command or task description. It uses its reasoning and planning capabilities to break down the task into a series of actionable steps.\n\n2. **Identifying Tool Usage:**  \n   When a step requires an action the model cannot perform natively (such as accessing the internet, writing to a file, or controlling a device), it identifies which available tool is suitable for that action.\n\n3. **Tool Invocation:**  \n   The model generates a command or request to invoke the appropriate tool, following a standard protocol (like Model Context Protocol, or MCP) to communicate with the tool. For example, if the model needs to send an email, it would call an email-sending tool with the required details.\n\n4. **Receiving Results:**  \n   The tool executes the action and returns the results or outcome to the model. For example, a database tool might return queried data, or a web search tool might return search results.\n\n5. **Incorporating Output:**  \n   The model incorporates the tool’s output into its ongoing reasoning or workflow. It may use this information to inform the next steps, answer the user, or perform further actions.\n\n6. **Completing the Task:**  \n   The model repeats this process as needed, calling different tools for different steps, until the task is complete.\n\n**In summary:**  \nThe model acts as a coordinator, planning the steps needed to complete a task and calling on tools to perform actions it cannot do on its own. Tools enable the model to interact with external systems, and the results are fed back into the model’s workflow to achieve the final goal."
        },
        {
          "role": "user",
          "content": "What is action-space, and what determines the range of actions an agent is permitted to take?"
        },
        {
          "role": "assistant",
          "content": "**Action-space** (sometimes called authority or capabilities) refers to the full range of actions that an agent is permitted to take within an agentic system. It defines what the agent can and cannot do when completing tasks.\n\n### What determines an agent’s action-space?\n\nThe action-space is determined by:\n- **The tools the agent is allowed to use:** If an agent has access to tools for writing files, sending emails, or controlling devices, its action-space includes those actions. If certain tools are restricted, those actions are not available.\n- **Permitted transactions or operations:** Specific permissions—like whether the agent can make purchases, update databases, or access sensitive information—also define the boundaries of its action-space.\n- **Instructions and behavioural constraints:** The rules set in the agent’s instructions (system prompts or policies) may explicitly allow or forbid certain actions, further shaping its action-space.\n- **Protocols and environment:** The protocols the agent follows and the environment it operates in (e.g., enterprise vs. public system) can add technical or policy constraints on what actions are available.\n\n**In summary:**  \nThe action-space is the set of all actions an agent is authorized and technically able to take, determined by its tool access, permissions, instructions, and system protocols."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-001",
      "turn_number": 1,
      "question": "What topics are covered under the Model AI Governance Framework for Agentic AI?",
      "page_reference": 1,
      "response": "The **Model AI Governance Framework for Agentic AI** covers the following topics:\n\n1. **Assess and bound the risks upfront**\n   - **Determine suitable use cases for agent deployment** (Section 2.1.1)\n   - **Bound risks through design by defining agents' limits and permissions** (Section 2.1.2)\n\n2. **Make humans meaningfully accountable**\n   - **Clear allocation of responsibilities within and outside the organization** (Section 2.2.1)\n   - **Design for meaningful human oversight** (Section 2.2.2)\n\n3. **Implement technical controls and processes**\n   - **During design and development, use technical controls** (Section 2.3.1)\n   - **Before deploying, test agents** (Section 2.3.2)\n   - **When deploying, continuously monitor and test** (Section 2.3.3)\n\n4. **Enable end-user responsibility**\n   - **Different users, different needs** (Section 2.4.1)\n   - **Users who interact with agents** (Section 2.4.2)\n   - **Users who integrate agents into their work processes** (Section 2.4.3)",
      "latency_ms": 7886.261701583862,
      "api_latency_ms": 7886.261701583862,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 468,
      "completion_tokens": 253,
      "cached_tokens": 0,
      "total_tokens": 721,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following topics:\n\n1. **Assess and bound the risks upfront**\n   - **Determine suitable use cases for agent deployment** (Section 2.1.1)\n   - **Bound risks through design by defining agents' limits and permissions** (Section 2.1.2)\n\n2. **Make humans meaningfully accountable**\n   - **Clear allocation of responsibilities within and outside the organization** (Section 2.2.1)\n   - **Design for meaningful human oversight** (Section 2.2.2)\n\n3. **Implement technical controls and processes**\n   - **During design and development, use technical controls** (Section 2.3.1)\n   - **Before deploying, test agents** (Section 2.3.2)\n   - **When deploying, continuously monitor and test** (Section 2.3.3)\n\n4. **Enable end-user responsibility**\n   - **Different users, different needs** (Section 2.4.1)\n   - **Users who interact with agents** (Section 2.4.2)\n   - **Users who integrate agents into their work processes** (Section 2.4.3)"
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-001",
      "turn_number": 2,
      "question": "Within that framework, what are the specific steps listed under implementing technical controls and processes?",
      "page_reference": 1,
      "response": "Under the **\"Implement technical controls and processes\"** section of the Model AI Governance Framework for Agentic AI, the specific steps listed are:\n\n1. **During design and development, use technical controls** (Section 2.3.1):\n   - Employ technical measures during the design and development phase to address and mitigate risks.\n\n2. **Before deploying, test agents** (Section 2.3.2):\n   - Conduct thorough testing of agents before deployment to ensure their reliability, safety, and alignment with intended use cases.\n\n3. **When deploying, continuously monitor and test** (Section 2.3.3):\n   - Implement ongoing monitoring and testing after deployment to ensure agents function as intended and adapt to any changes in their operating environment.",
      "latency_ms": 2776.125431060791,
      "api_latency_ms": 2776.125431060791,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following topics:\n\n1. **Assess and bound the risks upfront**\n   - **Determine suitable use cases for agent deployment** (Section 2.1.1)\n   - **Bound risks through design by defining agents' limits and permissions** (Section 2.1.2)\n\n2. **Make humans meaningfully accountable**\n   - **Clear allocation of responsibilities within and outside the organization** (Section 2.2.1)\n   - **Design for meaningful human oversight** (Section 2.2.2)\n\n3. **Implement technical controls and processes**\n   - **During design and development, use technical controls** (Section 2.3.1)\n   - **Before deploying, test agents** (Section 2.3.2)\n   - **When deploying, continuously monitor and test** (Section 2.3.3)\n\n4. **Enable end-user responsibility**\n   - **Different users, different needs** (Section 2.4.1)\n   - **Users who interact with agents** (Section 2.4.2)\n   - **Users who integrate agents into their work processes** (Section 2.4.3)"
        }
      ],
      "prompt_tokens": 745,
      "completion_tokens": 154,
      "cached_tokens": 0,
      "total_tokens": 899,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following topics:\n\n1. **Assess and bound the risks upfront**\n   - **Determine suitable use cases for agent deployment** (Section 2.1.1)\n   - **Bound risks through design by defining agents' limits and permissions** (Section 2.1.2)\n\n2. **Make humans meaningfully accountable**\n   - **Clear allocation of responsibilities within and outside the organization** (Section 2.2.1)\n   - **Design for meaningful human oversight** (Section 2.2.2)\n\n3. **Implement technical controls and processes**\n   - **During design and development, use technical controls** (Section 2.3.1)\n   - **Before deploying, test agents** (Section 2.3.2)\n   - **When deploying, continuously monitor and test** (Section 2.3.3)\n\n4. **Enable end-user responsibility**\n   - **Different users, different needs** (Section 2.4.1)\n   - **Users who interact with agents** (Section 2.4.2)\n   - **Users who integrate agents into their work processes** (Section 2.4.3)"
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Under the **\"Implement technical controls and processes\"** section of the Model AI Governance Framework for Agentic AI, the specific steps listed are:\n\n1. **During design and development, use technical controls** (Section 2.3.1):\n   - Employ technical measures during the design and development phase to address and mitigate risks.\n\n2. **Before deploying, test agents** (Section 2.3.2):\n   - Conduct thorough testing of agents before deployment to ensure their reliability, safety, and alignment with intended use cases.\n\n3. **When deploying, continuously monitor and test** (Section 2.3.3):\n   - Implement ongoing monitoring and testing after deployment to ensure agents function as intended and adapt to any changes in their operating environment."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-001",
      "turn_number": 3,
      "question": "How does the framework describe enabling end-user responsibility in terms of different user types?",
      "page_reference": 1,
      "response": "The framework describes enabling **end-user responsibility** in terms of different user types as follows:\n\n1. **Different users, different needs** (Section 2.4.1):\n   - Recognizes that users have varying levels of expertise, responsibilities, and interactions with agentic AI. The framework emphasizes tailoring guidance and tools to meet the specific needs of different user groups.\n\n2. **Users who interact with agents** (Section 2.4.2):\n   - Focuses on users who directly interact with agentic AI systems. These users need to understand how to use the agents effectively, including knowing their capabilities, limitations, and permissions. Clear instructions and user-friendly interfaces are crucial for facilitating responsible use.\n\n3. **Users who integrate agents into their work processes** (Section 2.4.3):\n   - Addresses users responsible for integrating agents into broader work processes or systems. These users must ensure that the agents are appropriately configured, aligned with organizational goals, and compliant with relevant policies or regulations. Support, training, and guidance are necessary for these users to take responsibility for proper integration.",
      "latency_ms": 4956.964015960693,
      "api_latency_ms": 4956.964015960693,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 2.0,
        "relevance": 4.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following topics:\n\n1. **Assess and bound the risks upfront**\n   - **Determine suitable use cases for agent deployment** (Section 2.1.1)\n   - **Bound risks through design by defining agents' limits and permissions** (Section 2.1.2)\n\n2. **Make humans meaningfully accountable**\n   - **Clear allocation of responsibilities within and outside the organization** (Section 2.2.1)\n   - **Design for meaningful human oversight** (Section 2.2.2)\n\n3. **Implement technical controls and processes**\n   - **During design and development, use technical controls** (Section 2.3.1)\n   - **Before deploying, test agents** (Section 2.3.2)\n   - **When deploying, continuously monitor and test** (Section 2.3.3)\n\n4. **Enable end-user responsibility**\n   - **Different users, different needs** (Section 2.4.1)\n   - **Users who interact with agents** (Section 2.4.2)\n   - **Users who integrate agents into their work processes** (Section 2.4.3)"
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Under the **\"Implement technical controls and processes\"** section of the Model AI Governance Framework for Agentic AI, the specific steps listed are:\n\n1. **During design and development, use technical controls** (Section 2.3.1):\n   - Employ technical measures during the design and development phase to address and mitigate risks.\n\n2. **Before deploying, test agents** (Section 2.3.2):\n   - Conduct thorough testing of agents before deployment to ensure their reliability, safety, and alignment with intended use cases.\n\n3. **When deploying, continuously monitor and test** (Section 2.3.3):\n   - Implement ongoing monitoring and testing after deployment to ensure agents function as intended and adapt to any changes in their operating environment."
        }
      ],
      "prompt_tokens": 922,
      "completion_tokens": 221,
      "cached_tokens": 0,
      "total_tokens": 1143,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\nContents \n \nExecutive Summary ........................................................................................................... 1 \n1 \nIntroduction to Agentic AI ........................................................................................... 3 \n1.1 \nWhat is Agentic AI? ............................................................................................. 3 \n1.1.1 \nCore components of an agent ................................................................................... 3 \n1.1.2 \nMulti-agent setups .................................................................................................... 4 \n1.1.3 \nHow agent design affects the limits and capabilities of each agent ............................. 4 \n1.2 \nRisks of Agentic AI............................................................................................... 6 \n1.2.1 \nSources of risk .......................................................................................................... 6 \n1.2.2 \nTypes of risk ............................................................................................................. 7 \n2 \nModel AI Governance Framework for Agentic AI ........................................................... 8 \n2.1 \nAssess and bound the risks upfront ...................................................................... 9 \n2.1.1 \nDetermine suitable use cases for agent deployment .................................................. 9 \n2.1.2 \nBound risks through design by defining agents limits and permissions ...................... 11 \n2.2 \nMake humans meaningfully accountable ............................................................ 13 \n2.2.1 \nClear allocation of responsibilities within and outside the organisation .................... 13 \n2.2.2 \nDesign for meaningful human oversight ................................................................... 16 \n2.3 \nImplement technical controls and processes ...................................................... 18 \n2.3.1 \nDuring design and development, use technical controls ........................................... 18 \n2.3.2 \nBefore deploying, test agents .................................................................................. 19 \n2.3.3 \nWhen deploying, continuously monitor and test ....................................................... 20 \n2.4 \nEnable end-user responsibility ........................................................................... 22 \n2.4.1 \nDifferent users, different needs ............................................................................... 22 \n2.4.2 \nUsers who interact with agents ................................................................................ 23 \n2.4.3 \nUsers who integrate agents into their work processes .............................................. 23 \nAnnex A: Further resources ............................................................................................... 25 \nAnnex B: Call for feedback and case studies ...................................................................... 27"
        },
        {
          "role": "user",
          "content": "What topics are covered under the Model AI Governance Framework for Agentic AI?"
        },
        {
          "role": "assistant",
          "content": "The **Model AI Governance Framework for Agentic AI** covers the following topics:\n\n1. **Assess and bound the risks upfront**\n   - **Determine suitable use cases for agent deployment** (Section 2.1.1)\n   - **Bound risks through design by defining agents' limits and permissions** (Section 2.1.2)\n\n2. **Make humans meaningfully accountable**\n   - **Clear allocation of responsibilities within and outside the organization** (Section 2.2.1)\n   - **Design for meaningful human oversight** (Section 2.2.2)\n\n3. **Implement technical controls and processes**\n   - **During design and development, use technical controls** (Section 2.3.1)\n   - **Before deploying, test agents** (Section 2.3.2)\n   - **When deploying, continuously monitor and test** (Section 2.3.3)\n\n4. **Enable end-user responsibility**\n   - **Different users, different needs** (Section 2.4.1)\n   - **Users who interact with agents** (Section 2.4.2)\n   - **Users who integrate agents into their work processes** (Section 2.4.3)"
        },
        {
          "role": "user",
          "content": "Within that framework, what are the specific steps listed under implementing technical controls and processes?"
        },
        {
          "role": "assistant",
          "content": "Under the **\"Implement technical controls and processes\"** section of the Model AI Governance Framework for Agentic AI, the specific steps listed are:\n\n1. **During design and development, use technical controls** (Section 2.3.1):\n   - Employ technical measures during the design and development phase to address and mitigate risks.\n\n2. **Before deploying, test agents** (Section 2.3.2):\n   - Conduct thorough testing of agents before deployment to ensure their reliability, safety, and alignment with intended use cases.\n\n3. **When deploying, continuously monitor and test** (Section 2.3.3):\n   - Implement ongoing monitoring and testing after deployment to ensure agents function as intended and adapt to any changes in their operating environment."
        },
        {
          "role": "user",
          "content": "How does the framework describe enabling end-user responsibility in terms of different user types?"
        },
        {
          "role": "assistant",
          "content": "The framework describes enabling **end-user responsibility** in terms of different user types as follows:\n\n1. **Different users, different needs** (Section 2.4.1):\n   - Recognizes that users have varying levels of expertise, responsibilities, and interactions with agentic AI. The framework emphasizes tailoring guidance and tools to meet the specific needs of different user groups.\n\n2. **Users who interact with agents** (Section 2.4.2):\n   - Focuses on users who directly interact with agentic AI systems. These users need to understand how to use the agents effectively, including knowing their capabilities, limitations, and permissions. Clear instructions and user-friendly interfaces are crucial for facilitating responsible use.\n\n3. **Users who integrate agents into their work processes** (Section 2.4.3):\n   - Addresses users responsible for integrating agents into broader work processes or systems. These users must ensure that the agents are appropriately configured, aligned with organizational goals, and compliant with relevant policies or regulations. Support, training, and guidance are necessary for these users to take responsibility for proper integration."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-002",
      "turn_number": 1,
      "question": "What capabilities distinguish AI agents from generative AI?",
      "page_reference": 2,
      "response": "AI agents distinguish themselves from generative AI by their ability to **take actions**, **adapt to new information**, and **interact with other agents and systems** to complete tasks on behalf of humans. While generative AI focuses primarily on creating content (e.g., text, images, or code) based on input prompts, AI agents go beyond content generation to execute tasks, make decisions, and dynamically respond to changes in their environment. These capabilities enable AI agents to perform more complex, task-oriented operations, such as updating databases, making payments, and automating workflows.",
      "latency_ms": 3354.8762798309326,
      "api_latency_ms": 3354.8762798309326,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 621,
      "completion_tokens": 115,
      "cached_tokens": 0,
      "total_tokens": 736,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents distinguish themselves from generative AI by their ability to **take actions**, **adapt to new information**, and **interact with other agents and systems** to complete tasks on behalf of humans. While generative AI focuses primarily on creating content (e.g., text, images, or code) based on input prompts, AI agents go beyond content generation to execute tasks, make decisions, and dynamically respond to changes in their environment. These capabilities enable AI agents to perform more complex, task-oriented operations, such as updating databases, making payments, and automating workflows."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-002",
      "turn_number": 2,
      "question": "What workplace examples are given for how agents are already transforming work?",
      "page_reference": 2,
      "response": "The workplace examples provided for how AI agents are already transforming work include:\n\n1. **Coding assistants** – AI agents that help with programming tasks, potentially improving efficiency and accuracy in software development.\n2. **Customer service agents** – AI agents that handle customer inquiries and support, enhancing responsiveness and productivity in customer-facing roles.\n3. **Automating enterprise productivity workflows** – AI agents that streamline and automate various organizational processes, improving overall operational efficiency.",
      "latency_ms": 2547.477960586548,
      "api_latency_ms": 2547.477960586548,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents distinguish themselves from generative AI by their ability to **take actions**, **adapt to new information**, and **interact with other agents and systems** to complete tasks on behalf of humans. While generative AI focuses primarily on creating content (e.g., text, images, or code) based on input prompts, AI agents go beyond content generation to execute tasks, make decisions, and dynamically respond to changes in their environment. These capabilities enable AI agents to perform more complex, task-oriented operations, such as updating databases, making payments, and automating workflows."
        }
      ],
      "prompt_tokens": 756,
      "completion_tokens": 90,
      "cached_tokens": 0,
      "total_tokens": 846,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents distinguish themselves from generative AI by their ability to **take actions**, **adapt to new information**, and **interact with other agents and systems** to complete tasks on behalf of humans. While generative AI focuses primarily on creating content (e.g., text, images, or code) based on input prompts, AI agents go beyond content generation to execute tasks, make decisions, and dynamically respond to changes in their environment. These capabilities enable AI agents to perform more complex, task-oriented operations, such as updating databases, making payments, and automating workflows."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The workplace examples provided for how AI agents are already transforming work include:\n\n1. **Coding assistants** – AI agents that help with programming tasks, potentially improving efficiency and accuracy in software development.\n2. **Customer service agents** – AI agents that handle customer inquiries and support, enhancing responsiveness and productivity in customer-facing roles.\n3. **Automating enterprise productivity workflows** – AI agents that streamline and automate various organizational processes, improving overall operational efficiency."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-002",
      "turn_number": 3,
      "question": "What factors determine the risks posed by an agent’s actions, and what are two planning-stage steps organisations can take to manage those risks early?",
      "page_reference": 2,
      "response": "### **Factors Determining the Risks Posed by an Agent’s Actions:**\n1. **Scope of actions the agent can take** – The range and impact of tasks the agent is authorized to perform.\n2. **Reversibility of those actions** – Whether the agent's actions can be undone or corrected if necessary.\n3. **Agent’s level of autonomy** – How independently the agent operates without human intervention.\n\n---\n\n### **Two Planning-Stage Steps Organisations Can Take to Manage Risks Early:**\n1. **Limit the scope of impact of agents** – Design appropriate boundaries to restrict the agent’s access to tools and external systems, thereby reducing the potential for harm.\n2. **Ensure traceable and controllable actions** – Establish robust identity management and access controls for agents to ensure their actions can be tracked and regulated.",
      "latency_ms": 3259.1280937194824,
      "api_latency_ms": 3259.1280937194824,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents distinguish themselves from generative AI by their ability to **take actions**, **adapt to new information**, and **interact with other agents and systems** to complete tasks on behalf of humans. While generative AI focuses primarily on creating content (e.g., text, images, or code) based on input prompts, AI agents go beyond content generation to execute tasks, make decisions, and dynamically respond to changes in their environment. These capabilities enable AI agents to perform more complex, task-oriented operations, such as updating databases, making payments, and automating workflows."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The workplace examples provided for how AI agents are already transforming work include:\n\n1. **Coding assistants** – AI agents that help with programming tasks, potentially improving efficiency and accuracy in software development.\n2. **Customer service agents** – AI agents that handle customer inquiries and support, enhancing responsiveness and productivity in customer-facing roles.\n3. **Automating enterprise productivity workflows** – AI agents that streamline and automate various organizational processes, improving overall operational efficiency."
        }
      ],
      "prompt_tokens": 881,
      "completion_tokens": 167,
      "cached_tokens": 0,
      "total_tokens": 1048,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n1 \n \nExecutive Summary  \n \nAgentic AI is the next evolution of AI, holding transformative potential for users and businesses. \nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \nevolving, agents are already transforming the workplace through coding assistants, customer \nservice agents, and automating enterprise productivity workflows. \n \nThese greater capabilities also bring forth new risks. Agents’ access to sensitive data and ability \nto make changes to their environment, such as updating a customer database or making a payment, \nare double-edged swords. As we move towards deploying multiple agents with complex interactions, \noutcomes also become more unpredictable.   \n \nHumans must remain accountable and properly manage these risks. While existing governance \nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \nneed to be translated in practice for agents. Meaningful human control and oversight need to be \nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \nhuman oversight over all agent workflows becomes impractical at scale.  \n \nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or \nusing third-party agentic solutions. Building on our previous model governance frameworks, we have \noutlined key considerations for organisations in four areas when it comes to agents: \n \n1. Assess and bound the risks upfront \n \nOrganisations should adapt their internal structures and processes to account for new risks \nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \ndepend on factors such as the scope of actions the agent can take, the reversibility of those \nactions, and the agent’s level of autonomy.  \n \nTo manage these risks early, organisations could limit the scope of impact of their agents by \ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \nto tools and external systems. They could also ensure that the agent’s actions are traceable \nand controllable through establishing robust identity management and access controls for \nagents.  \n \n2. Make humans meaningfully accountable  \n \nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \nto ensure human accountability. However, the autonomy of agents may complicate \ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \ntherefore important to clearly define the responsibilities of different stakeholders, both"
        },
        {
          "role": "user",
          "content": "What capabilities distinguish AI agents from generative AI?"
        },
        {
          "role": "assistant",
          "content": "AI agents distinguish themselves from generative AI by their ability to **take actions**, **adapt to new information**, and **interact with other agents and systems** to complete tasks on behalf of humans. While generative AI focuses primarily on creating content (e.g., text, images, or code) based on input prompts, AI agents go beyond content generation to execute tasks, make decisions, and dynamically respond to changes in their environment. These capabilities enable AI agents to perform more complex, task-oriented operations, such as updating databases, making payments, and automating workflows."
        },
        {
          "role": "user",
          "content": "What workplace examples are given for how agents are already transforming work?"
        },
        {
          "role": "assistant",
          "content": "The workplace examples provided for how AI agents are already transforming work include:\n\n1. **Coding assistants** – AI agents that help with programming tasks, potentially improving efficiency and accuracy in software development.\n2. **Customer service agents** – AI agents that handle customer inquiries and support, enhancing responsiveness and productivity in customer-facing roles.\n3. **Automating enterprise productivity workflows** – AI agents that streamline and automate various organizational processes, improving overall operational efficiency."
        },
        {
          "role": "user",
          "content": "What factors determine the risks posed by an agent’s actions, and what are two planning-stage steps organisations can take to manage those risks early?"
        },
        {
          "role": "assistant",
          "content": "### **Factors Determining the Risks Posed by an Agent’s Actions:**\n1. **Scope of actions the agent can take** – The range and impact of tasks the agent is authorized to perform.\n2. **Reversibility of those actions** – Whether the agent's actions can be undone or corrected if necessary.\n3. **Agent’s level of autonomy** – How independently the agent operates without human intervention.\n\n---\n\n### **Two Planning-Stage Steps Organisations Can Take to Manage Risks Early:**\n1. **Limit the scope of impact of agents** – Design appropriate boundaries to restrict the agent’s access to tools and external systems, thereby reducing the potential for harm.\n2. **Ensure traceable and controllable actions** – Establish robust identity management and access controls for agents to ensure their actions can be tracked and regulated."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-003",
      "turn_number": 1,
      "question": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?",
      "page_reference": 3,
      "response": "To address automation bias in increasingly capable agents, “human-in-the-loop” should be adapted by implementing the following measures:\n\n1. **Defining significant checkpoints**: Establish specific points in the agent's workflow that require human approval, particularly for critical, high-stakes, or irreversible actions. This ensures that human judgment is consistently applied to important decisions.\n\n2. **Regularly auditing human oversight**: Conduct ongoing evaluations to ensure that human oversight remains effective over time. This involves reviewing the processes and mechanisms for human involvement to determine whether they are functioning as intended and adapting them as necessary.\n\nThese measures aim to counter automation bias, where humans may overly rely on or trust automated systems without sufficient scrutiny, particularly as agents become more capable.",
      "latency_ms": 4690.8721923828125,
      "api_latency_ms": 4690.8721923828125,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 497,
      "completion_tokens": 147,
      "cached_tokens": 0,
      "total_tokens": 644,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "To address automation bias in increasingly capable agents, “human-in-the-loop” should be adapted by implementing the following measures:\n\n1. **Defining significant checkpoints**: Establish specific points in the agent's workflow that require human approval, particularly for critical, high-stakes, or irreversible actions. This ensures that human judgment is consistently applied to important decisions.\n\n2. **Regularly auditing human oversight**: Conduct ongoing evaluations to ensure that human oversight remains effective over time. This involves reviewing the processes and mechanisms for human involvement to determine whether they are functioning as intended and adapting them as necessary.\n\nThese measures aim to counter automation bias, where humans may overly rely on or trust automated systems without sufficient scrutiny, particularly as agents become more capable."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-003",
      "turn_number": 2,
      "question": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?",
      "page_reference": 3,
      "response": "**Checkpoints in an agentic workflow that should require human approval:**  \n1. **High-stakes actions:** Decisions or actions with significant consequences, such as those affecting safety, security, finances, or critical operations.  \n2. **Irreversible actions:** Actions that cannot be undone or easily corrected, such as permanent modifications to systems or data.  \n3. **Ethical or sensitive decisions:** Decisions involving ethical concerns, privacy, or sensitive data handling.  \n\n**Ongoing activity to ensure human oversight stays effective over time:**  \nRegular auditing of human oversight processes is recommended. This involves:  \n1. Periodically reviewing and evaluating how humans are interacting with and supervising the agents.  \n2. Ensuring that human approval mechanisms are functioning as intended and adapting them as needed.  \n3. Updating training and protocols for humans involved in the oversight to enhance their ability to manage and assess agent behavior effectively.",
      "latency_ms": 5501.349925994873,
      "api_latency_ms": 5501.349925994873,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 3.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "To address automation bias in increasingly capable agents, “human-in-the-loop” should be adapted by implementing the following measures:\n\n1. **Defining significant checkpoints**: Establish specific points in the agent's workflow that require human approval, particularly for critical, high-stakes, or irreversible actions. This ensures that human judgment is consistently applied to important decisions.\n\n2. **Regularly auditing human oversight**: Conduct ongoing evaluations to ensure that human oversight remains effective over time. This involves reviewing the processes and mechanisms for human involvement to determine whether they are functioning as intended and adapting them as necessary.\n\nThese measures aim to counter automation bias, where humans may overly rely on or trust automated systems without sufficient scrutiny, particularly as agents become more capable."
        }
      ],
      "prompt_tokens": 680,
      "completion_tokens": 183,
      "cached_tokens": 0,
      "total_tokens": 863,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "To address automation bias in increasingly capable agents, “human-in-the-loop” should be adapted by implementing the following measures:\n\n1. **Defining significant checkpoints**: Establish specific points in the agent's workflow that require human approval, particularly for critical, high-stakes, or irreversible actions. This ensures that human judgment is consistently applied to important decisions.\n\n2. **Regularly auditing human oversight**: Conduct ongoing evaluations to ensure that human oversight remains effective over time. This involves reviewing the processes and mechanisms for human involvement to determine whether they are functioning as intended and adapting them as necessary.\n\nThese measures aim to counter automation bias, where humans may overly rely on or trust automated systems without sufficient scrutiny, particularly as agents become more capable."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "**Checkpoints in an agentic workflow that should require human approval:**  \n1. **High-stakes actions:** Decisions or actions with significant consequences, such as those affecting safety, security, finances, or critical operations.  \n2. **Irreversible actions:** Actions that cannot be undone or easily corrected, such as permanent modifications to systems or data.  \n3. **Ethical or sensitive decisions:** Decisions involving ethical concerns, privacy, or sensitive data handling.  \n\n**Ongoing activity to ensure human oversight stays effective over time:**  \nRegular auditing of human oversight processes is recommended. This involves:  \n1. Periodically reviewing and evaluating how humans are interacting with and supervising the agents.  \n2. Ensuring that human approval mechanisms are functioning as intended and adapting them as needed.  \n3. Updating training and protocols for humans involved in the oversight to enhance their ability to manage and assess agent behavior effectively."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-003",
      "turn_number": 3,
      "question": "What is the recommended approach during and after deployment to handle the fact that agents interact dynamically with their environment and not all risks can be anticipated upfront?",
      "page_reference": 3,
      "response": "The recommended approach during and after deployment to address the dynamic interaction of agents with their environment and the unpredictability of risks is:\n\n1. **Gradual rollout of agents**: Introduce AI agents incrementally rather than deploying them all at once. This allows for controlled observation of their performance and behavior in real-world scenarios.\n\n2. **Continuous monitoring after deployment**: Implement ongoing monitoring processes to assess the agent’s interactions with its environment, identify emerging risks, and ensure safety and reliability. This enables timely detection and mitigation of any issues that arise during operation.\n\nThis approach ensures that agents are deployed responsibly while maintaining the ability to adapt to unforeseen challenges.",
      "latency_ms": 3704.3094635009766,
      "api_latency_ms": 3704.3094635009766,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "To address automation bias in increasingly capable agents, “human-in-the-loop” should be adapted by implementing the following measures:\n\n1. **Defining significant checkpoints**: Establish specific points in the agent's workflow that require human approval, particularly for critical, high-stakes, or irreversible actions. This ensures that human judgment is consistently applied to important decisions.\n\n2. **Regularly auditing human oversight**: Conduct ongoing evaluations to ensure that human oversight remains effective over time. This involves reviewing the processes and mechanisms for human involvement to determine whether they are functioning as intended and adapting them as necessary.\n\nThese measures aim to counter automation bias, where humans may overly rely on or trust automated systems without sufficient scrutiny, particularly as agents become more capable."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "**Checkpoints in an agentic workflow that should require human approval:**  \n1. **High-stakes actions:** Decisions or actions with significant consequences, such as those affecting safety, security, finances, or critical operations.  \n2. **Irreversible actions:** Actions that cannot be undone or easily corrected, such as permanent modifications to systems or data.  \n3. **Ethical or sensitive decisions:** Decisions involving ethical concerns, privacy, or sensitive data handling.  \n\n**Ongoing activity to ensure human oversight stays effective over time:**  \nRegular auditing of human oversight processes is recommended. This involves:  \n1. Periodically reviewing and evaluating how humans are interacting with and supervising the agents.  \n2. Ensuring that human approval mechanisms are functioning as intended and adapting them as needed.  \n3. Updating training and protocols for humans involved in the oversight to enhance their ability to manage and assess agent behavior effectively."
        }
      ],
      "prompt_tokens": 899,
      "completion_tokens": 130,
      "cached_tokens": 0,
      "total_tokens": 1029,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n2 \n \nwithin the organisation and with external vendors, while emphasising adaptive governance, \nso that the organisation is set up to quickly understand new developments and update its \napproach as the technology evolves. \n \nSpecifically, “human-in-the-loop” has to be adapted to address automation bias, which has \nbecome a bigger concern with increasingly capable agents. This includes defining significant \ncheckpoints in the agentic workflow that require human approval, such as high-stakes or \nirreversible actions, and regularly auditing human oversight to check that it remains effective \nover time. \n \n3. Implement technical controls and processes  \n \nOrganisations should ensure the safe and reliable operationalisation of AI agents by \nimplementing technical measures across the agent lifecycle. During development, \norganisations should incorporate technical controls for new agentic components such as \nplanning, tools and still-maturing protocols, to address increased risks from these new \nattack surfaces.  \n \nBefore deployment, organisations should test agents for baseline safety and reliability, \nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \nuse. New testing approaches will be needed to evaluate agents. \n \nDuring and after deployment, as agents interact dynamically with their environment and not \nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \ncontinuous monitoring after deployment.  \n \n4. Enable end-user responsibility \n \nTrustworthy deployment of agents does not rely solely on developers, but also on end-users \nusing them responsibly. To enable responsible use, as a baseline, users should be informed \nof the agent’s range of actions, access to data, and the user’s own responsibilities. \nOrganisations should consider layering on training to equip employees with the knowledge \nrequired to manage human-agent interactions and exercise effective oversight, while \nmaintaining their tradecraft and foundational skills.  \n \nThis is a living document. We have worked with government agencies and leading companies to \ncollate current best practices, but this is a fast-developing space, and best practices will evolve. This \nframework will need to be continuously updated to keep pace with new developments. We invite \nfeedback to refine the framework, and case studies demonstrating how the framework can be \napplied for responsible agentic deployment."
        },
        {
          "role": "user",
          "content": "How should “human-in-the-loop” be adapted to address automation bias in increasingly capable agents?"
        },
        {
          "role": "assistant",
          "content": "To address automation bias in increasingly capable agents, “human-in-the-loop” should be adapted by implementing the following measures:\n\n1. **Defining significant checkpoints**: Establish specific points in the agent's workflow that require human approval, particularly for critical, high-stakes, or irreversible actions. This ensures that human judgment is consistently applied to important decisions.\n\n2. **Regularly auditing human oversight**: Conduct ongoing evaluations to ensure that human oversight remains effective over time. This involves reviewing the processes and mechanisms for human involvement to determine whether they are functioning as intended and adapting them as necessary.\n\nThese measures aim to counter automation bias, where humans may overly rely on or trust automated systems without sufficient scrutiny, particularly as agents become more capable."
        },
        {
          "role": "user",
          "content": "What kinds of checkpoints in an agentic workflow should require human approval, and what ongoing activity is recommended to ensure human oversight stays effective over time?"
        },
        {
          "role": "assistant",
          "content": "**Checkpoints in an agentic workflow that should require human approval:**  \n1. **High-stakes actions:** Decisions or actions with significant consequences, such as those affecting safety, security, finances, or critical operations.  \n2. **Irreversible actions:** Actions that cannot be undone or easily corrected, such as permanent modifications to systems or data.  \n3. **Ethical or sensitive decisions:** Decisions involving ethical concerns, privacy, or sensitive data handling.  \n\n**Ongoing activity to ensure human oversight stays effective over time:**  \nRegular auditing of human oversight processes is recommended. This involves:  \n1. Periodically reviewing and evaluating how humans are interacting with and supervising the agents.  \n2. Ensuring that human approval mechanisms are functioning as intended and adapting them as needed.  \n3. Updating training and protocols for humans involved in the oversight to enhance their ability to manage and assess agent behavior effectively."
        },
        {
          "role": "user",
          "content": "What is the recommended approach during and after deployment to handle the fact that agents interact dynamically with their environment and not all risks can be anticipated upfront?"
        },
        {
          "role": "assistant",
          "content": "The recommended approach during and after deployment to address the dynamic interaction of agents with their environment and the unpredictability of risks is:\n\n1. **Gradual rollout of agents**: Introduce AI agents incrementally rather than deploying them all at once. This allows for controlled observation of their performance and behavior in real-world scenarios.\n\n2. **Continuous monitoring after deployment**: Implement ongoing monitoring processes to assess the agent’s interactions with its environment, identify emerging risks, and ensure safety and reliability. This enables timely detection and mitigation of any issues that arise during operation.\n\nThis approach ensures that agents are deployed responsibly while maintaining the ability to adapt to unforeseen challenges."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-004",
      "turn_number": 1,
      "question": "What are agentic AI systems, and what common features do agents usually have?",
      "page_reference": 4,
      "response": "Agentic AI systems are systems that can plan across multiple steps to achieve specified objectives using AI agents. These systems are built to independently perform tasks and make decisions to accomplish user-defined goals.\n\nAgents, which are central to agentic AI systems, typically have certain common features. These include:\n\n1. **Independent Planning and Action-Taking**: Agents possess some degree of autonomy, enabling them to plan and take actions across multiple steps.\n2. **Goal-Oriented Behavior**: Agents work toward achieving a user-defined goal or objective.\n3. **Interaction with External Systems**: Agents may perform actions such as searching the web, creating files, or interacting with tools to complete tasks.\n\nIn essence, agents are designed to independently plan and act to accomplish tasks across a sequence of steps, based on user-defined instructions.",
      "latency_ms": 3428.3037185668945,
      "api_latency_ms": 3428.3037185668945,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 4.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 476,
      "completion_tokens": 163,
      "cached_tokens": 0,
      "total_tokens": 639,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that can plan across multiple steps to achieve specified objectives using AI agents. These systems are built to independently perform tasks and make decisions to accomplish user-defined goals.\n\nAgents, which are central to agentic AI systems, typically have certain common features. These include:\n\n1. **Independent Planning and Action-Taking**: Agents possess some degree of autonomy, enabling them to plan and take actions across multiple steps.\n2. **Goal-Oriented Behavior**: Agents work toward achieving a user-defined goal or objective.\n3. **Interaction with External Systems**: Agents may perform actions such as searching the web, creating files, or interacting with tools to complete tasks.\n\nIn essence, agents are designed to independently plan and act to accomplish tasks across a sequence of steps, based on user-defined instructions."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-004",
      "turn_number": 2,
      "question": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?",
      "page_reference": 4,
      "response": "In the described framework, the **\"brain\"** of an agent is the **language model**, which serves as the central reasoning and planning engine. It is responsible for processing instructions, interpreting user inputs, and generating contextually appropriate responses.\n\nThe kinds of language models that can fill this role include:\n\n1. **Small Language Models (SLM)**: These are smaller-scale AI models with limited capabilities.\n2. **Large Language Models (LLM)**: These are more advanced AI models with greater capacity for reasoning, understanding, and generating complex responses.\n3. **Multimodal Large Language Models (MLLM)**: These models can process and generate responses across multiple modalities, such as text, images, or other types of data.\n\nThese language models, depending on their size and capabilities, form the core of the agent’s ability to reason, plan, and make decisions.",
      "latency_ms": 2979.3972969055176,
      "api_latency_ms": 2979.3972969055176,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that can plan across multiple steps to achieve specified objectives using AI agents. These systems are built to independently perform tasks and make decisions to accomplish user-defined goals.\n\nAgents, which are central to agentic AI systems, typically have certain common features. These include:\n\n1. **Independent Planning and Action-Taking**: Agents possess some degree of autonomy, enabling them to plan and take actions across multiple steps.\n2. **Goal-Oriented Behavior**: Agents work toward achieving a user-defined goal or objective.\n3. **Interaction with External Systems**: Agents may perform actions such as searching the web, creating files, or interacting with tools to complete tasks.\n\nIn essence, agents are designed to independently plan and act to accomplish tasks across a sequence of steps, based on user-defined instructions."
        }
      ],
      "prompt_tokens": 673,
      "completion_tokens": 177,
      "cached_tokens": 0,
      "total_tokens": 850,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that can plan across multiple steps to achieve specified objectives using AI agents. These systems are built to independently perform tasks and make decisions to accomplish user-defined goals.\n\nAgents, which are central to agentic AI systems, typically have certain common features. These include:\n\n1. **Independent Planning and Action-Taking**: Agents possess some degree of autonomy, enabling them to plan and take actions across multiple steps.\n2. **Goal-Oriented Behavior**: Agents work toward achieving a user-defined goal or objective.\n3. **Interaction with External Systems**: Agents may perform actions such as searching the web, creating files, or interacting with tools to complete tasks.\n\nIn essence, agents are designed to independently plan and act to accomplish tasks across a sequence of steps, based on user-defined instructions."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In the described framework, the **\"brain\"** of an agent is the **language model**, which serves as the central reasoning and planning engine. It is responsible for processing instructions, interpreting user inputs, and generating contextually appropriate responses.\n\nThe kinds of language models that can fill this role include:\n\n1. **Small Language Models (SLM)**: These are smaller-scale AI models with limited capabilities.\n2. **Large Language Models (LLM)**: These are more advanced AI models with greater capacity for reasoning, understanding, and generating complex responses.\n3. **Multimodal Large Language Models (MLLM)**: These models can process and generate responses across multiple modalities, such as text, images, or other types of data.\n\nThese language models, depending on their size and capabilities, form the core of the agent’s ability to reason, plan, and make decisions."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-004",
      "turn_number": 3,
      "question": "What is the role of the model component in a simple LLM-based app, and what does it do with user inputs and instructions?",
      "page_reference": 4,
      "response": "In a simple LLM-based app, the **model component** serves as the **central reasoning and planning engine**, essentially acting as the \"brain\" of the agent. Its role is crucial to the app's functionality.\n\nThe model component performs the following tasks with user inputs and instructions:\n\n1. **Processes Instructions**: It interprets the user's commands or objectives, understanding what needs to be done.\n   \n2. **Interprets User Inputs**: It analyzes the input provided by the user (e.g., questions, prompts, or data) to determine the context and requirements.\n\n3. **Generates Appropriate Responses**: Based on the processed instructions and inputs, it creates contextually appropriate and meaningful outputs or responses to complete tasks or provide information.\n\nIn summary, the model component forms the core of the app's intelligence, enabling it to understand user needs, reason through the task, and produce suitable outputs.",
      "latency_ms": 2827.991724014282,
      "api_latency_ms": 2827.991724014282,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 5.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that can plan across multiple steps to achieve specified objectives using AI agents. These systems are built to independently perform tasks and make decisions to accomplish user-defined goals.\n\nAgents, which are central to agentic AI systems, typically have certain common features. These include:\n\n1. **Independent Planning and Action-Taking**: Agents possess some degree of autonomy, enabling them to plan and take actions across multiple steps.\n2. **Goal-Oriented Behavior**: Agents work toward achieving a user-defined goal or objective.\n3. **Interaction with External Systems**: Agents may perform actions such as searching the web, creating files, or interacting with tools to complete tasks.\n\nIn essence, agents are designed to independently plan and act to accomplish tasks across a sequence of steps, based on user-defined instructions."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In the described framework, the **\"brain\"** of an agent is the **language model**, which serves as the central reasoning and planning engine. It is responsible for processing instructions, interpreting user inputs, and generating contextually appropriate responses.\n\nThe kinds of language models that can fill this role include:\n\n1. **Small Language Models (SLM)**: These are smaller-scale AI models with limited capabilities.\n2. **Large Language Models (LLM)**: These are more advanced AI models with greater capacity for reasoning, understanding, and generating complex responses.\n3. **Multimodal Large Language Models (MLLM)**: These models can process and generate responses across multiple modalities, such as text, images, or other types of data.\n\nThese language models, depending on their size and capabilities, form the core of the agent’s ability to reason, plan, and make decisions."
        }
      ],
      "prompt_tokens": 884,
      "completion_tokens": 184,
      "cached_tokens": 0,
      "total_tokens": 1068,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n3 \n1 Introduction to Agentic AI \n \n1.1 What is Agentic AI? \n \nAgentic AI systems are systems that can plan across multiple steps to achieve specified \nobjectives, using AI agents.1 There is no consensus on what defines an agent, but there are certain \ncommon features – agents usually possess some degree of independent planning and action taking \n(e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal.2 \n \nIn this framework, we focus on agents built on language models, which are increasingly being \nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \nas its brain to make decisions and complete tasks. However, it is worth noting that software agents \nare not a new concept and other types of agents exist, such as those which use deterministic rules, \nor other neural networks, to make decisions.3 \n \n1.1.1 Core components of an agent \n \n \nCore components of a simple agent4 \n \nAs agents are built on top of language models, it is helpful to start with the core components of \na simple LLM-based app. \n \n1. Model: an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \ncontextually appropriate responses. \n \n1  \nAdapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI. \n2  \nSee International AI Safety Report. \n3  \nSee World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance. \n4  \nAdapted from GovTech Singapore, Agentic Risk & Capability Framework, CSA Singapore, Draft \nAddendum on Securing Agentic AI and Anthropic, Building Effective Agents).  \nGuide decisions \nand actions \nRetrieve and store \ninfo for long-term \nInteract with \nexternal systems \nModel \n \n \nPlanning & \nReasoning \nInstructions \nTools \nMemory \n1\n2\n3\n5\n4"
        },
        {
          "role": "user",
          "content": "What are agentic AI systems, and what common features do agents usually have?"
        },
        {
          "role": "assistant",
          "content": "Agentic AI systems are systems that can plan across multiple steps to achieve specified objectives using AI agents. These systems are built to independently perform tasks and make decisions to accomplish user-defined goals.\n\nAgents, which are central to agentic AI systems, typically have certain common features. These include:\n\n1. **Independent Planning and Action-Taking**: Agents possess some degree of autonomy, enabling them to plan and take actions across multiple steps.\n2. **Goal-Oriented Behavior**: Agents work toward achieving a user-defined goal or objective.\n3. **Interaction with External Systems**: Agents may perform actions such as searching the web, creating files, or interacting with tools to complete tasks.\n\nIn essence, agents are designed to independently plan and act to accomplish tasks across a sequence of steps, based on user-defined instructions."
        },
        {
          "role": "user",
          "content": "In the framework described, what serves as the “brain” of an agent, and what kinds of language models can fill that role?"
        },
        {
          "role": "assistant",
          "content": "In the described framework, the **\"brain\"** of an agent is the **language model**, which serves as the central reasoning and planning engine. It is responsible for processing instructions, interpreting user inputs, and generating contextually appropriate responses.\n\nThe kinds of language models that can fill this role include:\n\n1. **Small Language Models (SLM)**: These are smaller-scale AI models with limited capabilities.\n2. **Large Language Models (LLM)**: These are more advanced AI models with greater capacity for reasoning, understanding, and generating complex responses.\n3. **Multimodal Large Language Models (MLLM)**: These models can process and generate responses across multiple modalities, such as text, images, or other types of data.\n\nThese language models, depending on their size and capabilities, form the core of the agent’s ability to reason, plan, and make decisions."
        },
        {
          "role": "user",
          "content": "What is the role of the model component in a simple LLM-based app, and what does it do with user inputs and instructions?"
        },
        {
          "role": "assistant",
          "content": "In a simple LLM-based app, the **model component** serves as the **central reasoning and planning engine**, essentially acting as the \"brain\" of the agent. Its role is crucial to the app's functionality.\n\nThe model component performs the following tasks with user inputs and instructions:\n\n1. **Processes Instructions**: It interprets the user's commands or objectives, understanding what needs to be done.\n   \n2. **Interprets User Inputs**: It analyzes the input provided by the user (e.g., questions, prompts, or data) to determine the context and requirements.\n\n3. **Generates Appropriate Responses**: Based on the processed instructions and inputs, it creates contextually appropriate and meaningful outputs or responses to complete tasks or provide information.\n\nIn summary, the model component forms the core of the app's intelligence, enabling it to understand user needs, reason through the task, and produce suitable outputs."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-005",
      "turn_number": 1,
      "question": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?",
      "page_reference": 5,
      "response": "In an agentic system, tools are components that enable an agent to take actions and interact with external systems. They extend the agent's functionality beyond generating responses and allow it to perform various tasks in the real world or within a digital system.\n\n### Examples of actions tools enable an agent to take include:\n1. **Writing and managing data**:\n   - Writing to files or databases.\n   - Editing or updating existing information.\n   - Logging activities or results for record-keeping.\n\n2. **Interacting with external devices or systems**:\n   - Controlling devices (e.g., turning on/off a smart light or adjusting a thermostat).\n   - Operating machinery or robotic systems.\n   - Sending commands to IoT devices.\n\n3. **Performing transactions**:\n   - Making payments or transferring money.\n   - Executing trades or purchases.\n   - Processing orders in e-commerce systems.\n\n4. **Retrieving information**:\n   - Querying databases or APIs for specific data.\n   - Accessing real-time information from external sources (e.g., weather updates or stock prices).\n\n5. **Communicating**:\n   - Sending emails or messages.\n   - Posting updates on social media platforms.\n   - Engaging in conversations with other agents or systems.\n\n6. **Conducting complex computations or analyses**:\n   - Running specialized algorithms or simulations.\n   - Performing data processing or analytics tasks.\n\n7. **Facilitating integration**:\n   - Connecting with other systems via APIs or protocols.\n   - Orchestrating workflows between multiple systems or agents.\n\nBy leveraging tools, agents can go beyond simply reasoning or planning and can execute real-world or system-level actions, making them highly functional and versatile in a wide range of use cases.",
      "latency_ms": 8682.735204696655,
      "api_latency_ms": 8682.735204696655,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 3.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": null,
      "prompt_tokens": 621,
      "completion_tokens": 349,
      "cached_tokens": 0,
      "total_tokens": 970,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, tools are components that enable an agent to take actions and interact with external systems. They extend the agent's functionality beyond generating responses and allow it to perform various tasks in the real world or within a digital system.\n\n### Examples of actions tools enable an agent to take include:\n1. **Writing and managing data**:\n   - Writing to files or databases.\n   - Editing or updating existing information.\n   - Logging activities or results for record-keeping.\n\n2. **Interacting with external devices or systems**:\n   - Controlling devices (e.g., turning on/off a smart light or adjusting a thermostat).\n   - Operating machinery or robotic systems.\n   - Sending commands to IoT devices.\n\n3. **Performing transactions**:\n   - Making payments or transferring money.\n   - Executing trades or purchases.\n   - Processing orders in e-commerce systems.\n\n4. **Retrieving information**:\n   - Querying databases or APIs for specific data.\n   - Accessing real-time information from external sources (e.g., weather updates or stock prices).\n\n5. **Communicating**:\n   - Sending emails or messages.\n   - Posting updates on social media platforms.\n   - Engaging in conversations with other agents or systems.\n\n6. **Conducting complex computations or analyses**:\n   - Running specialized algorithms or simulations.\n   - Performing data processing or analytics tasks.\n\n7. **Facilitating integration**:\n   - Connecting with other systems via APIs or protocols.\n   - Orchestrating workflows between multiple systems or agents.\n\nBy leveraging tools, agents can go beyond simply reasoning or planning and can execute real-world or system-level actions, making them highly functional and versatile in a wide range of use cases."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-005",
      "turn_number": 2,
      "question": "How does the model use tools to complete a task?",
      "page_reference": 5,
      "response": "The model uses tools to complete a task by identifying the specific actions required to achieve the desired outcome and then calling the appropriate tools to execute those actions. This process typically involves reasoning, planning, and interacting with the tools in a structured and systematic way. Here's a breakdown of how the model uses tools:\n\n---\n\n### 1. **Understanding the Task**  \n   - The model first interprets the task or request provided by the user.\n   - It determines what needs to be accomplished, breaking the task into actionable steps if necessary.\n\n---\n\n### 2. **Planning and Reasoning**  \n   - The model reasons about how to achieve the goal and plans a sequence of actions.\n   - It identifies which steps require external interaction or execution through tools.\n   - For example, if the task is \"Send an email to confirm a meeting,\" the model might recognize that it needs to use an email-sending tool.\n\n---\n\n### 3. **Selecting the Appropriate Tool**  \n   - The model chooses the most suitable tool(s) from the set of tools available in its action-space (the range of tools it is permitted to use).  \n   - Each tool typically has a specific function, such as retrieving data, performing a transaction, or sending a command to a device.\n\n---\n\n### 4. **Communicating with the Tool**  \n   - The model communicates with the tool using a standardized protocol (e.g., **Model Context Protocol (MCP)**) to ensure compatibility and clarity.\n   - It provides the tool with the necessary inputs or parameters (e.g., email recipient, subject, and message content for an email tool).\n\n---\n\n### 5. **Executing the Action via the Tool**  \n   - The tool performs the action as instructed by the model.\n   - For example, the tool might query a database, send a message, or retrieve weather data.\n\n---\n\n### 6. **Processing the Output**  \n   - The tool returns the result or feedback to the model.\n   - The model evaluates the output to determine whether the task is complete or if further action is needed.\n   - For example, if a tool retrieves data, the model may process or analyze it before presenting the final result to the user.\n\n---\n\n### 7. **Iterating if Necessary**  \n   - If the task involves multiple steps, the model may call different tools in sequence or repeat interactions with a tool to refine the results.\n   - For example, if the task is to book a flight, the model might first use a travel search tool to find available flights and then use a booking tool to finalize the purchase.\n\n---\n\n### 8. **Completing the Task**  \n   - Once all the necessary actions are completed using the tools, the model compiles the final result or confirmation and provides it to the user.\n   - For instance, after sending an email, the model might confirm to the user, \"The email has been sent successfully.\"\n\n---\n\n### Summary  \nBy reasoning and planning, selecting the right tools, and using protocols to communicate with and control these tools, the model is able to execute complex tasks that require external actions. This capability allows agents to go beyond just providing information and to perform real-world operations effectively.",
      "latency_ms": 15141.498804092407,
      "api_latency_ms": 15141.498804092407,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 4.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, tools are components that enable an agent to take actions and interact with external systems. They extend the agent's functionality beyond generating responses and allow it to perform various tasks in the real world or within a digital system.\n\n### Examples of actions tools enable an agent to take include:\n1. **Writing and managing data**:\n   - Writing to files or databases.\n   - Editing or updating existing information.\n   - Logging activities or results for record-keeping.\n\n2. **Interacting with external devices or systems**:\n   - Controlling devices (e.g., turning on/off a smart light or adjusting a thermostat).\n   - Operating machinery or robotic systems.\n   - Sending commands to IoT devices.\n\n3. **Performing transactions**:\n   - Making payments or transferring money.\n   - Executing trades or purchases.\n   - Processing orders in e-commerce systems.\n\n4. **Retrieving information**:\n   - Querying databases or APIs for specific data.\n   - Accessing real-time information from external sources (e.g., weather updates or stock prices).\n\n5. **Communicating**:\n   - Sending emails or messages.\n   - Posting updates on social media platforms.\n   - Engaging in conversations with other agents or systems.\n\n6. **Conducting complex computations or analyses**:\n   - Running specialized algorithms or simulations.\n   - Performing data processing or analytics tasks.\n\n7. **Facilitating integration**:\n   - Connecting with other systems via APIs or protocols.\n   - Orchestrating workflows between multiple systems or agents.\n\nBy leveraging tools, agents can go beyond simply reasoning or planning and can execute real-world or system-level actions, making them highly functional and versatile in a wide range of use cases."
        }
      ],
      "prompt_tokens": 988,
      "completion_tokens": 650,
      "cached_tokens": 0,
      "total_tokens": 1638,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, tools are components that enable an agent to take actions and interact with external systems. They extend the agent's functionality beyond generating responses and allow it to perform various tasks in the real world or within a digital system.\n\n### Examples of actions tools enable an agent to take include:\n1. **Writing and managing data**:\n   - Writing to files or databases.\n   - Editing or updating existing information.\n   - Logging activities or results for record-keeping.\n\n2. **Interacting with external devices or systems**:\n   - Controlling devices (e.g., turning on/off a smart light or adjusting a thermostat).\n   - Operating machinery or robotic systems.\n   - Sending commands to IoT devices.\n\n3. **Performing transactions**:\n   - Making payments or transferring money.\n   - Executing trades or purchases.\n   - Processing orders in e-commerce systems.\n\n4. **Retrieving information**:\n   - Querying databases or APIs for specific data.\n   - Accessing real-time information from external sources (e.g., weather updates or stock prices).\n\n5. **Communicating**:\n   - Sending emails or messages.\n   - Posting updates on social media platforms.\n   - Engaging in conversations with other agents or systems.\n\n6. **Conducting complex computations or analyses**:\n   - Running specialized algorithms or simulations.\n   - Performing data processing or analytics tasks.\n\n7. **Facilitating integration**:\n   - Connecting with other systems via APIs or protocols.\n   - Orchestrating workflows between multiple systems or agents.\n\nBy leveraging tools, agents can go beyond simply reasoning or planning and can execute real-world or system-level actions, making them highly functional and versatile in a wide range of use cases."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools to complete a task by identifying the specific actions required to achieve the desired outcome and then calling the appropriate tools to execute those actions. This process typically involves reasoning, planning, and interacting with the tools in a structured and systematic way. Here's a breakdown of how the model uses tools:\n\n---\n\n### 1. **Understanding the Task**  \n   - The model first interprets the task or request provided by the user.\n   - It determines what needs to be accomplished, breaking the task into actionable steps if necessary.\n\n---\n\n### 2. **Planning and Reasoning**  \n   - The model reasons about how to achieve the goal and plans a sequence of actions.\n   - It identifies which steps require external interaction or execution through tools.\n   - For example, if the task is \"Send an email to confirm a meeting,\" the model might recognize that it needs to use an email-sending tool.\n\n---\n\n### 3. **Selecting the Appropriate Tool**  \n   - The model chooses the most suitable tool(s) from the set of tools available in its action-space (the range of tools it is permitted to use).  \n   - Each tool typically has a specific function, such as retrieving data, performing a transaction, or sending a command to a device.\n\n---\n\n### 4. **Communicating with the Tool**  \n   - The model communicates with the tool using a standardized protocol (e.g., **Model Context Protocol (MCP)**) to ensure compatibility and clarity.\n   - It provides the tool with the necessary inputs or parameters (e.g., email recipient, subject, and message content for an email tool).\n\n---\n\n### 5. **Executing the Action via the Tool**  \n   - The tool performs the action as instructed by the model.\n   - For example, the tool might query a database, send a message, or retrieve weather data.\n\n---\n\n### 6. **Processing the Output**  \n   - The tool returns the result or feedback to the model.\n   - The model evaluates the output to determine whether the task is complete or if further action is needed.\n   - For example, if a tool retrieves data, the model may process or analyze it before presenting the final result to the user.\n\n---\n\n### 7. **Iterating if Necessary**  \n   - If the task involves multiple steps, the model may call different tools in sequence or repeat interactions with a tool to refine the results.\n   - For example, if the task is to book a flight, the model might first use a travel search tool to find available flights and then use a booking tool to finalize the purchase.\n\n---\n\n### 8. **Completing the Task**  \n   - Once all the necessary actions are completed using the tools, the model compiles the final result or confirmation and provides it to the user.\n   - For instance, after sending an email, the model might confirm to the user, \"The email has been sent successfully.\"\n\n---\n\n### Summary  \nBy reasoning and planning, selecting the right tools, and using protocols to communicate with and control these tools, the model is able to execute complex tasks that require external actions. This capability allows agents to go beyond just providing information and to perform real-world operations effectively."
        }
      ]
    },
    {
      "model_name": "gpt-4o",
      "conversation_id": "conv-005",
      "turn_number": 3,
      "question": "What is action-space, and what determines the range of actions an agent is permitted to take?",
      "page_reference": 5,
      "response": "**Action-space** refers to the range of actions an agent is permitted to take within an agentic system. It essentially defines the scope of what the agent is allowed and capable of doing. The action-space is determined by the tools the agent has access to, the transactions it is authorized to execute, and the constraints or permissions defined in its design.\n\n---\n\n### **What Determines the Range of Actions an Agent Can Take?**\n\n1. **Tools Available to the Agent**  \n   - The tools integrated into the agent define what actions it can perform. For example:\n     - If the agent has access to an email-sending tool, it can send emails.\n     - If it has access to a database query tool, it can retrieve and manipulate data.\n   - The variety and functionality of tools directly determine the agent's capabilities.\n\n2. **Permissions and Constraints**\n   - Agents are often designed with specific behavioral or operational constraints for safety, ethical, or practical reasons. For example:\n     - An agent may only be authorized to retrieve data but not modify it.\n     - Financial transaction limits may be imposed (e.g., it can only process payments up to a certain amount).\n   - These constraints ensure the agent operates within a defined and controlled environment.\n\n3. **Instructions (System Prompts)**\n   - The instructions or system prompt define the agent's role, purpose, and behavior. For example:\n     - A customer support agent may be instructed to only provide information and escalate unresolved issues to a human.\n     - A research assistant agent may be instructed to search academic databases but not access restricted content.\n\n4. **Protocols and Standards**\n   - Communication protocols (like **Model Context Protocol (MCP)**) and other standards define how the agent interacts with tools and other agents. These protocols ensure the agent only performs actions it is allowed to and that it interacts with tools in a secure and structured way.\n\n5. **Security and Authorization**\n   - Agents often operate within security frameworks that limit their access to sensitive data or systems.\n   - For example:\n     - An agent might require authentication to access certain databases or APIs.\n     - It may have limited access to sensitive information to protect user privacy.\n\n6. **Agent Design and Use Case**  \n   - The purpose and design of the agent play a significant role in defining its action-space. For instance:\n     - A travel booking agent may have tools for searching flights, hotels, and car rentals but not tools for financial analysis.\n     - A robotic agent controlling a smart home will have an action-space limited to commands related to smart devices (e.g., turning lights on/off, adjusting the thermostat).\n\n7. **External Governance or Policies**  \n   - External factors, such as business policies, legal regulations, or ethical guidelines, may restrict the agent's capabilities. For example:\n     - An AI agent in healthcare might only be allowed to provide general health information and not make specific medical diagnoses.\n     - Agents handling financial data may need to comply with privacy laws like GDPR or CCPA.\n\n---\n\n### **Key Takeaways**\n- **Action-space** defines the \"boundaries\" of what an agent can do.  \n- It is determined by the tools available, permissions granted, system prompts, protocols, security measures, use case design, and external policies.  \n- By carefully designing the action-space, developers can ensure the agent operates effectively while staying within safe, ethical, and practical limits.",
      "latency_ms": 16074.177265167236,
      "api_latency_ms": 16074.177265167236,
      "success": true,
      "metrics": {
        "latency_ms": null,
        "groundedness": 3.0,
        "relevance": 5.0,
        "coherence": 5.0,
        "fluency": 5.0
      },
      "error": null,
      "chat_history": [
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, tools are components that enable an agent to take actions and interact with external systems. They extend the agent's functionality beyond generating responses and allow it to perform various tasks in the real world or within a digital system.\n\n### Examples of actions tools enable an agent to take include:\n1. **Writing and managing data**:\n   - Writing to files or databases.\n   - Editing or updating existing information.\n   - Logging activities or results for record-keeping.\n\n2. **Interacting with external devices or systems**:\n   - Controlling devices (e.g., turning on/off a smart light or adjusting a thermostat).\n   - Operating machinery or robotic systems.\n   - Sending commands to IoT devices.\n\n3. **Performing transactions**:\n   - Making payments or transferring money.\n   - Executing trades or purchases.\n   - Processing orders in e-commerce systems.\n\n4. **Retrieving information**:\n   - Querying databases or APIs for specific data.\n   - Accessing real-time information from external sources (e.g., weather updates or stock prices).\n\n5. **Communicating**:\n   - Sending emails or messages.\n   - Posting updates on social media platforms.\n   - Engaging in conversations with other agents or systems.\n\n6. **Conducting complex computations or analyses**:\n   - Running specialized algorithms or simulations.\n   - Performing data processing or analytics tasks.\n\n7. **Facilitating integration**:\n   - Connecting with other systems via APIs or protocols.\n   - Orchestrating workflows between multiple systems or agents.\n\nBy leveraging tools, agents can go beyond simply reasoning or planning and can execute real-world or system-level actions, making them highly functional and versatile in a wide range of use cases."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools to complete a task by identifying the specific actions required to achieve the desired outcome and then calling the appropriate tools to execute those actions. This process typically involves reasoning, planning, and interacting with the tools in a structured and systematic way. Here's a breakdown of how the model uses tools:\n\n---\n\n### 1. **Understanding the Task**  \n   - The model first interprets the task or request provided by the user.\n   - It determines what needs to be accomplished, breaking the task into actionable steps if necessary.\n\n---\n\n### 2. **Planning and Reasoning**  \n   - The model reasons about how to achieve the goal and plans a sequence of actions.\n   - It identifies which steps require external interaction or execution through tools.\n   - For example, if the task is \"Send an email to confirm a meeting,\" the model might recognize that it needs to use an email-sending tool.\n\n---\n\n### 3. **Selecting the Appropriate Tool**  \n   - The model chooses the most suitable tool(s) from the set of tools available in its action-space (the range of tools it is permitted to use).  \n   - Each tool typically has a specific function, such as retrieving data, performing a transaction, or sending a command to a device.\n\n---\n\n### 4. **Communicating with the Tool**  \n   - The model communicates with the tool using a standardized protocol (e.g., **Model Context Protocol (MCP)**) to ensure compatibility and clarity.\n   - It provides the tool with the necessary inputs or parameters (e.g., email recipient, subject, and message content for an email tool).\n\n---\n\n### 5. **Executing the Action via the Tool**  \n   - The tool performs the action as instructed by the model.\n   - For example, the tool might query a database, send a message, or retrieve weather data.\n\n---\n\n### 6. **Processing the Output**  \n   - The tool returns the result or feedback to the model.\n   - The model evaluates the output to determine whether the task is complete or if further action is needed.\n   - For example, if a tool retrieves data, the model may process or analyze it before presenting the final result to the user.\n\n---\n\n### 7. **Iterating if Necessary**  \n   - If the task involves multiple steps, the model may call different tools in sequence or repeat interactions with a tool to refine the results.\n   - For example, if the task is to book a flight, the model might first use a travel search tool to find available flights and then use a booking tool to finalize the purchase.\n\n---\n\n### 8. **Completing the Task**  \n   - Once all the necessary actions are completed using the tools, the model compiles the final result or confirmation and provides it to the user.\n   - For instance, after sending an email, the model might confirm to the user, \"The email has been sent successfully.\"\n\n---\n\n### Summary  \nBy reasoning and planning, selecting the right tools, and using protocols to communicate with and control these tools, the model is able to execute complex tasks that require external actions. This capability allows agents to go beyond just providing information and to perform real-world operations effectively."
        }
      ],
      "prompt_tokens": 1664,
      "completion_tokens": 693,
      "cached_tokens": 1536,
      "total_tokens": 2357,
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant.\n\nUse the following context to answer questions:\n4 \n2. Instructions: Natural language commands that define an agent's role, capabilities, and \nbehavioural constraints e.g. a system prompt for an LLM. \n3. Memory: Information that is stored and accessible to the LLM, either in short or long-term \nstorage. Sometimes added to allow the model to obtain information from previous user \ninteractions or external knowledge sources. \n \nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In \naddition, it has other components that enable it to complete more complex tasks: \n \n4. Planning and reasoning: The model is usually trained to reason and plan, meaning that it \ncan output a series of steps needed for a task. \n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing \nto files and databases, controlling devices, or performing transactions. The model calls tools \nto complete a task. \n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. \nFor example, the Model Context Protocol (MCP) has been developed for agents to \ncommunicate with tools,5 whereas the Agent2Agent Protocol (A2A) defines a standard for \nagents to communicate with each other.6 \n \n1.1.2 Multi-agent setups \n \nIn an agentic system, it is common for multiple agents to be set up to work together. This can \nsometimes improve performance, by allowing each agent to specialise in a certain function or task \nand work in parallel. 7 \n \nThree common design patterns for multi-agent systems are:8 \n \n• \nSequential: Agents work one after another in a linear workflow. Each agent’s output \nbecomes the next agent’s input. \n• \nSupervisor: One supervising agent coordinates specialised agents under it. \n• \nSwarm: Agents work at the same time, handing off to another agent when needed \n \n1.1.3 How agent design affects the limits and capabilities of each agent \n \nWhile each agent may have the same core components, the design of each component can \nsignificantly affect what the agent can do. It is generally helpful to distinguish between two \nconcepts when considering what an agent can do:9 \n \n• \nAction-space (or authority, capabilities): Range of actions the agent is permitted to take, \ndetermined by the tools it is allowed to use, transactions it can execute, etc. \n \n5  \nSee Anthropic, Model Context Protocol. \n6  \nSee Google, Agent2Agent Protocol. \n7  \nSee LangChain, Benchmarking Multi-Agent Architectures. \n8  \nAdapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova. \n9  \nSee WEF, AI Agents in Action: Foundations for Evaluation and Governance."
        },
        {
          "role": "user",
          "content": "What are tools in an agentic system, and what kinds of actions do they enable an agent to take?"
        },
        {
          "role": "assistant",
          "content": "In an agentic system, tools are components that enable an agent to take actions and interact with external systems. They extend the agent's functionality beyond generating responses and allow it to perform various tasks in the real world or within a digital system.\n\n### Examples of actions tools enable an agent to take include:\n1. **Writing and managing data**:\n   - Writing to files or databases.\n   - Editing or updating existing information.\n   - Logging activities or results for record-keeping.\n\n2. **Interacting with external devices or systems**:\n   - Controlling devices (e.g., turning on/off a smart light or adjusting a thermostat).\n   - Operating machinery or robotic systems.\n   - Sending commands to IoT devices.\n\n3. **Performing transactions**:\n   - Making payments or transferring money.\n   - Executing trades or purchases.\n   - Processing orders in e-commerce systems.\n\n4. **Retrieving information**:\n   - Querying databases or APIs for specific data.\n   - Accessing real-time information from external sources (e.g., weather updates or stock prices).\n\n5. **Communicating**:\n   - Sending emails or messages.\n   - Posting updates on social media platforms.\n   - Engaging in conversations with other agents or systems.\n\n6. **Conducting complex computations or analyses**:\n   - Running specialized algorithms or simulations.\n   - Performing data processing or analytics tasks.\n\n7. **Facilitating integration**:\n   - Connecting with other systems via APIs or protocols.\n   - Orchestrating workflows between multiple systems or agents.\n\nBy leveraging tools, agents can go beyond simply reasoning or planning and can execute real-world or system-level actions, making them highly functional and versatile in a wide range of use cases."
        },
        {
          "role": "user",
          "content": "How does the model use tools to complete a task?"
        },
        {
          "role": "assistant",
          "content": "The model uses tools to complete a task by identifying the specific actions required to achieve the desired outcome and then calling the appropriate tools to execute those actions. This process typically involves reasoning, planning, and interacting with the tools in a structured and systematic way. Here's a breakdown of how the model uses tools:\n\n---\n\n### 1. **Understanding the Task**  \n   - The model first interprets the task or request provided by the user.\n   - It determines what needs to be accomplished, breaking the task into actionable steps if necessary.\n\n---\n\n### 2. **Planning and Reasoning**  \n   - The model reasons about how to achieve the goal and plans a sequence of actions.\n   - It identifies which steps require external interaction or execution through tools.\n   - For example, if the task is \"Send an email to confirm a meeting,\" the model might recognize that it needs to use an email-sending tool.\n\n---\n\n### 3. **Selecting the Appropriate Tool**  \n   - The model chooses the most suitable tool(s) from the set of tools available in its action-space (the range of tools it is permitted to use).  \n   - Each tool typically has a specific function, such as retrieving data, performing a transaction, or sending a command to a device.\n\n---\n\n### 4. **Communicating with the Tool**  \n   - The model communicates with the tool using a standardized protocol (e.g., **Model Context Protocol (MCP)**) to ensure compatibility and clarity.\n   - It provides the tool with the necessary inputs or parameters (e.g., email recipient, subject, and message content for an email tool).\n\n---\n\n### 5. **Executing the Action via the Tool**  \n   - The tool performs the action as instructed by the model.\n   - For example, the tool might query a database, send a message, or retrieve weather data.\n\n---\n\n### 6. **Processing the Output**  \n   - The tool returns the result or feedback to the model.\n   - The model evaluates the output to determine whether the task is complete or if further action is needed.\n   - For example, if a tool retrieves data, the model may process or analyze it before presenting the final result to the user.\n\n---\n\n### 7. **Iterating if Necessary**  \n   - If the task involves multiple steps, the model may call different tools in sequence or repeat interactions with a tool to refine the results.\n   - For example, if the task is to book a flight, the model might first use a travel search tool to find available flights and then use a booking tool to finalize the purchase.\n\n---\n\n### 8. **Completing the Task**  \n   - Once all the necessary actions are completed using the tools, the model compiles the final result or confirmation and provides it to the user.\n   - For instance, after sending an email, the model might confirm to the user, \"The email has been sent successfully.\"\n\n---\n\n### Summary  \nBy reasoning and planning, selecting the right tools, and using protocols to communicate with and control these tools, the model is able to execute complex tasks that require external actions. This capability allows agents to go beyond just providing information and to perform real-world operations effectively."
        },
        {
          "role": "user",
          "content": "What is action-space, and what determines the range of actions an agent is permitted to take?"
        },
        {
          "role": "assistant",
          "content": "**Action-space** refers to the range of actions an agent is permitted to take within an agentic system. It essentially defines the scope of what the agent is allowed and capable of doing. The action-space is determined by the tools the agent has access to, the transactions it is authorized to execute, and the constraints or permissions defined in its design.\n\n---\n\n### **What Determines the Range of Actions an Agent Can Take?**\n\n1. **Tools Available to the Agent**  \n   - The tools integrated into the agent define what actions it can perform. For example:\n     - If the agent has access to an email-sending tool, it can send emails.\n     - If it has access to a database query tool, it can retrieve and manipulate data.\n   - The variety and functionality of tools directly determine the agent's capabilities.\n\n2. **Permissions and Constraints**\n   - Agents are often designed with specific behavioral or operational constraints for safety, ethical, or practical reasons. For example:\n     - An agent may only be authorized to retrieve data but not modify it.\n     - Financial transaction limits may be imposed (e.g., it can only process payments up to a certain amount).\n   - These constraints ensure the agent operates within a defined and controlled environment.\n\n3. **Instructions (System Prompts)**\n   - The instructions or system prompt define the agent's role, purpose, and behavior. For example:\n     - A customer support agent may be instructed to only provide information and escalate unresolved issues to a human.\n     - A research assistant agent may be instructed to search academic databases but not access restricted content.\n\n4. **Protocols and Standards**\n   - Communication protocols (like **Model Context Protocol (MCP)**) and other standards define how the agent interacts with tools and other agents. These protocols ensure the agent only performs actions it is allowed to and that it interacts with tools in a secure and structured way.\n\n5. **Security and Authorization**\n   - Agents often operate within security frameworks that limit their access to sensitive data or systems.\n   - For example:\n     - An agent might require authentication to access certain databases or APIs.\n     - It may have limited access to sensitive information to protect user privacy.\n\n6. **Agent Design and Use Case**  \n   - The purpose and design of the agent play a significant role in defining its action-space. For instance:\n     - A travel booking agent may have tools for searching flights, hotels, and car rentals but not tools for financial analysis.\n     - A robotic agent controlling a smart home will have an action-space limited to commands related to smart devices (e.g., turning lights on/off, adjusting the thermostat).\n\n7. **External Governance or Policies**  \n   - External factors, such as business policies, legal regulations, or ethical guidelines, may restrict the agent's capabilities. For example:\n     - An AI agent in healthcare might only be allowed to provide general health information and not make specific medical diagnoses.\n     - Agents handling financial data may need to comply with privacy laws like GDPR or CCPA.\n\n---\n\n### **Key Takeaways**\n- **Action-space** defines the \"boundaries\" of what an agent can do.  \n- It is determined by the tools available, permissions granted, system prompts, protocols, security measures, use case design, and external policies.  \n- By carefully designing the action-space, developers can ensure the agent operates effectively while staying within safe, ethical, and practical limits."
        }
      ]
    }
  ],
  "status": "completed",
  "started_at": "2026-01-29T09:44:02.899977Z",
  "completed_at": null
}